{"cluster": 3, "subcluster": 22, "abstract_summ": "A substantial portion of the query volume for e-commerce search engines consists of infrequent queries and identifying user intent in such tail queries is critical in retrieving relevant products.Our work supports query completion by extending a user query prefix (one or two characters) to a complete query utilising a foraging-based probabilistic patch selection model.These results show the potential of using query expansion to generate better queries for BERT-based rankers.The intent of a query is defined as a labelling of its tokens with the product attributes whose values are matched against the query tokens during retrieval.Compared to short, keywords queries, higher accuracy of BERT were observed on long, natural language queries, demonstrating BERT\u2019s ability in extracting rich information from complex queries.", "title_summ": "About Query Reformulations?Utilising Information Foraging Theory for User Interaction with Image Query Auto-CompletionVariational Recurrent Sequence-to-Sequence Retrieval for Stepwise IllustrationMultimodal Entity Linking for TweetsGraph-Embedding Empowered Entity RetrievalUsing Image Captions and Multitask Learning for Recommending Query ReformulationsA Regularised Intent Model for Discovering Multiple Intents in E-Commerce Tail QueriesCanonicalizing Knowledge Bases for Recruitment DomainHIN: Hierarchical Inference Network for Document-Level Relation ExtractionJPLink: On Linking Jobs to Vocational Interest TypesEntity Summarization with User FeedbackUsing Topic Information to Improve Non-exact Keyword-Based Search for Mobile ApplicationsJointly Linking Visual and Textual Entity Mentions withThe Story of How Lucene Significantly Improved Query Evaluation PerformanceText-Image-Video Summary Generation Using Joint Integer Linear ProgrammingNeural-IR-Explorer: A Content-Focused Tool to Explore Neural Re-ranking ResultsOn the Temporality of Priors in Entity LinkingRethinking Query Expansion for BERT RerankingEasing Legal News Monitoring with Learning to Rank and BERTAssessing the Impact of OCR Errors in Information RetrievalNeural Embedding-Based Metrics for Pre-retrieval Query Performance PredictionInteractive Learning for Multimedia at LargeJoint Word and Entity Embeddings for Entity Retrieval from a Knowledge GraphWhatThe Effect of Content-Equivalent Near-Duplicates on the Evaluation of Search EnginesUnsupervised Ensemble of Ranking Models for News Comments Using Pseudo AnswersTowards Query Logs for Privacy Studies:Background KnowledgeMining Implicit Relevance Feedback from User Behavior for Web Question AnsweringInteractive Extractive Search over Biomedical CorporaA Feature Analysis for Multimodal News RetrievalAutomatic Textual Evidence Mining in COVID-19 LiteratureOn Deriving Search Queries from QuestionsDualism in Topical RelevancePredicting the Size of Candidate Document Set for Implicit Web Search Result DiversificationA Latent Model for Ad Hoc Table RetrievalFrom MAXSCORE to Block-Max Wand:", "title_abstract_phrases": "A Regularised Intent Model for Discovering Multiple Intents in E-Commerce Tail QueriesA substantial portion of the query volume for e-commerce search engines consists of infrequent queries and identifying user intent in such tail queries is critical in retrieving relevant products.Our work supports query completion by extending a user query prefix (one or two characters) to a complete query utilising a foraging-based probabilistic patch selection model.These results show the potential of using query expansion to generate better queries for BERT-based rankers.The intent of a query is defined as a labelling of its tokens with the product attributes whose values are matched against the query tokens during retrieval.Compared to short, keywords queries, higher accuracy of BERT were observed on long, natural language queries, demonstrating BERT\u2019s ability in extracting rich information from complex queries."}