{"cluster": 7, "subcluster": 9, "abstract_summ": "Instead of relying on experts\u2019 manual analysis, this paper targets using machine learning methods to generalize GPU performance counter data to determine the characteristics of a GPU kernel as they will reveal possible reasons for low performance.Through extensive experimentation with custom software for simulating task scheduling problems on user-defined CPU-GPU platforms, we show that HOFT can obtain schedules at least [Formula: see text] shorter than HEFT\u2019s for medium-to-large numerical linear algebra application task graphs and around [Formula: see text] shorter on average for a large collection of randomly-generated graphs.Based upon a set of experiments that collect hardware performance counters on multiple platforms, the reason for the runtime performance difference appears to be due primarily to algorithmic efficiency gains: the reformulation from the traditional C11-threads and OpenMP expression of the solution into that of data parallel primitives results in significantly fewer instructions being executed.The primary focus of this work and its main contribution is an in-depth study of shared-memory parallel performance of different implementations, which include those using alternative parallelization approaches such as C11-threads, OpenMP, and data parallel primitives (DPPs).This work examines performance characteristics of multiple shared-memory implementations of a probabilistic graphical modeling (PGM) optimization code, which forms the basis for an advanced, state-of-the art image segmentation method.", "title_summ": "FASTHash: FPGA-Based High Throughput Parallel Hash TableShared-Memory Parallel Probabilistic Graphical Modeling Optimization: Comparison of Threads, OpenMP, and Data-Parallel PrimitivesDesynchronization and Wave Pattern Formation in MPI-Parallel and Hybrid Memory-Bound ProgramsAn Efficient New Static Scheduling Heuristic for Accelerated ArchitecturesUtilizing GPU Performance Counters to Characterize GPU Kernels via Machine LearningheFFTe: Highly Efficient FFT for Exascale", "title_abstract_phrases": "Instead of relying on experts\u2019 manual analysis, this paper targets using machine learning methods to generalize GPU performance counter data to determine the characteristics of a GPU kernel as they will reveal possible reasons for low performance.The experimental results on stencil computing kernels and sparse matrix multiplications show the machine learning models\u2019 good accuracy, and demonstrate a feasible approach that is capable of classifying a kernel\u2019s characterizations and suggesting changes to a skilled user, who can subsequently improve kernel performance with less guessing.heFFTe: Highly Efficient FFT for ExascaleExascale computing aspires to meet the increasing demands from large scientific applications.Through extensive experimentation with custom software for simulating task scheduling problems on user-defined CPU-GPU platforms, we show that HOFT can obtain schedules at least [Formula: see text] shorter than HEFT\u2019s for medium-to-large numerical linear algebra application task graphs and around [Formula: see text] shorter on average for a large collection of randomly-generated graphs.Utilizing GPU Performance Counters to Characterize GPU Kernels via Machine LearningGPU computing kernels are relatively simple to write if achieving the best performance is not of the highest priority.Based upon a set of experiments that collect hardware performance counters on multiple platforms, the reason for the runtime performance difference appears to be due primarily to algorithmic efficiency gains: the reformulation from the traditional C11-threads and OpenMP expression of the solution into that of data parallel primitives results in significantly fewer instructions being executed."}