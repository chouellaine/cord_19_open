[{"cord_uid":"c6uk93fz","source_x":"PMC","title":"Predicting Information Diffusion on Twitter a Deep Learning Neural Network Model Using Custom Weighted Word Features","doi":"10.1007\/978-3-030-44999-5_38","abstract":"Researchers have been experimenting with various drivers of the diffusion rate like sentiment analysis which only considers the presence of certain words in a tweet. We theorize that the diffusion of particular content on Twitter can be driven by a sequence of nouns, adjectives, adverbs forming a sentence. We exhibit that the proposed approach is coherent with the intrinsic disposition of tweets to a common choice of words while constructing a sentence to express an opinion or sentiment. Through this paper, we propose a Custom Weighted Word Embedding (CWWE) to study the degree of diffusion of content (retweet on Twitter). Our framework first extracts the words, create a matrix of these words using the sequences in the tweet text. To this sequence matrix we further multiply custom weights basis the presence index in a sentence wherein higher weights are given if the impactful class of tokens\/words like nouns, adjectives are used at the beginning of the sentence than at last. We then try to predict the possibility of diffusion of information using Long-Short Term Memory Deep Neural Network architecture, which in turn is further optimized on the accuracy and training execution time by a Convolutional Neural Network architecture. The results of the proposed CWWE are compared to a pre-trained glove word embedding. For experimentation, we created a corpus of size 230,000 tweets posted by more than 45,000 users in 6 months. Research experimentations reveal that using the proposed framework of Custom Weighted Word Embedding (CWWE) from the tweet there is a significant improvement in the overall accuracy of Deep Learning framework model in predicting information diffusion through tweets.","publish_time":1583452800000,"author_summary":" Kushwaha, Amit Kumar; Kar, Arpan Kumar;<br>Vigneswara Ilavarasan, P.","abstract_summary":" Researchers have been experimenting with<br>various drivers of the diffusion rate like sentiment<br>analysis which only considers the presence of certain<br>words in a tweet. We theorize that the diffusion of<br>particular content on Twitter can be driven by a sequence of<br>nouns, adjectives, adverbs forming a sentence. We<br>exhibit that the proposed approach is coherent with the<br>intrinsic disposition of tweets to a common choice of<br>words while constructing a sentence to express an<br>opinion or sentiment. Through this paper, we propose a<br>Custom Weighted Word Embedding (CWWE) to study the<br>degree of diffusion of content (retweet on Twitter)....","title_summary":" Predicting Information Diffusion on Twitter a<br>Deep Learning Neural Network Model Using Custom<br>Weighted Word Features","x":-33.7591171265,"y":30.2978687286,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7591171265,"tsne_y":30.2978687286,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"kgje01qy","source_x":"PMC","title":"CLEF eHealth Evaluation Lab 2020","doi":"10.1007\/978-3-030-45442-5_76","abstract":"Laypeople\u2019s increasing difficulties to retrieve and digest valid and relevant information in their preferred language to make health-centred decisions has motivated CLEF eHealth to organize yearly labs since 2012. These 20 evaluation tasks on Information Extraction (IE), management, and Information Retrieval (IR) in 2013\u20132019 have been popular\u2014as demonstrated by the large number of team registrations, submissions, papers, their included authors, and citations (748, 177, 184, 741, and 1299, respectively, up to and including 2018)\u2014and achieved statistically significant improvements in the processing quality. In 2020, CLEF eHealth is calling for participants to contribute to the following two tasks: The 2020 Task 1 on IE focuses on term coding for clinical textual data in Spanish. The terms considered are extracted from clinical case records and they are mapped onto the Spanish version of the International Classification of Diseases, the 10th Revision, including also textual evidence spans for the clinical codes. The 2020 Task 2 is a novel extension of the most popular and established task in CLEF eHealth on CHS. This IR task uses the representative web corpus used in the 2018 challenge, but now also spoken queries, as well as textual transcripts of these queries, are offered to the participants. The task is structured into a number of optional subtasks, covering ad-hoc search using the spoken queries, textual transcripts of the spoken queries, or provided automatic speech-to-text conversions of the spoken queries. In this paper we describe the evolution of CLEF eHealth and this year\u2019s tasks. The substantial community interest in the tasks and their resources has led to CLEF eHealth maturing as a primary venue for all interdisciplinary actors of the ecosystem for producing, processing, and consuming electronic health information.","publish_time":1585008000000,"author_summary":" Suominen, Hanna; Kelly, Liadh; Goeuriot,<br>Lorraine; Krallinger, Martin","abstract_summary":" Laypeople\u2019s increasing difficulties to<br>retrieve and digest valid and relevant information in<br>their preferred language to make health-centred<br>decisions has motivated CLEF eHealth to organize yearly<br>labs since 2012. These 20 evaluation tasks on<br>Information Extraction (IE), management, and Information<br>Retrieval (IR) in 2013\u20132019 have been popular\u2014as<br>demonstrated by the large number of team registrations,<br>submissions, papers, their included authors, and citations<br>(748, 177, 184, 741, and 1299, respectively, up to and<br>including 2018)\u2014and achieved statistically<br>significant improvements in the processing quality. In<br>2020, CLEF eHealth is calling for participants to<br>contribute to the following two tasks: The 2020 Task...","title_summary":" CLEF eHealth Evaluation Lab 2020","x":-31.6800727844,"y":31.7262134552,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.6800727844,"tsne_y":31.7262134552,"subcluster":7,"subcluster_description":"Clef Ehealth Evaluation Lab","shape":"p"},{"cord_uid":"d3oru1w5","source_x":"PMC","title":"A Multi-task Approach to Open Domain Suggestion Mining Using Language Model for Text Over-Sampling","doi":"10.1007\/978-3-030-45442-5_28","abstract":"Consumer reviews online may contain suggestions useful for improving commercial products and services. Mining suggestions is challenging due to the absence of large labeled and balanced datasets. Furthermore, most prior studies attempting to mine suggestions, have focused on a single domain such as Hotel or Travel only. In this work, we introduce a novel over-sampling technique to address the problem of class imbalance, and propose a multi-task deep learning approach for mining suggestions from multiple domains. Experimental results on a publicly available dataset show that our over-sampling technique, coupled with the multi-task framework outperforms state-of-the-art open domain suggestion mining models in terms of the F-1 measure and AUC.","publish_time":1585008000000,"author_summary":" Leekha, Maitree; Goswami, Mononito; Jain,<br>Minni","abstract_summary":" Consumer reviews online may contain<br>suggestions useful for improving commercial products and<br>services. Mining suggestions is challenging due to the<br>absence of large labeled and balanced datasets.<br>Furthermore, most prior studies attempting to mine<br>suggestions, have focused on a single domain such as Hotel or<br>Travel only. In this work, we introduce a novel<br>over-sampling technique to address the problem of class<br>imbalance, and propose a multi-task deep learning<br>approach for mining suggestions from multiple domains.<br>Experimental results on a publicly available dataset show<br>that our over-sampling technique, coupled with the<br>multi-task framework outperforms state-of-the-art open<br>domain suggestion mining models...","title_summary":" A Multi-task Approach to Open Domain<br>Suggestion Mining Using Language Model for Text<br>Over-Sampling","x":-33.7563323975,"y":32.7371749878,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7563323975,"tsne_y":32.7371749878,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"tqfqmlv8","source_x":"PMC","title":"SentiInc: Incorporating Sentiment Information into Sentiment Transfer Without Parallel Data","doi":"10.1007\/978-3-030-45442-5_39","abstract":"Sentiment-to-sentiment transfer involves changing the sentiment of the given text while preserving the underlying information. In this work, we present a model SentiInc for sentiment-to-sentiment transfer using unpaired mono-sentiment data. Existing sentiment-to-sentiment transfer models ignore the valuable sentiment-specific details already present in the text. We address this issue by providing a simple framework for encoding sentiment-specific information in the target sentence while preserving the content information. This is done by incorporating sentiment based loss in the back-translation based style transfer. Extensive experiments over the Yelp dataset show that the SentiInc outperforms state-of-the-art methods by a margin of as large as [Formula: see text]11% in G-score. The results also demonstrate that our model produces sentiment-accurate and information-preserved sentences.","publish_time":1585008000000,"author_summary":" Pant, Kartikey; Verma, Yash; Mamidi, Radhika","abstract_summary":" Sentiment-to-sentiment transfer involves<br>changing the sentiment of the given text while<br>preserving the underlying information. In this work, we<br>present a model SentiInc for sentiment-to-sentiment<br>transfer using unpaired mono-sentiment data. Existing<br>sentiment-to-sentiment transfer models ignore the valuable<br>sentiment-specific details already present in the text. We address<br>this issue by providing a simple framework for<br>encoding sentiment-specific information in the target<br>sentence while preserving the content information.<br>This is done by incorporating sentiment based loss<br>in the back-translation based style transfer.<br>Extensive experiments over the Yelp dataset show that the<br>SentiInc outperforms state-of-the-art methods by a<br>margin of as large as...","title_summary":" SentiInc: Incorporating Sentiment<br>Information into Sentiment Transfer Without Parallel Data","x":-34.0181732178,"y":30.5552864075,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.0181732178,"tsne_y":30.5552864075,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"5p7pshyj","source_x":"PMC","title":"Identifying Notable News Stories","doi":"10.1007\/978-3-030-45442-5_44","abstract":"The volume of news content has increased significantly in recent years and systems to process and deliver this information in an automated fashion at scale are becoming increasingly prevalent. One critical component that is required in such systems is a method to automatically determine how notable a certain news story is, in order to prioritize these stories during delivery. One way to do so is to compare each story in a stream of news stories to a notable event. In other words, the problem of detecting notable news can be defined as a ranking task; given a trusted source of notable events and a stream of candidate news stories, we aim to answer the question: \u201cWhich of the candidate news stories is most similar to the notable one?\u201d. We employ different combinations of features and learning to rank (LTR) models and gather relevance labels using crowdsourcing. In our approach, we use structured representations of candidate news stories (triples) and we link them to corresponding entities. Our evaluation shows that the features in our proposed method outperform standard ranking methods, and that the trained model generalizes well to unseen news stories.","publish_time":1585008000000,"author_summary":" Saravanou, Antonia; Stefanoni, Giorgio;<br>Meij, Edgar","abstract_summary":" The volume of news content has increased<br>significantly in recent years and systems to process and<br>deliver this information in an automated fashion at<br>scale are becoming increasingly prevalent. One<br>critical component that is required in such systems is a<br>method to automatically determine how notable a<br>certain news story is, in order to prioritize these<br>stories during delivery. One way to do so is to compare<br>each story in a stream of news stories to a notable<br>event. In other words, the problem of detecting<br>notable news can be defined as a ranking task; given a<br>trusted source of...","title_summary":" Identifying Notable News Stories","x":-33.5434532166,"y":31.7879600525,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5434532166,"tsne_y":31.7879600525,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"lt21rklb","source_x":"PMC","title":"Session-Based Path Prediction by Combining Local and Global Content Preferences","doi":"10.1007\/978-3-030-45442-5_16","abstract":"Session-based future page prediction is important for online web experiences to understand user behavior, pre-fetching future content, and for creating future experiences for users. While webpages visited by the user in the current session capture the users\u2019 local preferences, in this work, we show how the global content preferences at the given instant can assist in this task. We present DRS-LaG, a Deep Reinforcement Learning System, based on Local and Global preferences. We capture these global content preferences by tracking a key analytics KPI, the number of views. The problem is formulated using an agent which predicts the next page to be visited by the user, based on the historic webpage content and analytics. In an offline setting, we show how the model can be used for predicting the next webpage that the user visits. The online evaluation shows how this framework can be deployed on a website for dynamic adaptation of web experiences, based on both local and global preferences.","publish_time":1585008000000,"author_summary":" Chawla, Kushal; Chhaya, Niyati","abstract_summary":" Session-based future page prediction is<br>important for online web experiences to understand user<br>behavior, pre-fetching future content, and for creating<br>future experiences for users. While webpages visited<br>by the user in the current session capture the<br>users\u2019 local preferences, in this work, we show how the<br>global content preferences at the given instant can<br>assist in this task. We present DRS-LaG, a Deep<br>Reinforcement Learning System, based on Local and Global<br>preferences. We capture these global content preferences by<br>tracking a key analytics KPI, the number of views. The<br>problem is formulated using an agent which predicts the<br>next page...","title_summary":" Session-Based Path Prediction by Combining<br>Local and Global Content Preferences","x":-30.4240283966,"y":34.2819442749,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.4240283966,"tsne_y":34.2819442749,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"x9n0rbc2","source_x":"PMC","title":"Domain Adaptation via Context Prediction for Engineering Diagram Search","doi":"10.1007\/978-3-030-45442-5_25","abstract":"Effective search for engineering diagram images in larger collections is challenging because most existing feature extraction models are pre-trained on natural image data rather than diagrams. Surprisingly, we observe through experiments that even in-domain training with standard unsupervised representation learning techniques leads to poor results. We argue that, because of their structured nature, diagram images require more specially-tailored learning objectives. We propose a new method for unsupervised adaptation of out-of-domain feature extractors that asks the model to reason about spatial context. Specifically, we fine-tune a pre-trained image encoder by requiring it to correctly predict the relative orientation between pairs of nearby image regions. Experiments on the recently released Ikea Diagram Dataset show that our proposed method leads to substantial improvements on a downstream search task, more than doubling recall for certain query categories in the dataset.","publish_time":1585008000000,"author_summary":" Jhamtani, Harsh; Berg-Kirkpatrick, Taylor","abstract_summary":" Effective search for engineering diagram<br>images in larger collections is challenging because<br>most existing feature extraction models are<br>pre-trained on natural image data rather than diagrams.<br>Surprisingly, we observe through experiments that even<br>in-domain training with standard unsupervised<br>representation learning techniques leads to poor results. We<br>argue that, because of their structured nature,<br>diagram images require more specially-tailored<br>learning objectives. We propose a new method for<br>unsupervised adaptation of out-of-domain feature<br>extractors that asks the model to reason about spatial<br>context. Specifically, we fine-tune a pre-trained<br>image encoder by requiring it to correctly predict<br>the relative orientation between pairs of...","title_summary":" Domain Adaptation via Context Prediction for<br>Engineering Diagram Search","x":-32.9829444885,"y":33.7640457153,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.9829444885,"tsne_y":33.7640457153,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"azbatms6","source_x":"PMC","title":"Principle-to-Program: Neural Methods for Similar Question Retrieval in Online Communities","doi":"10.1007\/978-3-030-45442-5_88","abstract":"Similar question retrieval is a challenge due to lexical gap between query and candidates in archive and is very different from traditional IR methods for duplicate detection, paraphrase identification and semantic equivalence. This tutorial covers recent deep learning techniques which overcome feature engineering issues with existing approaches based on translation models and latent topics. Hands-on proposal thus will introduce each concept from end user (e.g., question-answer pairs) and technique (e.g., attention) perspectives, present state of the art methods and a walkthrough of programs executed on Jupyter notebook using real-world datasets demonstrating principles introduced.","publish_time":1585008000000,"author_summary":" Chelliah, Muthusamy; Shrivastava, Manish;<br>Ram Tej, Jaidam","abstract_summary":" Similar question retrieval is a challenge due<br>to lexical gap between query and candidates in<br>archive and is very different from traditional IR<br>methods for duplicate detection, paraphrase<br>identification and semantic equivalence. This tutorial<br>covers recent deep learning techniques which<br>overcome feature engineering issues with existing<br>approaches based on translation models and latent topics.<br>Hands-on proposal thus will introduce each concept from<br>end user (e.g., question-answer pairs) and<br>technique (e.g., attention) perspectives, present<br>state of the art methods and a walkthrough of programs<br>executed on Jupyter notebook using real-world datasets<br>demonstrating principles introduced.","title_summary":" Principle-to-Program: Neural Methods for<br>Similar Question Retrieval in Online Communities","x":-34.3308868408,"y":33.0481834412,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.3308868408,"tsne_y":33.0481834412,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"94lscws8","source_x":"PMC","title":"BERT for Evidence Retrieval and Claim Verification","doi":"10.1007\/978-3-030-45442-5_45","abstract":"We investigate BERT in an evidence retrieval and claim verification pipeline for the task of evidence-based claim verification. To this end, we propose to use two BERT models, one for retrieving evidence sentences supporting or rejecting claims, and another for verifying claims based on the retrieved evidence sentences. To train the BERT retrieval system, we use pointwise and pairwise loss functions and examine the effect of hard negative mining. Our system achieves a new state of the art recall of 87.1 for retrieving evidence sentences out of the FEVER dataset 50K Wikipedia pages, and scores second in the leaderboard with the FEVER score of 69.7.","publish_time":1585008000000,"author_summary":" Soleimani, Amir; Monz, Christof; Worring,<br>Marcel","abstract_summary":" We investigate BERT in an evidence retrieval<br>and claim verification pipeline for the task of<br>evidence-based claim verification. To this end, we propose to<br>use two BERT models, one for retrieving evidence<br>sentences supporting or rejecting claims, and another<br>for verifying claims based on the retrieved<br>evidence sentences. To train the BERT retrieval system,<br>we use pointwise and pairwise loss functions and<br>examine the effect of hard negative mining. Our system<br>achieves a new state of the art recall of 87.1 for<br>retrieving evidence sentences out of the FEVER dataset 50K<br>Wikipedia pages, and scores second in the leaderboard<br>with...","title_summary":" BERT for Evidence Retrieval and Claim<br>Verification","x":-35.1731185913,"y":34.1365470886,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.1731185913,"tsne_y":34.1365470886,"subcluster":5,"subcluster_description":"Variantsrandom Steinhaus Distances","shape":"p"},{"cord_uid":"3sbicp3v","source_x":"PMC","title":"Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-Shot Learning","doi":"10.1007\/978-3-030-45442-5_31","abstract":"While billions of non-English speaking users rely on search engines every day, the problem of ad-hoc information retrieval is rarely studied for non-English languages. This is primarily due to a lack of data set that are suitable to train ranking algorithms. In this paper, we tackle the lack of data by leveraging pre-trained multilingual language models to transfer a retrieval system trained on English collections to non-English queries and documents. Our model is evaluated in a zero-shot setting, meaning that we use them to predict relevance scores for query-document pairs in languages never seen during training. Our results show that the proposed approach can significantly outperform unsupervised retrieval techniques for Arabic, Chinese Mandarin, and Spanish. We also show that augmenting the English training collection with some examples from the target language can sometimes improve performance.","publish_time":1585008000000,"author_summary":" MacAvaney, Sean; Soldaini, Luca; Goharian,<br>Nazli","abstract_summary":" While billions of non-English speaking users<br>rely on search engines every day, the problem of<br>ad-hoc information retrieval is rarely studied for<br>non-English languages. This is primarily due to a lack of<br>data set that are suitable to train ranking<br>algorithms. In this paper, we tackle the lack of data by<br>leveraging pre-trained multilingual language models to<br>transfer a retrieval system trained on English<br>collections to non-English queries and documents. Our<br>model is evaluated in a zero-shot setting, meaning<br>that we use them to predict relevance scores for<br>query-document pairs in languages never seen during training.<br>Our results show that...","title_summary":" Teaching a New Dog Old Tricks: Resurrecting<br>Multilingual Retrieval Using Zero-Shot Learning","x":-34.6974945068,"y":32.438041687,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6974945068,"tsne_y":32.438041687,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"6mu7ccbp","source_x":"PMC","title":"The Effect of Content-Equivalent Near-Duplicates on the Evaluation of Search Engines","doi":"10.1007\/978-3-030-45442-5_2","abstract":"Current best practices for the evaluation of search engines do not take into account duplicate documents. Dependent on their prevalence, not discounting duplicates during evaluation artificially inflates performance scores, and, it penalizes those whose search systems diligently filter them. Although these negative effects have already been demonstrated a long time ago by Bernstein and Zobel [4], we find that this has failed to move the community. In this paper, we reproduce the aforementioned study and extend it to incorporate all TREC Terabyte, Web, and Core tracks. The worst-case penalty of having filtered duplicates in any of these tracks were losses between 8 and 53 ranks.","publish_time":1585008000000,"author_summary":" Fr\u00f6be, Maik; Bittner, Jan Philipp; Potthast,<br>Martin; Hagen, Matthias","abstract_summary":" Current best practices for the evaluation of<br>search engines do not take into account duplicate<br>documents. Dependent on their prevalence, not<br>discounting duplicates during evaluation artificially<br>inflates performance scores, and, it penalizes those<br>whose search systems diligently filter them.<br>Although these negative effects have already been<br>demonstrated a long time ago by Bernstein and Zobel [4], we<br>find that this has failed to move the community. In<br>this paper, we reproduce the aforementioned study<br>and extend it to incorporate all TREC Terabyte,<br>Web, and Core tracks. The worst-case penalty of<br>having filtered duplicates in any of these tracks were<br>losses...","title_summary":" The Effect of Content-Equivalent<br>Near-Duplicates on the Evaluation of Search Engines","x":-34.3105125427,"y":34.1779823303,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.3105125427,"tsne_y":34.1779823303,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"iuvqbb76","source_x":"PMC","title":"Shared Tasks on Authorship Analysis at PAN 2020","doi":"10.1007\/978-3-030-45442-5_66","abstract":"The paper gives a brief overview of the four shared tasks that are to be organized at the PAN 2020 lab on digital text forensics and stylometry, hosted at CLEF conference. The tasks include author profiling, celebrity profiling, cross-domain author verification, and style change detection, seeking to advance the state of the art and to evaluate it on new benchmark datasets.","publish_time":1585008000000,"author_summary":" Bevendorff, Janek; Ghanem, Bilal; Giachanou,<br>Anastasia; Kestemont, Mike; Manjavacas, Enrique;<br>Potthast, Martin; Rangel, Francisco; Rosso, Paolo;<br>Specht, G\u00fcnther; Stamatatos, Efstathios; Stein,<br>Benno; Wiegmann, Matti; Zangerle, Eva","abstract_summary":" The paper gives a brief overview of the four<br>shared tasks that are to be organized at the PAN 2020 lab<br>on digital text forensics and stylometry, hosted<br>at CLEF conference. The tasks include author<br>profiling, celebrity profiling, cross-domain author<br>verification, and style change detection, seeking to advance<br>the state of the art and to evaluate it on new<br>benchmark datasets.","title_summary":" Shared Tasks on Authorship Analysis at PAN 2020","x":-33.2534103394,"y":29.7618961334,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.2534103394,"tsne_y":29.7618961334,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"lof49r72","source_x":"PMC","title":"Novel and Diverse Recommendations by Leveraging Linear Models with User and Item Embeddings","doi":"10.1007\/978-3-030-45442-5_27","abstract":"Nowadays, item recommendation is an increasing concern for many companies. Users tend to be more reactive than proactive for solving information needs. Recommendation accuracy became the most studied aspect of the quality of the suggestions. However, novel and diverse suggestions also contribute to user satisfaction. Unfortunately, it is common to harm those two aspects when optimizing recommendation accuracy. In this paper, we present EER, a linear model for the top-N recommendation task, which takes advantage of user and item embeddings for improving novelty and diversity without harming accuracy.","publish_time":1585008000000,"author_summary":" Landin, Alfonso; Parapar, Javier; Barreiro,<br>\u00c1lvaro","abstract_summary":" Nowadays, item recommendation is an<br>increasing concern for many companies. Users tend to be<br>more reactive than proactive for solving<br>information needs. Recommendation accuracy became the<br>most studied aspect of the quality of the<br>suggestions. However, novel and diverse suggestions also<br>contribute to user satisfaction. Unfortunately, it is<br>common to harm those two aspects when optimizing<br>recommendation accuracy. In this paper, we present EER, a<br>linear model for the top-N recommendation task, which<br>takes advantage of user and item embeddings for<br>improving novelty and diversity without harming<br>accuracy.","title_summary":" Novel and Diverse Recommendations by<br>Leveraging Linear Models with User and Item Embeddings","x":-31.0449123383,"y":34.2853469849,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0449123383,"tsne_y":34.2853469849,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"n16b2ymi","source_x":"PMC","title":"Distant Supervision for Extractive Question Summarization","doi":"10.1007\/978-3-030-45442-5_23","abstract":"Questions are often lengthy and difficult to understand because they tend to contain peripheral information. Previous work relies on costly human-annotated data or question-title pairs. In this work, we propose a distant supervision framework that can train a question summarizer without annotation costs or question-title pairs, where sentences are automatically annotated by means of heuristic rules. The key idea is that a single-sentence question tends to have a summary-like property. We empirically show that our models trained on the framework perform competitively with respect to supervised models without the requirement of a costly human-annotated dataset.","publish_time":1585008000000,"author_summary":" Ishigaki, Tatsuya; Machida, Kazuya;<br>Kobayashi, Hayato; Takamura, Hiroya; Okumura, Manabu","abstract_summary":" Questions are often lengthy and difficult to<br>understand because they tend to contain peripheral<br>information. Previous work relies on costly<br>human-annotated data or question-title pairs. In this work, we<br>propose a distant supervision framework that can train<br>a question summarizer without annotation costs<br>or question-title pairs, where sentences are<br>automatically annotated by means of heuristic rules. The key<br>idea is that a single-sentence question tends to<br>have a summary-like property. We empirically show<br>that our models trained on the framework perform<br>competitively with respect to supervised models without the<br>requirement of a costly human-annotated dataset.","title_summary":" Distant Supervision for Extractive Question<br>Summarization","x":-34.6510734558,"y":32.788684845,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6510734558,"tsne_y":32.788684845,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"wpso3jug","source_x":"PMC","title":"Machine-Actionable Data Management Plans: A Knowledge Retrieval Approach to Automate the Assessment of Funders\u2019 Requirements","doi":"10.1007\/978-3-030-45442-5_15","abstract":"Funding bodies and other policy-makers are increasingly more concerned with Research Data Management (RDM). The Data Management Plan (DMP) is one of the tools available to perform RDM tasks, however it is not a perfect concept. The Machine-Actionable Data Management Plan (maDMP) is a concept that aims to make the DMP interoperable, automated and increasingly standardised. In this paper we showcase that through the usage of semantic technologies, it is possible to both express and exploit the features of the maDMP. In particular, we focus on showing how a maDMP formalised as an ontology can be used automate the assessment of a funder\u2019s requirements for a given organisation.","publish_time":1585008000000,"author_summary":" Cardoso, Jo\u00e3o; Proen\u00e7a, Diogo; Borbinha, Jos\u00e9","abstract_summary":" Funding bodies and other policy-makers are<br>increasingly more concerned with Research Data Management<br>(RDM). The Data Management Plan (DMP) is one of the<br>tools available to perform RDM tasks, however it is<br>not a perfect concept. The Machine-Actionable<br>Data Management Plan (maDMP) is a concept that aims<br>to make the DMP interoperable, automated and<br>increasingly standardised. In this paper we showcase that<br>through the usage of semantic technologies, it is<br>possible to both express and exploit the features of the<br>maDMP. In particular, we focus on showing how a maDMP<br>formalised as an ontology can be used automate the<br>assessment...","title_summary":" Machine-Actionable Data Management Plans: A<br>Knowledge Retrieval Approach to Automate the Assessment<br>of Funders\u2019 Requirements","x":-33.9577331543,"y":36.776966095,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9577331543,"tsne_y":36.776966095,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"jg1z2gy9","source_x":"PMC","title":"Graph-Based Entity-Oriented Search: A Unified Framework in Information Retrieval","doi":"10.1007\/978-3-030-45442-5_78","abstract":"Modern search engines have evolved beyond document retrieval. Nowadays, the information needs of the users can be directly satisfied through entity-oriented search, by taking into account the entities that better relate to the query, as opposed to relying exclusively on the best matching terms. Evolving from keyword-based to entity-oriented search poses several challenges, not only regarding the understanding of natural language queries, which are more familiar to the end-user, but also regarding the integration of unstructured documents and structured information sources such as knowledge bases. One opportunity that remains open is the research of unified frameworks for the representation and retrieval of heterogeneous information sources. The doctoral work we present here proposes graph-based models to promote the cooperation between different units of information, in order to maximize the amount of available leads that help the user satisfy an information need.","publish_time":1585008000000,"author_summary":" Devezas, Jos\u00e9","abstract_summary":" Modern search engines have evolved beyond<br>document retrieval. Nowadays, the information needs of<br>the users can be directly satisfied through<br>entity-oriented search, by taking into account the entities<br>that better relate to the query, as opposed to<br>relying exclusively on the best matching terms.<br>Evolving from keyword-based to entity-oriented search<br>poses several challenges, not only regarding the<br>understanding of natural language queries, which are more<br>familiar to the end-user, but also regarding the<br>integration of unstructured documents and structured<br>information sources such as knowledge bases. One<br>opportunity that remains open is the research of unified<br>frameworks for the representation and...","title_summary":" Graph-Based Entity-Oriented Search: A<br>Unified Framework in Information Retrieval","x":-33.5355987549,"y":35.0172348022,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5355987549,"tsne_y":35.0172348022,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"j0gjgstn","source_x":"PMC","title":"MedLinker: Medical Entity Linking with Neural Representations and Dictionary Matching","doi":"10.1007\/978-3-030-45442-5_29","abstract":"Progress in the field of Natural Language Processing (NLP) has been closely followed by applications in the medical domain. Recent advancements in Neural Language Models (NLMs) have transformed the field and are currently motivating numerous works exploring their application in different domains. In this paper, we explore how NLMs can be used for Medical Entity Linking with the recently introduced MedMentions dataset, which presents two major challenges: (1) a large target ontology of over 2M concepts, and (2) low overlap between concepts in train, validation and test sets. We introduce a solution, MedLinker, that addresses these issues by leveraging specialized NLMs with Approximate Dictionary Matching, and show that it performs competitively on semantic type linking, while improving the state-of-the-art on the more fine-grained task of concept linking (+4 F1 on MedMentions main task).","publish_time":1585008000000,"author_summary":" Loureiro, Daniel; Jorge, Al\u00edpio M\u00e1rio","abstract_summary":" Progress in the field of Natural Language<br>Processing (NLP) has been closely followed by<br>applications in the medical domain. Recent advancements in<br>Neural Language Models (NLMs) have transformed the<br>field and are currently motivating numerous works<br>exploring their application in different domains. In<br>this paper, we explore how NLMs can be used for<br>Medical Entity Linking with the recently introduced<br>MedMentions dataset, which presents two major challenges:<br>(1) a large target ontology of over 2M concepts, and<br>(2) low overlap between concepts in train,<br>validation and test sets. We introduce a solution,<br>MedLinker, that addresses these issues by leveraging<br>specialized...","title_summary":" MedLinker: Medical Entity Linking with Neural<br>Representations and Dictionary Matching","x":-32.150806427,"y":32.2231063843,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.150806427,"tsne_y":32.2231063843,"subcluster":9,"subcluster_description":"Large-Scale Biomedical Semantic Indexing","shape":"p"},{"cord_uid":"pacy48qx","source_x":"PMC","title":"Incremental Approach for Automatic Generation of Domain-Specific Sentiment Lexicon","doi":"10.1007\/978-3-030-45442-5_81","abstract":"Sentiment lexicon plays a vital role in lexicon-based sentiment analysis. The lexicon-based method is often preferred because it leads to more explainable answers in comparison with many machine learning-based methods. But, semantic orientation of a word depends on its domain. Hence, a general-purpose sentiment lexicon may gives sub-optimal performance compare with a domain-specific lexicon. However, it is challenging to manually generate a domain-specific sentiment lexicon for each domain. Still, it is impractical to generate complete sentiment lexicon for a domain from a single corpus. To this end, we propose an approach to automatically generate a domain-specific sentiment lexicon using a vector model enriched by weights. Importantly, we propose an incremental approach for updating an existing lexicon to either the same domain or different domain (domain-adaptation). Finally, we discuss how to incorporate sentiment lexicons information in neural models (word embedding) for better performance.","publish_time":1585008000000,"author_summary":" Muhammad, Shamsuddeen Hassan; Brazdil,<br>Pavel; Jorge, Al\u00edpio","abstract_summary":" Sentiment lexicon plays a vital role in<br>lexicon-based sentiment analysis. The lexicon-based method<br>is often preferred because it leads to more<br>explainable answers in comparison with many machine<br>learning-based methods. But, semantic orientation of a word<br>depends on its domain. Hence, a general-purpose<br>sentiment lexicon may gives sub-optimal performance<br>compare with a domain-specific lexicon. However, it is<br>challenging to manually generate a domain-specific<br>sentiment lexicon for each domain. Still, it is<br>impractical to generate complete sentiment lexicon for a<br>domain from a single corpus. To this end, we propose an<br>approach to automatically generate a domain-specific<br>sentiment lexicon using a...","title_summary":" Incremental Approach for Automatic<br>Generation of Domain-Specific Sentiment Lexicon","x":-34.10521698,"y":30.8543338776,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.10521698,"tsne_y":30.8543338776,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"i7i5b2ou","source_x":"PMC","title":"Hybrid Semantic Recommender System for Chemical Compounds","doi":"10.1007\/978-3-030-45442-5_12","abstract":"Recommending Chemical Compounds of interest to a particular researcher is a poorly explored field. The few existent datasets with information about the preferences of the researchers use implicit feedback. The lack of Recommender Systems in this particular field presents a challenge for the development of new recommendations models. In this work, we propose a Hybrid recommender model for recommending Chemical Compounds. The model integrates collaborative-filtering algorithms for implicit feedback (Alternating Least Squares (ALS) and Bayesian Personalized Ranking (BPR)) and semantic similarity between the Chemical Compounds in the ChEBI ontology (ONTO). We evaluated the model in an implicit dataset of Chemical Compounds, CheRM. The Hybrid model was able to improve the results of state-of-the-art collaborative-filtering algorithms, especially for Mean Reciprocal Rank, with an increase of 6.7% when comparing the collaborative-filtering ALS and the Hybrid ALS_ONTO.","publish_time":1585008000000,"author_summary":" Barros, M\u00e1rcia; Moitinho, Andr\u00e9; Couto,<br>Francisco M.","abstract_summary":" Recommending Chemical Compounds of interest<br>to a particular researcher is a poorly explored<br>field. The few existent datasets with information<br>about the preferences of the researchers use<br>implicit feedback. The lack of Recommender Systems in<br>this particular field presents a challenge for the<br>development of new recommendations models. In this work, we<br>propose a Hybrid recommender model for recommending<br>Chemical Compounds. The model integrates<br>collaborative-filtering algorithms for implicit feedback<br>(Alternating Least Squares (ALS) and Bayesian Personalized<br>Ranking (BPR)) and semantic similarity between the<br>Chemical Compounds in the ChEBI ontology (ONTO). We<br>evaluated the model in an implicit dataset of Chemical...","title_summary":" Hybrid Semantic Recommender System for<br>Chemical Compounds","x":-32.403465271,"y":34.8872413635,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.403465271,"tsne_y":34.8872413635,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"0wc23ixy","source_x":"PMC","title":"ANTIQUE: A Non-factoid Question Answering Benchmark","doi":"10.1007\/978-3-030-45442-5_21","abstract":"Considering the widespread use of mobile and voice search, answer passage retrieval for non-factoid questions plays a critical role in modern information retrieval systems. Despite the importance of the task, the community still feels the significant lack of large-scale non-factoid question answering collections with real questions and comprehensive relevance judgments. In this paper, we develop and release a collection of 2,626 open-domain non-factoid questions from a diverse set of categories. The dataset, called ANTIQUE, contains 34k manual relevance annotations. The questions were asked by real users in a community question answering service, i.e., Yahoo! Answers. Relevance judgments for all the answers to each question were collected through crowdsourcing. To facilitate further research, we also include a brief analysis of the data as well as baseline results on both classical and neural IR models.","publish_time":1585008000000,"author_summary":" Hashemi, Helia; Aliannejadi, Mohammad;<br>Zamani, Hamed; Croft, W. Bruce","abstract_summary":" Considering the widespread use of mobile and<br>voice search, answer passage retrieval for<br>non-factoid questions plays a critical role in modern<br>information retrieval systems. Despite the importance of<br>the task, the community still feels the<br>significant lack of large-scale non-factoid question<br>answering collections with real questions and<br>comprehensive relevance judgments. In this paper, we develop<br>and release a collection of 2,626 open-domain<br>non-factoid questions from a diverse set of categories. The<br>dataset, called ANTIQUE, contains 34k manual relevance<br>annotations. The questions were asked by real users in a<br>community question answering service, i.e., Yahoo!<br>Answers. Relevance judgments for all the...","title_summary":" ANTIQUE: A Non-factoid Question Answering<br>Benchmark","x":-34.9652023315,"y":32.9128417969,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.9652023315,"tsne_y":32.9128417969,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"1ty7wzlv","source_x":"PMC","title":"Knowledge Graph Entity Alignment with Graph Convolutional Networks: Lessons Learned","doi":"10.1007\/978-3-030-45442-5_1","abstract":"In this work, we focus on the problem of entity alignment in Knowledge Graphs (KG) and we report on our experiences when applying a Graph Convolutional Network (GCN) based model for this task. Variants of GCN are used in multiple state-of-the-art approaches and therefore it is important to understand the specifics and limitations of GCN-based models. Despite serious efforts, we were not able to fully reproduce the results from the original paper and after a thorough audit of the code provided by authors, we concluded, that their implementation is different from the architecture described in the paper. In addition, several tricks are required to make the model work and some of them are not very intuitive.We provide an extensive ablation study to quantify the effects these tricks and changes of architecture have on final performance. Furthermore, we examine current evaluation approaches and systematize available benchmark datasets.We believe that people interested in KG matching might profit from our work, as well as novices entering the field. (Code: https:\/\/github.com\/Valentyn1997\/kg-alignment-lessons-learned).","publish_time":1585008000000,"author_summary":" Berrendorf, Max; Faerman, Evgeniy;<br>Melnychuk, Valentyn; Tresp, Volker; Seidl, Thomas","abstract_summary":" In this work, we focus on the problem of entity<br>alignment in Knowledge Graphs (KG) and we report on our<br>experiences when applying a Graph Convolutional Network<br>(GCN) based model for this task. Variants of GCN are<br>used in multiple state-of-the-art approaches and<br>therefore it is important to understand the specifics and<br>limitations of GCN-based models. Despite serious efforts,<br>we were not able to fully reproduce the results<br>from the original paper and after a thorough audit of<br>the code provided by authors, we concluded, that<br>their implementation is different from the<br>architecture described in the paper. In addition, several...","title_summary":" Knowledge Graph Entity Alignment with Graph<br>Convolutional Networks: Lessons Learned","x":-32.0707015991,"y":35.2560691833,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.0707015991,"tsne_y":35.2560691833,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"j9eboa94","source_x":"PMC","title":"Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants","doi":"10.1007\/978-3-030-45442-5_4","abstract":"When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.\u2019s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity \u201cmatter\u201d? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene\u2019s often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work.","publish_time":1585008000000,"author_summary":" Kamphuis, Chris; de Vries, Arjen P.; Boytsov,<br>Leonid; Lin, Jimmy","abstract_summary":" When researchers speak of BM25, it is not<br>entirely clear which variant they mean, since many<br>tweaks to Robertson et al.\u2019s original formulation<br>have been proposed. When practitioners speak of<br>BM25, they most likely refer to the implementation in<br>the Lucene open-source search library. Does this<br>ambiguity \u201cmatter\u201d? We attempt to answer this question<br>with a large-scale reproducibility study of BM25,<br>considering eight variants. Experiments on three newswire<br>collections show that there are no significant<br>effectiveness differences between them, including Lucene\u2019s<br>often maligned approximation of document length. As<br>an added benefit, our empirical approach takes<br>advantage of databases for rapid...","title_summary":" Which BM25 Do You Mean? A Large-Scale<br>Reproducibility Study of Scoring Variants","x":-34.8525009155,"y":33.9815979004,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.8525009155,"tsne_y":33.9815979004,"subcluster":5,"subcluster_description":"Variantsrandom Steinhaus Distances","shape":"p"},{"cord_uid":"aqd4f252","source_x":"PMC","title":"Unsupervised Ensemble of Ranking Models for News Comments Using Pseudo Answers","doi":"10.1007\/978-3-030-45442-5_17","abstract":"Ranking comments on an online news service is a practically important task, and thus there have been many studies on this task. Although ensemble techniques are widely known to improve the performance of models, there is little types of research on ensemble neural-ranking models. In this paper, we investigate how to improve the performance on the comment-ranking task by using unsupervised ensemble methods. We propose a new hybrid method composed of an output selection method and a typical averaging method. Our method uses a pseudo answer represented by the average of multiple model outputs. The pseudo answer is used to evaluate multiple model outputs via ranking evaluation metrics, and the results are used to select and weight the models. Experimental results on the comment-ranking task show that our proposed method outperforms several ensemble baselines, including supervised one.","publish_time":1585008000000,"author_summary":" Fujita, Soichiro; Kobayashi, Hayato;<br>Okumura, Manabu","abstract_summary":" Ranking comments on an online news service is a<br>practically important task, and thus there have been many<br>studies on this task. Although ensemble techniques are<br>widely known to improve the performance of models,<br>there is little types of research on ensemble<br>neural-ranking models. In this paper, we investigate how to<br>improve the performance on the comment-ranking task by<br>using unsupervised ensemble methods. We propose a<br>new hybrid method composed of an output selection<br>method and a typical averaging method. Our method uses<br>a pseudo answer represented by the average of<br>multiple model outputs. The pseudo answer is used to...","title_summary":" Unsupervised Ensemble of Ranking Models for<br>News Comments Using Pseudo Answers","x":-33.27368927,"y":33.4266433716,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.27368927,"tsne_y":33.4266433716,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"zuy90chc","source_x":"PMC","title":"Generating Query Suggestions for Cross-language and Cross-terminology Health Information Retrieval","doi":"10.1007\/978-3-030-45442-5_43","abstract":"Medico-scientific concepts are not easily understood by laypeople that frequently use lay synonyms. For this reason, strategies that help users formulate health queries are essential. Health Suggestions is an existing extension for Google Chrome that provides suggestions in lay and medico-scientific terminologies, both in English and Portuguese. This work proposes, evaluates, and compares further strategies for generating suggestions based on the initial consumer query, using multi-concept recognition and the Unified Medical Language System (UMLS). The evaluation was done with an English and a Portuguese test collection, considering as baseline the suggestions initially provided by Health Suggestions. Given the importance of understandability, we used measures that combine relevance and understandability, namely, uRBP and uRBPgr. Our best method merges the Consumer Health Vocabulary (CHV)-preferred expression for each concept identified in the initial query for lay suggestions and the UMLS-preferred expressions for medico-scientific suggestions. Multi-concept recognition was critical for this improvement.","publish_time":1585008000000,"author_summary":" Santos, Paulo Miguel; Teixeira Lopes, Carla","abstract_summary":" Medico-scientific concepts are not easily<br>understood by laypeople that frequently use lay synonyms.<br>For this reason, strategies that help users<br>formulate health queries are essential. Health<br>Suggestions is an existing extension for Google Chrome that<br>provides suggestions in lay and medico-scientific<br>terminologies, both in English and Portuguese. This work<br>proposes, evaluates, and compares further strategies<br>for generating suggestions based on the initial<br>consumer query, using multi-concept recognition and<br>the Unified Medical Language System (UMLS). The<br>evaluation was done with an English and a Portuguese test<br>collection, considering as baseline the suggestions<br>initially provided by Health Suggestions. Given the<br>importance of...","title_summary":" Generating Query Suggestions for<br>Cross-language and Cross-terminology Health Information<br>Retrieval","x":-31.8699493408,"y":32.2538566589,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.8699493408,"tsne_y":32.2538566589,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"a13zmu4b","source_x":"PMC","title":"Towards Query Logs for Privacy Studies: On Deriving Search Queries from Questions","doi":"10.1007\/978-3-030-45442-5_14","abstract":"Detailed query histories often contain a precise picture of a person\u2019s life, including sensitive and personally identifiable information. As sanitization of such logs is an unsolved research problem, commercial Web search engines that possess large datasets of this kind at their disposal refrain from disseminating them to the wider research community. Ironically, studies examining privacy in search often require detailed search logs with user profiles. This paper builds on an observation that information needs are also expressed in the form of questions in online Community Question Answering (CQA) communities. We take a step towards understanding the process of formulating queries from questions to form a basis for automatic derivation of search logs from CQA forums. Specifically, we sample natural language (NL) questions spanning diverse themes from the StackExchange platform, and conduct a large-scale conversion experiment where crowdworkers submit search queries they would use when looking for equivalent information. We also release a dataset of 7,000 question-query pairs from our study.","publish_time":1585008000000,"author_summary":" Biega, Asia J.; Schmidt, Jana; Roy, Rishiraj<br>Saha","abstract_summary":" Detailed query histories often contain a<br>precise picture of a person\u2019s life, including<br>sensitive and personally identifiable information. As<br>sanitization of such logs is an unsolved research problem,<br>commercial Web search engines that possess large datasets<br>of this kind at their disposal refrain from<br>disseminating them to the wider research community.<br>Ironically, studies examining privacy in search often<br>require detailed search logs with user profiles. This<br>paper builds on an observation that information<br>needs are also expressed in the form of questions in<br>online Community Question Answering (CQA)<br>communities. We take a step towards understanding the<br>process of formulating queries...","title_summary":" Towards Query Logs for Privacy Studies: On<br>Deriving Search Queries from Questions","x":-34.4109230042,"y":34.2622718811,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.4109230042,"tsne_y":34.2622718811,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"ky2nzzh7","source_x":"PMC","title":"Proposal of the First International Workshop on Semantic Indexing and Information Retrieval for Health from Heterogeneous Content Types and Languages (SIIRH)","doi":"10.1007\/978-3-030-45442-5_87","abstract":"The application of Information Retrieval (IR) and deep learning strategies to explore the vast amount of rapidly growing health-related content is of utmost importance, but is also particularly challenging, due to the very specialized domain language, and implicit differences in language characteristics depending on the content type. This workshop aims at presenting and discussing current and future directions for IR and machine learning approaches devoted to the retrieval and classification of different types of health-related documents ranging from layman or patient generated texts to highly specialized medical literature or clinical records. It includes a session on the MESINESP shared task, supported by the Spanish National Language Technology plan (Plan TL), in order to address the importance and impact of community evaluation efforts, in particular BioASQ, BioCreative, eHealth CLEF, MEDIQA and TREC, as scenarios for exploring evaluation settings and generate data collections of key importance for promoting the development and comparison of IR resources. Additionally, an open session will address IR technologies for heterogeneous health-related content open to multiple languages with a particular interest in the exploitation of structured controlled vocabularies and entity linking, covering the following topics: multilingual and non-English health-related IR, concept indexing, text categorization, generation of evaluation resources biomedical document IR strategies; scalability, robustness and reproducibility of health IR and text mining resources; use of specialized machine translation and advanced deep learning approaches for improving health related search results; medical Question Answering search tools; retrieval of multilingual health related web-content; and other related topics.","publish_time":1585008000000,"author_summary":" Couto, Francisco M.; Krallinger, Martin","abstract_summary":" The application of Information Retrieval (IR)<br>and deep learning strategies to explore the vast<br>amount of rapidly growing health-related content is<br>of utmost importance, but is also particularly<br>challenging, due to the very specialized domain language,<br>and implicit differences in language<br>characteristics depending on the content type. This workshop<br>aims at presenting and discussing current and<br>future directions for IR and machine learning<br>approaches devoted to the retrieval and classification of<br>different types of health-related documents ranging<br>from layman or patient generated texts to highly<br>specialized medical literature or clinical records. It<br>includes a session on the MESINESP shared task,...","title_summary":" Proposal of the First International Workshop<br>on Semantic Indexing and Information Retrieval<br>for Health from Heterogeneous Content Types and<br>Languages (SIIRH)","x":-32.1541824341,"y":32.1720123291,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.1541824341,"tsne_y":32.1720123291,"subcluster":9,"subcluster_description":"Large-Scale Biomedical Semantic Indexing","shape":"p"},{"cord_uid":"cbikq0v0","source_x":"PMC","title":"Dualism in Topical Relevance","doi":"10.1007\/978-3-030-45442-5_40","abstract":"There are several concepts whose interpretation and meaning is defined through their binary opposition with other opposite concepts. To this end, in this paper we elaborate on the idea of leveraging the available antonyms of the original query terms for eventually producing an answer which provides a better overview of the related conceptual and information space. Specifically, we sketch a method in which antonyms are used for producing dual queries, which can in turn be exploited for defining a multi-dimensional topical relevance based on the antonyms. We motivate this direction by providing examples and by conducting a preliminary evaluation that shows its importance to specific users.","publish_time":1585008000000,"author_summary":" Papadakos, Panagiotis; Kalipolitis, Orfeas","abstract_summary":" There are several concepts whose<br>interpretation and meaning is defined through their binary<br>opposition with other opposite concepts. To this end, in<br>this paper we elaborate on the idea of leveraging the<br>available antonyms of the original query terms for<br>eventually producing an answer which provides a better<br>overview of the related conceptual and information<br>space. Specifically, we sketch a method in which<br>antonyms are used for producing dual queries, which can<br>in turn be exploited for defining a<br>multi-dimensional topical relevance based on the antonyms. We<br>motivate this direction by providing examples and by<br>conducting a preliminary evaluation that...","title_summary":" Dualism in Topical Relevance","x":-34.2535133362,"y":34.403678894,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2535133362,"tsne_y":34.403678894,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"me2spkll","source_x":"PMC","title":"Graph Databases for Information Retrieval","doi":"10.1007\/978-3-030-45442-5_79","abstract":"Graph models have been deployed in the context of information retrieval for many years. Computations involving the graph structure are often separated from computations related to the base ranking. In recent years, graph data management has been a topic of interest in database research. We propose to deploy graph database management systems to implement existing and novel graph-based models for information retrieval. For this a unifying mapping from a graph query language to graph based retrieval models needs to be developed; extending standard graph database operations with functionality for keyword search. We also investigate how data structures and algorithms for ranking should change in presence of continuous database updates. We want to investigate how temporal decay can affect ranking when data is continuously updated. Finally, can databases be deployed for efficient two-stage retrieval approaches?","publish_time":1585008000000,"author_summary":" Kamphuis, Chris","abstract_summary":" Graph models have been deployed in the context<br>of information retrieval for many years.<br>Computations involving the graph structure are often<br>separated from computations related to the base ranking.<br>In recent years, graph data management has been a<br>topic of interest in database research. We propose to<br>deploy graph database management systems to<br>implement existing and novel graph-based models for<br>information retrieval. For this a unifying mapping from a<br>graph query language to graph based retrieval models<br>needs to be developed; extending standard graph<br>database operations with functionality for keyword<br>search. We also investigate how data structures and<br>algorithms for...","title_summary":" Graph Databases for Information Retrieval","x":-33.4132614136,"y":35.393497467,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4132614136,"tsne_y":35.393497467,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"hv8ae3vk","source_x":"PMC","title":"Predicting the Size of Candidate Document Set for Implicit Web Search Result Diversification","doi":"10.1007\/978-3-030-45442-5_51","abstract":"Implicit result diversification methods exploit the content of the documents in the candidate set, i.e., the initial retrieval results of a query, to obtain a relevant and diverse ranking. As our first contribution, we explore whether recently introduced word embeddings can be exploited for representing documents to improve diversification, and show a positive result. As a second improvement, we propose to automatically predict the size of candidate set on per query basis. Experimental evaluations using our BM25 runs as well as the best-performing ad hoc runs submitted to TREC (2009\u20132012) show that our approach improves the performance of implicit diversification up to 5.4% wrt. initial ranking.","publish_time":1585008000000,"author_summary":" Ulu, Yasar Baris; Altingovde, Ismail Sengor","abstract_summary":" Implicit result diversification methods<br>exploit the content of the documents in the candidate<br>set, i.e., the initial retrieval results of a query,<br>to obtain a relevant and diverse ranking. As our<br>first contribution, we explore whether recently<br>introduced word embeddings can be exploited for<br>representing documents to improve diversification, and<br>show a positive result. As a second improvement, we<br>propose to automatically predict the size of candidate<br>set on per query basis. Experimental evaluations<br>using our BM25 runs as well as the best-performing ad<br>hoc runs submitted to TREC (2009\u20132012) show that<br>our approach improves the performance of implicit<br>diversification...","title_summary":" Predicting the Size of Candidate Document Set<br>for Implicit Web Search Result Diversification","x":-33.8084945679,"y":34.2139968872,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8084945679,"tsne_y":34.2139968872,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"at9g2562","source_x":"PMC","title":"Keyphrase Extraction as Sequence Labeling Using Contextualized Embeddings","doi":"10.1007\/978-3-030-45442-5_41","abstract":"In this paper, we formulate keyphrase extraction from scholarly articles as a sequence labeling task solved using a BiLSTM-CRF, where the words in the input text are represented using deep contextualized embeddings. We evaluate the proposed architecture using both contextualized and fixed word embedding models on three different benchmark datasets, and compare with existing popular unsupervised and supervised techniques. Our results quantify the benefits of: (a) using contextualized embeddings over fixed word embeddings; (b) using a BiLSTM-CRF architecture with contextualized word embeddings over fine-tuning the contextualized embedding model directly; and (c) using domain-specific contextualized embeddings (SciBERT). Through error analysis, we also provide some insights into why particular models work better than the others. Lastly, we present a case study where we analyze different self-attention layers of the two best models (BERT and SciBERT) to better understand their predictions.","publish_time":1585008000000,"author_summary":" Sahrawat, Dhruva; Mahata, Debanjan; Zhang,<br>Haimin; Kulkarni, Mayank; Sharma, Agniv; Gosangi,<br>Rakesh; Stent, Amanda; Kumar, Yaman; Shah, Rajiv Ratn;<br>Zimmermann, Roger","abstract_summary":" In this paper, we formulate keyphrase<br>extraction from scholarly articles as a sequence labeling<br>task solved using a BiLSTM-CRF, where the words in<br>the input text are represented using deep<br>contextualized embeddings. We evaluate the proposed<br>architecture using both contextualized and fixed word<br>embedding models on three different benchmark datasets,<br>and compare with existing popular unsupervised<br>and supervised techniques. Our results quantify<br>the benefits of: (a) using contextualized<br>embeddings over fixed word embeddings; (b) using a<br>BiLSTM-CRF architecture with contextualized word<br>embeddings over fine-tuning the contextualized<br>embedding model directly; and (c) using domain-specific<br>contextualized embeddings (SciBERT). Through error<br>analysis, we...","title_summary":" Keyphrase Extraction as Sequence Labeling<br>Using Contextualized Embeddings","x":-34.197013855,"y":32.6094551086,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.197013855,"tsne_y":32.6094551086,"subcluster":19,"subcluster_description":"Select Important Context Words","shape":"p"},{"cord_uid":"ef481p8k","source_x":"PMC","title":"PMD: An Optimal Transportation-Based User Distance for Recommender Systems","doi":"10.1007\/978-3-030-45442-5_34","abstract":"Collaborative filtering predicts a user\u2019s preferences by aggregating ratings from similar users and thus the user similarity (or distance) measure is key to good performance. Existing similarity measures either consider only the co-rated items for a pair of users (but co-rated items are rare in real-world sparse datasets), or try to utilize the non-co-rated items via some heuristics. We propose a novel user distance measure, called Preference Mover\u2019s Distance (PMD), based on the optimal transportation theory. PMD exploits all ratings made by each user and works even if users do not share co-rated items at all. In addition, PMD is a metric and has favorable properties such as triangle inequality and zero self-distance. Experimental results show that PMD achieves superior recommendation accuracy compared with the state-of-the-art similarity measures, especially on highly sparse datasets.","publish_time":1585008000000,"author_summary":" Meng, Yitong; Dai, Xinyan; Yan, Xiao; Cheng,<br>James; Liu, Weiwen; Guo, Jun; Liao, Benben; Chen,<br>Guangyong","abstract_summary":" Collaborative filtering predicts a user\u2019s<br>preferences by aggregating ratings from similar users and<br>thus the user similarity (or distance) measure is<br>key to good performance. Existing similarity<br>measures either consider only the co-rated items for a<br>pair of users (but co-rated items are rare in<br>real-world sparse datasets), or try to utilize the<br>non-co-rated items via some heuristics. We propose a novel<br>user distance measure, called Preference Mover\u2019s<br>Distance (PMD), based on the optimal transportation<br>theory. PMD exploits all ratings made by each user and<br>works even if users do not share co-rated items at all.<br>In addition, PMD is...","title_summary":" PMD: An Optimal Transportation-Based User<br>Distance for Recommender Systems","x":-30.8335399628,"y":34.1979675293,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8335399628,"tsne_y":34.1979675293,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"aeyl8mj4","source_x":"PMC","title":"BiOnt: Deep Learning Using Multiple Biomedical Ontologies for Relation Extraction","doi":"10.1007\/978-3-030-45442-5_46","abstract":"Successful biomedical relation extraction can provide evidence to researchers and clinicians about possible unknown associations between biomedical entities, advancing the current knowledge we have about those entities and their inherent mechanisms. Most biomedical relation extraction systems do not resort to external sources of knowledge, such as domain-specific ontologies. However, using deep learning methods, along with biomedical ontologies, has been recently shown to effectively advance the biomedical relation extraction field. To perform relation extraction, our deep learning system, BiOnt, employs four types of biomedical ontologies, namely, the Gene Ontology, the Human Phenotype Ontology, the Human Disease Ontology, and the Chemical Entities of Biological Interest, regarding gene-products, phenotypes, diseases, and chemical compounds, respectively. We tested our system with three data sets that represent three different types of relations of biomedical entities. BiOnt achieved, in F-score, an improvement of 4.93% points for drug-drug interactions (DDI corpus), 4.99% points for phenotype-gene relations (PGR corpus), and 2.21% points for chemical-induced disease relations (BC5CDR corpus), relatively to the state-of-the-art. The code supporting this system is available at https:\/\/github.com\/lasigeBioTM\/BiONT.","publish_time":1585008000000,"author_summary":" Sousa, Diana; Couto, Francisco M.","abstract_summary":" Successful biomedical relation extraction<br>can provide evidence to researchers and<br>clinicians about possible unknown associations between<br>biomedical entities, advancing the current knowledge we<br>have about those entities and their inherent<br>mechanisms. Most biomedical relation extraction systems<br>do not resort to external sources of knowledge,<br>such as domain-specific ontologies. However,<br>using deep learning methods, along with biomedical<br>ontologies, has been recently shown to effectively advance<br>the biomedical relation extraction field. To<br>perform relation extraction, our deep learning<br>system, BiOnt, employs four types of biomedical<br>ontologies, namely, the Gene Ontology, the Human Phenotype<br>Ontology, the Human Disease Ontology, and the Chemical<br>Entities...","title_summary":" BiOnt: Deep Learning Using Multiple<br>Biomedical Ontologies for Relation Extraction","x":-31.6213302612,"y":32.0445976257,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.6213302612,"tsne_y":32.0445976257,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"97xmu329","source_x":"PMC","title":"Irony Detection in a Multilingual Context","doi":"10.1007\/978-3-030-45442-5_18","abstract":"This paper proposes the first multilingual (French, English and Arabic) and multicultural (Indo-European languages vs. less culturally close languages) irony detection system. We employ both feature-based models and neural architectures using monolingual word representation. We compare the performance of these systems with state-of-the-art systems to identify their capabilities. We show that these monolingual models trained separately on different languages using multilingual word representation or text-based features can open the door to irony detection in languages that lack of annotated data for irony.","publish_time":1585008000000,"author_summary":" Ghanem, Bilal; Karoui, Jihen; Benamara,<br>Farah; Rosso, Paolo; Moriceau, V\u00e9ronique","abstract_summary":" This paper proposes the first multilingual<br>(French, English and Arabic) and multicultural<br>(Indo-European languages vs. less culturally close<br>languages) irony detection system. We employ both<br>feature-based models and neural architectures using<br>monolingual word representation. We compare the<br>performance of these systems with state-of-the-art<br>systems to identify their capabilities. We show that<br>these monolingual models trained separately on<br>different languages using multilingual word<br>representation or text-based features can open the door to<br>irony detection in languages that lack of annotated<br>data for irony.","title_summary":" Irony Detection in a Multilingual Context","x":-35.1101608276,"y":31.3182201385,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.1101608276,"tsne_y":31.3182201385,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"xyqzp4h1","source_x":"PMC","title":"A Latent Model for Ad Hoc Table Retrieval","doi":"10.1007\/978-3-030-45442-5_11","abstract":"The ad hoc table retrieval task is concerned with satisfying a query with a ranked list of tables. While there are strong baselines in the literature that exploit learning to rank and semantic matching techniques, there are still a set of hard queries that are difficult for these baseline methods to address. We find that such hard queries are those whose constituting tokens (i.e., terms or entities) are not fully or partially observed in the relevant tables. We focus on proposing a latent factor model to address such hard queries. Our proposed model factorizes the token-table co-occurrence matrix into two low dimensional latent factor matrices that can be used for measuring table and query similarity even if no shared tokens exist between them. We find that the variation of our proposed model that considers keywords provides statistically significant improvement over three strong baselines in terms of NDCG and ERR.","publish_time":1585008000000,"author_summary":" Bagheri, Ebrahim; Al-Obeidat, Feras","abstract_summary":" The ad hoc table retrieval task is concerned<br>with satisfying a query with a ranked list of tables.<br>While there are strong baselines in the literature<br>that exploit learning to rank and semantic matching<br>techniques, there are still a set of hard queries that are<br>difficult for these baseline methods to address. We find<br>that such hard queries are those whose constituting<br>tokens (i.e., terms or entities) are not fully or<br>partially observed in the relevant tables. We focus on<br>proposing a latent factor model to address such hard<br>queries. Our proposed model factorizes the token-table<br>co-occurrence matrix into two...","title_summary":" A Latent Model for Ad Hoc Table Retrieval","x":-33.4270515442,"y":34.2061424255,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4270515442,"tsne_y":34.2061424255,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"iavwkdpr","source_x":"PMC","title":"ChEMU: Named Entity Recognition and Event Extraction of Chemical Reactions from Patents","doi":"10.1007\/978-3-030-45442-5_74","abstract":"We introduce a new evaluation lab named ChEMU (Cheminformatics Elsevier Melbourne University), part of the 11th Conference and Labs of the Evaluation Forum (CLEF-2020). ChEMU involves two key information extraction tasks over chemical reactions from patents. Task 1\u2014Named entity recognition\u2014involves identifying chemical compounds as well as their types in context, i.e., to assign the label of a chemical compound according to the role which the compound plays within a chemical reaction. Task 2\u2014Event extraction over chemical reactions\u2014involves event trigger detection and argument recognition. We briefly present the motivations and goals of the ChEMU tasks, as well as resources and evaluation methodology.","publish_time":1585008000000,"author_summary":" Nguyen, Dat Quoc; Zhai, Zenan; Yoshikawa,<br>Hiyori; Fang, Biaoyan; Druckenbrodt, Christian;<br>Thorne, Camilo; Hoessel, Ralph; Akhondi, Saber A.;<br>Cohn, Trevor; Baldwin, Timothy; Verspoor, Karin","abstract_summary":" We introduce a new evaluation lab named ChEMU<br>(Cheminformatics Elsevier Melbourne University), part of the<br>11th Conference and Labs of the Evaluation Forum<br>(CLEF-2020). ChEMU involves two key information extraction<br>tasks over chemical reactions from patents. Task<br>1\u2014Named entity recognition\u2014involves identifying<br>chemical compounds as well as their types in context,<br>i.e., to assign the label of a chemical compound<br>according to the role which the compound plays within a<br>chemical reaction. Task 2\u2014Event extraction over<br>chemical reactions\u2014involves event trigger detection<br>and argument recognition. We briefly present the<br>motivations and goals of the ChEMU tasks, as well as<br>resources and evaluation...","title_summary":" ChEMU: Named Entity Recognition and Event<br>Extraction of Chemical Reactions from Patents","x":-32.5574264526,"y":32.6479759216,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.5574264526,"tsne_y":32.6479759216,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"842taxsz","source_x":"PMC","title":"Army ANT: A Workbench for Innovation in Entity-Oriented Search","doi":"10.1007\/978-3-030-45442-5_56","abstract":"As entity-oriented search takes the lead in modern search, the need for increasingly flexible tools, capable of motivating innovation in information retrieval research, also becomes more evident. Army ANT is an open source framework that takes a step forward in generalizing information retrieval research, so that modern approaches can be easily integrated in a shared evaluation environment. We present an overview on the system architecture of Army ANT, which has four main abstractions: (i) readers, to iterate over text collections, potentially containing associated entities and triples; (ii) engines, that implement indexing and searching approaches, supporting different retrieval tasks and ranking functions; (iii) databases, to store additional document metadata; and (iv) evaluators, to assess retrieval performance for specific tasks and test collections. We also introduce the command line interface and the web interface, presenting a learn mode as a way to explore, analyze and understand representation and retrieval models, through tracing, score component visualization and documentation.","publish_time":1585008000000,"author_summary":" Devezas, Jos\u00e9; Nunes, S\u00e9rgio","abstract_summary":" As entity-oriented search takes the lead in<br>modern search, the need for increasingly flexible<br>tools, capable of motivating innovation in<br>information retrieval research, also becomes more<br>evident. Army ANT is an open source framework that takes a<br>step forward in generalizing information<br>retrieval research, so that modern approaches can be<br>easily integrated in a shared evaluation<br>environment. We present an overview on the system<br>architecture of Army ANT, which has four main abstractions:<br>(i) readers, to iterate over text collections,<br>potentially containing associated entities and triples;<br>(ii) engines, that implement indexing and<br>searching approaches, supporting different retrieval<br>tasks and ranking...","title_summary":" Army ANT: A Workbench for Innovation in<br>Entity-Oriented Search","x":-33.796672821,"y":35.3753433228,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.796672821,"tsne_y":35.3753433228,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"ch0fg9rp","source_x":"PMC","title":"From MAXSCORE to Block-Max Wand: The Story of How Lucene Significantly Improved Query Evaluation Performance","doi":"10.1007\/978-3-030-45442-5_3","abstract":"The latest major release of Lucene (version 8) in March 2019 incorporates block-max indexes and exploits the block-max variant of Wand for query evaluation, which are innovations that originated from academia. This paper shares the story of how this came to be, which provides an interesting case study at the intersection of reproducibility and academic research achieving impact in the \u201creal world\u201d. We offer additional thoughts on the often idiosyncratic processes by which academic research makes its way into deployed solutions.","publish_time":1585008000000,"author_summary":" Grand, Adrien; Muir, Robert; Ferenczi, Jim;<br>Lin, Jimmy","abstract_summary":" The latest major release of Lucene (version 8)<br>in March 2019 incorporates block-max indexes and<br>exploits the block-max variant of Wand for query<br>evaluation, which are innovations that originated from<br>academia. This paper shares the story of how this came to<br>be, which provides an interesting case study at the<br>intersection of reproducibility and academic research<br>achieving impact in the \u201creal world\u201d. We offer additional<br>thoughts on the often idiosyncratic processes by which<br>academic research makes its way into deployed<br>solutions.","title_summary":" From MAXSCORE to Block-Max Wand: The Story of<br>How Lucene Significantly Improved Query<br>Evaluation Performance","x":-34.440864563,"y":34.2318305969,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.440864563,"tsne_y":34.2318305969,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"n9v5ln2i","source_x":"PMC","title":"Text-Image-Video Summary Generation Using Joint Integer Linear Programming","doi":"10.1007\/978-3-030-45442-5_24","abstract":"Automatically generating a summary for asynchronous data can help users to keep up with the rapid growth of multi-modal information on the Internet. However, the current multi-modal systems usually generate summaries composed of text and images. In this paper, we propose a novel research problem of text-image-video summary generation (TIVS). We first develop a multi-modal dataset containing text documents, images and videos. We then propose a novel joint integer linear programming multi-modal summarization (JILP-MMS) framework. We report the performance of our model on the developed dataset.","publish_time":1585008000000,"author_summary":" Jangra, Anubhav; Jatowt, Adam; Hasanuzzaman,<br>Mohammad; Saha, Sriparna","abstract_summary":" Automatically generating a summary for<br>asynchronous data can help users to keep up with the rapid<br>growth of multi-modal information on the Internet.<br>However, the current multi-modal systems usually<br>generate summaries composed of text and images. In this<br>paper, we propose a novel research problem of<br>text-image-video summary generation (TIVS). We first develop a<br>multi-modal dataset containing text documents, images and<br>videos. We then propose a novel joint integer linear<br>programming multi-modal summarization (JILP-MMS)<br>framework. We report the performance of our model on the<br>developed dataset.","title_summary":" Text-Image-Video Summary Generation Using<br>Joint Integer Linear Programming","x":-33.0636405945,"y":33.5485153198,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.0636405945,"tsne_y":33.5485153198,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"gt3xayqp","source_x":"PMC","title":"The Unfairness of Popularity Bias in Music Recommendation: A Reproducibility Study","doi":"10.1007\/978-3-030-45442-5_5","abstract":"Research has shown that recommender systems are typically biased towards popular items, which leads to less popular items being underrepresented in recommendations. The recent work of Abdollahpouri et al. in the context of movie recommendations has shown that this popularity bias leads to unfair treatment of both long-tail items as well as users with little interest in popular items. In this paper, we reproduce the analyses of Abdollahpouri et al. in the context of music recommendation. Specifically, we investigate three user groups from the Last.fm music platform that are categorized based on how much their listening preferences deviate from the most popular music among all Last.fm users in the dataset: (i) low-mainstream users, (ii) medium-mainstream users, and (iii) high-mainstream users. In line with Abdollahpouri et al., we find that state-of-the-art recommendation algorithms favor popular items also in the music domain. However, their proposed Group Average Popularity metric yields different results for Last.fm than for the movie domain, presumably due to the larger number of available items (i.e., music artists) in the Last.fm dataset we use. Finally, we compare the accuracy results of the recommendation algorithms for the three user groups and find that the low-mainstreaminess group significantly receives the worst recommendations.","publish_time":1585008000000,"author_summary":" Kowald, Dominik; Schedl, Markus; Lex,<br>Elisabeth","abstract_summary":" Research has shown that recommender systems<br>are typically biased towards popular items, which<br>leads to less popular items being underrepresented<br>in recommendations. The recent work of<br>Abdollahpouri et al. in the context of movie recommendations<br>has shown that this popularity bias leads to unfair<br>treatment of both long-tail items as well as users with<br>little interest in popular items. In this paper, we<br>reproduce the analyses of Abdollahpouri et al. in the<br>context of music recommendation. Specifically, we<br>investigate three user groups from the Last.fm music<br>platform that are categorized based on how much their<br>listening preferences deviate from the...","title_summary":" The Unfairness of Popularity Bias in Music<br>Recommendation: A Reproducibility Study","x":-30.8106689453,"y":34.0677375793,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8106689453,"tsne_y":34.0677375793,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"3ed7g5m5","source_x":"PMC","title":"Touch\u00e9: First Shared Task on Argument Retrieval","doi":"10.1007\/978-3-030-45442-5_67","abstract":"Technologies for argument mining and argumentation processing are maturing continuously, giving rise to the idea of retrieving arguments in search scenarios. We introduce Touch\u00e9, the first lab on Argument Retrieval featuring two subtasks: (1) the retrieval of arguments from a focused debate collection to support argumentative conversations, and (2) the retrieval of arguments from a generic web crawl to answer comparative questions with argumentative results. The goal of this lab is to perform an evaluation of various strategies to retrieve argumentative information from the web content. In this paper, we describe the setting of each subtask: the motivation, the data, and the evaluation methodology.","publish_time":1585008000000,"author_summary":" Bondarenko, Alexander; Hagen, Matthias;<br>Potthast, Martin; Wachsmuth, Henning; Beloucif,<br>Meriem; Biemann, Chris; Panchenko, Alexander; Stein,<br>Benno","abstract_summary":" Technologies for argument mining and<br>argumentation processing are maturing continuously, giving<br>rise to the idea of retrieving arguments in search<br>scenarios. We introduce Touch\u00e9, the first lab on Argument<br>Retrieval featuring two subtasks: (1) the retrieval of<br>arguments from a focused debate collection to support<br>argumentative conversations, and (2) the retrieval of<br>arguments from a generic web crawl to answer comparative<br>questions with argumentative results. The goal of this<br>lab is to perform an evaluation of various<br>strategies to retrieve argumentative information from<br>the web content. In this paper, we describe the<br>setting of each subtask: the motivation, the data,...","title_summary":" Touch\u00e9: First Shared Task on Argument<br>Retrieval","x":-34.7431297302,"y":34.3623504639,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.7431297302,"tsne_y":34.3623504639,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"40f2p3t4","source_x":"PMC","title":"Neural-IR-Explorer: A Content-Focused Tool to Explore Neural Re-ranking Results","doi":"10.1007\/978-3-030-45442-5_58","abstract":"In this paper we look beyond metrics-based evaluation of Information Retrieval systems, to explore the reasons behind ranking results. We present the content-focused Neural-IR-Explorer, which empowers users to browse through retrieval results and inspect the inner workings and fine-grained results of neural re-ranking models. The explorer includes a categorized overview of the available queries, as well as an individual query result view with various options to highlight semantic connections between query-document pairs. The Neural-IR-Explorer is available at: https:\/\/neural-ir-explorer.ec.tuwien.ac.at\/.","publish_time":1585008000000,"author_summary":" Hofst\u00e4tter, Sebastian; Zlabinger, Markus;<br>Hanbury, Allan","abstract_summary":" In this paper we look beyond metrics-based<br>evaluation of Information Retrieval systems, to explore<br>the reasons behind ranking results. We present the<br>content-focused Neural-IR-Explorer, which empowers users to<br>browse through retrieval results and inspect the<br>inner workings and fine-grained results of neural<br>re-ranking models. The explorer includes a categorized<br>overview of the available queries, as well as an<br>individual query result view with various options to<br>highlight semantic connections between query-document<br>pairs. The Neural-IR-Explorer is available at:<br>https:\/\/neural-ir-explorer.ec.tuwien.ac.at\/.","title_summary":" Neural-IR-Explorer: A Content-Focused Tool<br>to Explore Neural Re-ranking Results","x":-33.8816604614,"y":34.0143432617,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8816604614,"tsne_y":34.0143432617,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"xpdy84zc","source_x":"PMC","title":"Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions","doi":"10.1007\/978-3-030-45442-5_53","abstract":"Graph embedding has attracted many research interests. Existing works mainly focus on static homogeneous\/heterogeneous networks or dynamic homogeneous networks. However, dynamic heterogeneous networks are more ubiquitous in reality, e.g. social network, e-commerce network, citation network, etc. There is still a lack of research on dynamic heterogeneous graph embedding. In this paper, we propose a novel dynamic heterogeneous graph embedding method using hierarchical attentions (DyHAN) that learns node embeddings leveraging both structural heterogeneity and temporal evolution. We evaluate our method on three real-world datasets. The results show that DyHAN outperforms various state-of-the-art baselines in terms of link prediction task.","publish_time":1585008000000,"author_summary":" Yang, Luwei; Xiao, Zhibo; Jiang, Wen; Wei, Yi;<br>Hu, Yi; Wang, Hao","abstract_summary":" Graph embedding has attracted many research<br>interests. Existing works mainly focus on static<br>homogeneous\/heterogeneous networks or dynamic homogeneous networks.<br>However, dynamic heterogeneous networks are more<br>ubiquitous in reality, e.g. social network, e-commerce<br>network, citation network, etc. There is still a lack of<br>research on dynamic heterogeneous graph embedding. In<br>this paper, we propose a novel dynamic<br>heterogeneous graph embedding method using hierarchical<br>attentions (DyHAN) that learns node embeddings<br>leveraging both structural heterogeneity and temporal<br>evolution. We evaluate our method on three real-world<br>datasets. The results show that DyHAN outperforms<br>various state-of-the-art baselines in terms of link<br>prediction task.","title_summary":" Dynamic Heterogeneous Graph Embedding Using<br>Hierarchical Attentions","x":-31.2404384613,"y":35.6044311523,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2404384613,"tsne_y":35.6044311523,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"cq4lbd0l","source_x":"PMC","title":"Calling Attention to Passages for Biomedical Question Answering","doi":"10.1007\/978-3-030-45442-5_9","abstract":"Question answering can be described as retrieving relevant information for questions expressed in natural language, possibly also generating a natural language answer. This paper presents a pipeline for document and passage retrieval for biomedical question answering built around a new variant of the DeepRank network model in which the recursive layer is replaced by a self-attention layer combined with a weighting mechanism. This adaptation halves the total number of parameters and makes the network more suited for identifying the relevant passages in each document. The overall retrieval system was evaluated on the BioASQ tasks 6 and 7, achieving similar retrieval performance when compared to more complex network architectures.","publish_time":1585008000000,"author_summary":" Almeida, Tiago; Matos, S\u00e9rgio","abstract_summary":" Question answering can be described as<br>retrieving relevant information for questions expressed<br>in natural language, possibly also generating a<br>natural language answer. This paper presents a<br>pipeline for document and passage retrieval for<br>biomedical question answering built around a new variant<br>of the DeepRank network model in which the<br>recursive layer is replaced by a self-attention layer<br>combined with a weighting mechanism. This adaptation<br>halves the total number of parameters and makes the<br>network more suited for identifying the relevant<br>passages in each document. The overall retrieval system<br>was evaluated on the BioASQ tasks 6 and 7, achieving<br>similar retrieval...","title_summary":" Calling Attention to Passages for Biomedical<br>Question Answering","x":-34.6816215515,"y":33.0781555176,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6816215515,"tsne_y":33.0781555176,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"5e6g5908","source_x":"PMC","title":"Aspect-Based Academic Search Using Domain-Specific KB","doi":"10.1007\/978-3-030-45442-5_52","abstract":"Academic search engines allow scientists to explore related work relevant to a given query. Often, the user is also aware of the aspect to retrieve a relevant document. In such cases, existing search engines can be used by expanding the query with terms describing that aspect. However, this approach does not guarantee good results since plain keyword matches do not always imply relevance. To address this issue, we define and solve a novel academic search task, called aspect-based retrieval, which allows the user to specify the aspect along with the query to retrieve a ranked list of relevant documents. The primary idea is to estimate a language model for the aspect as well as the query using a domain-specific knowledge base and use a mixture of the two to determine the relevance of the article. Our evaluation of the results over the Open Research Corpus dataset shows that our method outperforms keyword-based expansion of query with aspect with and without relevance feedback.","publish_time":1585008000000,"author_summary":" Upadhyay, Prajna; Bedathur, Srikanta;<br>Chakraborty, Tanmoy; Ramanath, Maya","abstract_summary":" Academic search engines allow scientists to<br>explore related work relevant to a given query. Often,<br>the user is also aware of the aspect to retrieve a<br>relevant document. In such cases, existing search<br>engines can be used by expanding the query with terms<br>describing that aspect. However, this approach does not<br>guarantee good results since plain keyword matches do not<br>always imply relevance. To address this issue, we<br>define and solve a novel academic search task, called<br>aspect-based retrieval, which allows the user to specify the<br>aspect along with the query to retrieve a ranked list of<br>relevant documents. The...","title_summary":" Aspect-Based Academic Search Using<br>Domain-Specific KB","x":-33.7734870911,"y":34.605796814,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7734870911,"tsne_y":34.605796814,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"7ryitk6y","source_x":"PMC","title":"The 3[Formula: see text] International Workshop on Narrative Extraction from Texts: Text2Story 2020","doi":"10.1007\/978-3-030-45442-5_86","abstract":"The Third International Workshop on Narrative Extraction from Texts (Text2Story\u201920) [text2story20.inesctec.pt] held in conjunction with the 42[Formula: see text] European Conference on Information Retrieval (ECIR 2020) gives researchers of IR, NLP and other fields, the opportunity to share their recent advances in extraction and formal representation of narratives. This workshop also presents a forum to consolidate the multi-disciplinary efforts and foster discussions around the narrative extraction task, a hot topic in recent years.","publish_time":1585008000000,"author_summary":" Campos, Ricardo; Jorge, Al\u00ed\u00adpio; Jatowt,<br>Adam; Bhatia, Sumit","abstract_summary":" The Third International Workshop on Narrative<br>Extraction from Texts (Text2Story\u201920)<br>[text2story20.inesctec.pt] held in conjunction with the 42[Formula: see<br>text] European Conference on Information Retrieval<br>(ECIR 2020) gives researchers of IR, NLP and other<br>fields, the opportunity to share their recent advances<br>in extraction and formal representation of<br>narratives. This workshop also presents a forum to<br>consolidate the multi-disciplinary efforts and foster<br>discussions around the narrative extraction task, a hot<br>topic in recent years.","title_summary":" The 3[Formula: see text] International<br>Workshop on Narrative Extraction from Texts:<br>Text2Story 2020","x":-34.5245132446,"y":31.0953941345,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.5245132446,"tsne_y":31.0953941345,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"mfh1ope6","source_x":"PMC","title":"DSR: A Collection for the Evaluation of Graded Disease-Symptom Relations","doi":"10.1007\/978-3-030-45442-5_54","abstract":"The effective extraction of ranked disease-symptom relationships is a critical component in various medical tasks, including computer-assisted medical diagnosis or the discovery of unexpected associations between diseases. While existing disease-symptom relationship extraction methods are used as the foundation in the various medical tasks, no collection is available to systematically evaluate the performance of such methods. In this paper, we introduce the Disease-Symptom Relation Collection (dsr-collection), created by five physicians as expert annotators. We provide graded symptom judgments for diseases by differentiating between relevant symptoms and primary symptoms. Further, we provide several strong baselines, based on the methods used in previous studies. The first method is based on word embeddings, and the second on co-occurrences of MeSH-keywords of medical articles. For the co-occurrence method, we propose an adaption in which not only keywords are considered, but also the full text of medical articles. The evaluation on the dsr-collection shows the effectiveness of the proposed adaption in terms of nDCG, precision, and recall.","publish_time":1585008000000,"author_summary":" Zlabinger, Markus; Hofst\u00e4tter, Sebastian;<br>Rekabsaz, Navid; Hanbury, Allan","abstract_summary":" The effective extraction of ranked<br>disease-symptom relationships is a critical component in<br>various medical tasks, including computer-assisted<br>medical diagnosis or the discovery of unexpected<br>associations between diseases. While existing<br>disease-symptom relationship extraction methods are used as<br>the foundation in the various medical tasks, no<br>collection is available to systematically evaluate the<br>performance of such methods. In this paper, we introduce the<br>Disease-Symptom Relation Collection (dsr-collection),<br>created by five physicians as expert annotators. We<br>provide graded symptom judgments for diseases by<br>differentiating between relevant symptoms and primary<br>symptoms. Further, we provide several strong baselines,<br>based on the methods used in previous...","title_summary":" DSR: A Collection for the Evaluation of Graded<br>Disease-Symptom Relations","x":-32.0261306763,"y":32.0869522095,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.0261306763,"tsne_y":32.0869522095,"subcluster":9,"subcluster_description":"Large-Scale Biomedical Semantic Indexing","shape":"p"},{"cord_uid":"f3l3h7zq","source_x":"PMC","title":"On the Temporality of Priors in Entity Linking","doi":"10.1007\/978-3-030-45442-5_47","abstract":"Entity linking is a fundamental task in natural language processing which deals with the lexical ambiguity in texts. An important component in entity linking approaches is the mention-to-entity prior probability. Even though there is a large number of works in entity linking, the existing approaches do not explicitly consider the time aspect, specifically the temporality of an entity\u2019s prior probability. We posit that this prior probability is temporal in nature and affects the performance of entity linking systems. In this paper we systematically study the effect of the prior on the entity linking performance over the temporal validity of both texts and KBs.","publish_time":1585008000000,"author_summary":" Stoffalette Jo\u00e3o, Renato","abstract_summary":" Entity linking is a fundamental task in natural<br>language processing which deals with the lexical<br>ambiguity in texts. An important component in entity<br>linking approaches is the mention-to-entity prior<br>probability. Even though there is a large number of works in<br>entity linking, the existing approaches do not<br>explicitly consider the time aspect, specifically the<br>temporality of an entity\u2019s prior probability. We posit that<br>this prior probability is temporal in nature and<br>affects the performance of entity linking systems. In<br>this paper we systematically study the effect of the<br>prior on the entity linking performance over the<br>temporal validity of both...","title_summary":" On the Temporality of Priors in Entity Linking","x":-33.7145347595,"y":34.1710929871,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7145347595,"tsne_y":34.1710929871,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"i71qcmik","source_x":"PMC","title":"Personal Research Assistant for Online Exploration of Historical News","doi":"10.1007\/978-3-030-45442-5_62","abstract":"We present a novel environment for exploratory search in large collections of historical newspapers developed as a part of the NewsEye project. In this paper we focus on the intelligent Personal Research Assistant (PRA) component in the environment and the web interface. The PRA is an interactive exploratory engine that combines results of various text analysis tools in an unsupervised fashion to conduct autonomous investigations on the data according to users\u2019 needs. The PRA is freely available online together with some datasets of European historical newspapers. The methods used by the assistant are of potential benefit to other exploratory search applications.","publish_time":1585008000000,"author_summary":" Pivovarova, Lidia; Jean-Caurant, Axel;<br>Avikainen, Jari; Alnajjar, Khalid; Granroth-Wilding,<br>Mark; Lepp\u00e4nen, Leo; Zosa, Elaine; Toivonen, Hannu","abstract_summary":" We present a novel environment for exploratory<br>search in large collections of historical newspapers<br>developed as a part of the NewsEye project. In this paper we<br>focus on the intelligent Personal Research<br>Assistant (PRA) component in the environment and the web<br>interface. The PRA is an interactive exploratory engine<br>that combines results of various text analysis<br>tools in an unsupervised fashion to conduct<br>autonomous investigations on the data according to users\u2019<br>needs. The PRA is freely available online together<br>with some datasets of European historical<br>newspapers. The methods used by the assistant are of<br>potential benefit to other exploratory search...","title_summary":" Personal Research Assistant for Online<br>Exploration of Historical News","x":-34.2866592407,"y":29.6166934967,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2866592407,"tsne_y":29.6166934967,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"cqu06lt5","source_x":"PMC","title":"Rethinking Query Expansion for BERT Reranking","doi":"10.1007\/978-3-030-45442-5_37","abstract":"Recent studies have shown promising results of using BERT for Information Retrieval with its advantages in understanding the text content of documents and queries. Compared to short, keywords queries, higher accuracy of BERT were observed on long, natural language queries, demonstrating BERT\u2019s ability in extracting rich information from complex queries. These results show the potential of using query expansion to generate better queries for BERT-based rankers. In this work, we explore BERT\u2019s sensitivity to the addition of structure and concepts. We find that traditional word-based query expansion is not entirely applicable, and provide insight into methods that produce better experimental results.","publish_time":1585008000000,"author_summary":" Padaki, Ramith; Dai, Zhuyun; Callan, Jamie","abstract_summary":" Recent studies have shown promising results of<br>using BERT for Information Retrieval with its<br>advantages in understanding the text content of documents<br>and queries. Compared to short, keywords queries,<br>higher accuracy of BERT were observed on long, natural<br>language queries, demonstrating BERT\u2019s ability in<br>extracting rich information from complex queries. These<br>results show the potential of using query expansion to<br>generate better queries for BERT-based rankers. In this<br>work, we explore BERT\u2019s sensitivity to the addition<br>of structure and concepts. We find that<br>traditional word-based query expansion is not entirely<br>applicable, and provide insight into methods that produce<br>better experimental...","title_summary":" Rethinking Query Expansion for BERT Reranking","x":-34.1085319519,"y":34.1892585754,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1085319519,"tsne_y":34.1892585754,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"qipr65bk","source_x":"PMC","title":"Text Meets Space: Geographic Content Extraction, Resolution and Information Retrieval","doi":"10.1007\/978-3-030-45442-5_89","abstract":"In this half-day tutorial, we will review the basic concepts of, methods for, and applications of geographic information retrieval, also showing some possible applications in fields such as the digital humanities. The tutorial is organized in four parts. First we introduce some basic ideas about geography, and demonstrate why text is a powerful way of exploring relevant questions. We then introduce a basic end-to-end pipeline discussing geographic information in documents, spatial and multi-dimensional indexing [19], and spatial retrieval and spatial filtering. After showing a range of possible applications, we conclude with suggestions for future work in the area.","publish_time":1585008000000,"author_summary":" Leidner, Jochen L.; Martins, Bruno;<br>McDonough, Katherine; Purves, Ross S.","abstract_summary":" In this half-day tutorial, we will review the<br>basic concepts of, methods for, and applications of<br>geographic information retrieval, also showing some<br>possible applications in fields such as the digital<br>humanities. The tutorial is organized in four parts. First<br>we introduce some basic ideas about geography,<br>and demonstrate why text is a powerful way of<br>exploring relevant questions. We then introduce a basic<br>end-to-end pipeline discussing geographic information<br>in documents, spatial and multi-dimensional<br>indexing [19], and spatial retrieval and spatial<br>filtering. After showing a range of possible<br>applications, we conclude with suggestions for future work in<br>the area.","title_summary":" Text Meets Space: Geographic Content<br>Extraction, Resolution and Information Retrieval","x":-34.0717353821,"y":34.807434082,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.0717353821,"tsne_y":34.807434082,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"lh8dicf4","source_x":"PMC","title":"Introducing the CLEF 2020 HIPE Shared Task: Named Entity Recognition and Linking on Historical Newspapers","doi":"10.1007\/978-3-030-45442-5_68","abstract":"Since its introduction some twenty years ago, named entity (NE) processing has become an essential component of virtually any text mining application and has undergone major changes. Recently, two main trends characterise its developments: the adoption of deep learning architectures and the consideration of textual material originating from historical and cultural heritage collections. While the former opens up new opportunities, the latter introduces new challenges with heterogeneous, historical and noisy inputs. If NE processing tools are increasingly being used in the context of historical documents, performance values are below the ones on contemporary data and are hardly comparable. In this context, this paper introduces the CLEF 2020 Evaluation Lab HIPE (Identifying Historical People, Places and other Entities) on named entity recognition and linking on diachronic historical newspaper material in French, German and English. Our objective is threefold: strengthening the robustness of existing approaches on non-standard inputs, enabling performance comparison of NE processing on historical texts, and, in the long run, fostering efficient semantic indexing of historical documents in order to support scholarship on digital cultural heritage collections.","publish_time":1585008000000,"author_summary":" Ehrmann, Maud; Romanello, Matteo; Bircher,<br>Stefan; Clematide, Simon","abstract_summary":" Since its introduction some twenty years ago,<br>named entity (NE) processing has become an essential<br>component of virtually any text mining application and<br>has undergone major changes. Recently, two main<br>trends characterise its developments: the adoption<br>of deep learning architectures and the<br>consideration of textual material originating from<br>historical and cultural heritage collections. While the<br>former opens up new opportunities, the latter<br>introduces new challenges with heterogeneous,<br>historical and noisy inputs. If NE processing tools are<br>increasingly being used in the context of historical<br>documents, performance values are below the ones on<br>contemporary data and are hardly comparable. In this...","title_summary":" Introducing the CLEF 2020 HIPE Shared Task:<br>Named Entity Recognition and Linking on Historical<br>Newspapers","x":-33.4870109558,"y":32.4977111816,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4870109558,"tsne_y":32.4977111816,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"lin7ya2l","source_x":"PMC","title":"Easing Legal News Monitoring with Learning to Rank and BERT","doi":"10.1007\/978-3-030-45442-5_42","abstract":"While ranking approaches have made rapid advances in the Web search, systems that cater to the complex information needs in professional search tasks are not widely developed, common issues and solutions typically rely on dedicated search strategies backed by ad-hoc retrieval models. In this paper we present a legal search problem where professionals monitor news articles with constant queries on a periodic basis. Firstly, we demonstrate the effectiveness of using traditional retrieval models against the Boolean search of documents in chronological order. In an attempt to capture the complex information needs of users, a learning to rank approach is adopted with user specified relevance criteria as features. This approach, however, only achieves mediocre results compared to the traditional models. However, we find that by fine-tuning a contextualised language model (e.g. BERT), significantly improved retrieval performance can be achieved, providing a flexible solution to satisfying complex information needs without explicit feature engineering.","publish_time":1585008000000,"author_summary":" Sanchez, Luis; He, Jiyin; Manotumruksa,<br>Jarana; Albakour, Dyaa; Martinez, Miguel; Lipani,<br>Aldo","abstract_summary":" While ranking approaches have made rapid<br>advances in the Web search, systems that cater to the<br>complex information needs in professional search<br>tasks are not widely developed, common issues and<br>solutions typically rely on dedicated search strategies<br>backed by ad-hoc retrieval models. In this paper we<br>present a legal search problem where professionals<br>monitor news articles with constant queries on a<br>periodic basis. Firstly, we demonstrate the<br>effectiveness of using traditional retrieval models against<br>the Boolean search of documents in chronological<br>order. In an attempt to capture the complex<br>information needs of users, a learning to rank approach is<br>adopted...","title_summary":" Easing Legal News Monitoring with Learning to<br>Rank and BERT","x":-33.9378204346,"y":34.0112762451,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9378204346,"tsne_y":34.0112762451,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"dsx0vk8n","source_x":"PMC","title":"Semi-supervised Extractive Question Summarization Using Question-Answer Pairs","doi":"10.1007\/978-3-030-45442-5_32","abstract":"Neural extractive summarization methods often require much labeled training data, for which headlines or lead summaries of news articles can sometimes be used. Such directly useful summaries are not always available, however, especially for user-generated content, such as questions posted on community question answering services. In this paper, we address an extractive summarization (i.e., headline extraction) task for such questions as a case study and consider how to alleviate the problem by using question-answer pairs, instead of missing-headline pairs. To this end, we propose a framework to examine how to use such unlabeled paired data from the viewpoint of training methods. Experimental results show that multi-task training performs well with undersampling and distant supervision.","publish_time":1585008000000,"author_summary":" Machida, Kazuya; Ishigaki, Tatsuya;<br>Kobayashi, Hayato; Takamura, Hiroya; Okumura, Manabu","abstract_summary":" Neural extractive summarization methods<br>often require much labeled training data, for which<br>headlines or lead summaries of news articles can<br>sometimes be used. Such directly useful summaries are not<br>always available, however, especially for<br>user-generated content, such as questions posted on community<br>question answering services. In this paper, we address<br>an extractive summarization (i.e., headline<br>extraction) task for such questions as a case study and<br>consider how to alleviate the problem by using<br>question-answer pairs, instead of missing-headline pairs. To<br>this end, we propose a framework to examine how to use<br>such unlabeled paired data from the viewpoint of<br>training...","title_summary":" Semi-supervised Extractive Question<br>Summarization Using Question-Answer Pairs","x":-34.5988388062,"y":32.8820762634,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.5988388062,"tsne_y":32.8820762634,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"bp514m2n","source_x":"PMC","title":"Assessing the Impact of OCR Errors in Information Retrieval","doi":"10.1007\/978-3-030-45442-5_13","abstract":"A significant amount of the textual content available on the Web is stored in PDF files. These files are typically converted into plain text before they can be processed by information retrieval or text mining systems. Automatic conversion typically introduces various errors, especially if OCR is needed. In this empirical study, we simulate OCR errors and investigate the impact that misspelled words have on retrieval accuracy. In order to quantify such impact, errors were systematically inserted at varying rates in an initially clean IR collection. Our results showed that significant impacts are noticed starting at a 5% error rate. Furthermore, stemming has proven to make systems more robust to errors.","publish_time":1585008000000,"author_summary":" Bazzo, Guilherme Torresan; Lorentz, Gustavo<br>Acauan; Suarez Vargas, Danny; Moreira, Viviane P.","abstract_summary":" A significant amount of the textual content<br>available on the Web is stored in PDF files. These files are<br>typically converted into plain text before they can be<br>processed by information retrieval or text mining<br>systems. Automatic conversion typically introduces<br>various errors, especially if OCR is needed. In this<br>empirical study, we simulate OCR errors and investigate<br>the impact that misspelled words have on retrieval<br>accuracy. In order to quantify such impact, errors were<br>systematically inserted at varying rates in an initially clean<br>IR collection. Our results showed that<br>significant impacts are noticed starting at a 5% error rate.<br>Furthermore,...","title_summary":" Assessing the Impact of OCR Errors in<br>Information Retrieval","x":-34.4585838318,"y":34.2729187012,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.4585838318,"tsne_y":34.2729187012,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"75li8upp","source_x":"PMC","title":"Towards a Better Contextualization of Web Contents via Entity-Level Analytics","doi":"10.1007\/978-3-030-45442-5_80","abstract":"With the abundance of data and wide access to the internet, a user can be overwhelmed with information. For an average Web user, it is very difficult to identify which information is relevant or irrelevant. Hence, in the era of continuously enhancing Web, organization and interpretation of Web contents are very important in order to easily access the relevant information. Many recent advancements in the area of Web content management such as classification of Web contents, information diffusion, credibility of information, etc. have been explored based on text and semantic of the document. In this paper, we propose a purely semantic contextualization of Web contents. We hypothesize that named entities and their types present in a Web document convey substantial semantic information. By extraction of this information, we aim to study the reasoning and explanation behind the Web contents or patterns. Furthermore, we also plan to exploit LOD (Linked Open Data) to get a deeper insight of Web contents.","publish_time":1585008000000,"author_summary":" Kumar, Amit","abstract_summary":" With the abundance of data and wide access to the<br>internet, a user can be overwhelmed with information. For<br>an average Web user, it is very difficult to<br>identify which information is relevant or irrelevant.<br>Hence, in the era of continuously enhancing Web,<br>organization and interpretation of Web contents are very<br>important in order to easily access the relevant<br>information. Many recent advancements in the area of Web<br>content management such as classification of Web<br>contents, information diffusion, credibility of<br>information, etc. have been explored based on text and<br>semantic of the document. In this paper, we propose a<br>purely...","title_summary":" Towards a Better Contextualization of Web<br>Contents via Entity-Level Analytics","x":-33.6627731323,"y":35.0616950989,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6627731323,"tsne_y":35.0616950989,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"qaztv02j","source_x":"PMC","title":"International Workshop on Algorithmic Bias in Search and Recommendation (Bias 2020)","doi":"10.1007\/978-3-030-45442-5_84","abstract":"Both search and recommendation algorithms provide results based on their relevance for the current user. In order to do so, such a relevance is usually computed by models trained on historical data, which is biased in most cases. Hence, the results produced by these algorithms naturally propagate, and frequently reinforce, biases hidden in the data, consequently strengthening inequalities. Being able to measure, characterize, and mitigate these biases while keeping high effectiveness is a topic of central interest for the information retrieval community. In this workshop, we aim to collect novel contributions in this emerging field and to provide a common ground for interested researchers and practitioners.","publish_time":1585008000000,"author_summary":" Boratto, Ludovico; Marras, Mirko; Faralli,<br>Stefano; Stilo, Giovanni","abstract_summary":" Both search and recommendation algorithms<br>provide results based on their relevance for the<br>current user. In order to do so, such a relevance is<br>usually computed by models trained on historical data,<br>which is biased in most cases. Hence, the results<br>produced by these algorithms naturally propagate, and<br>frequently reinforce, biases hidden in the data,<br>consequently strengthening inequalities. Being able to<br>measure, characterize, and mitigate these biases while<br>keeping high effectiveness is a topic of central<br>interest for the information retrieval community. In<br>this workshop, we aim to collect novel<br>contributions in this emerging field and to provide a common...","title_summary":" International Workshop on Algorithmic Bias in<br>Search and Recommendation (Bias 2020)","x":-32.2301483154,"y":34.1184844971,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.2301483154,"tsne_y":34.1184844971,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"x67zc8so","source_x":"PMC","title":"Neural Query-Biased Abstractive Summarization Using Copying Mechanism","doi":"10.1007\/978-3-030-45442-5_22","abstract":"This paper deals with the query-biased summarization task. Conventional non-neural network-based approaches have achieved better performance by primarily including the words overlapping between the source and the query in the summary. However, recurrent neural network (RNN)-based approaches do not explicitly model this phenomenon. Therefore, we model an RNN-based query-biased summarizer to primarily include the overlapping words in the summary, using a copying mechanism. Experimental results, in terms of both automatic evaluation with ROUGE and manual evaluation, show that the strategy to include the overlapping words also works well for neural query-biased summarizers.","publish_time":1585008000000,"author_summary":" Ishigaki, Tatsuya; Huang, Hen-Hsen;<br>Takamura, Hiroya; Chen, Hsin-Hsi; Okumura, Manabu","abstract_summary":" This paper deals with the query-biased<br>summarization task. Conventional non-neural network-based<br>approaches have achieved better performance by primarily<br>including the words overlapping between the source and<br>the query in the summary. However, recurrent<br>neural network (RNN)-based approaches do not<br>explicitly model this phenomenon. Therefore, we model an<br>RNN-based query-biased summarizer to primarily include<br>the overlapping words in the summary, using a<br>copying mechanism. Experimental results, in terms of<br>both automatic evaluation with ROUGE and manual<br>evaluation, show that the strategy to include the<br>overlapping words also works well for neural query-biased<br>summarizers.","title_summary":" Neural Query-Biased Abstractive<br>Summarization Using Copying Mechanism","x":-34.1854896545,"y":33.565246582,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1854896545,"tsne_y":33.565246582,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"aste7ylf","source_x":"PMC","title":"A Web-Based Platform for Mining and Ranking Association Rules","doi":"10.1007\/978-3-030-45442-5_55","abstract":"In this demo, we introduce an interactive system, which effectively applies multiple criteria analysis to rank association rules. We first use association rules techniques to explore the correlations between variables in given data (i.e., database and linked data (LD)), and secondly apply multiple criteria analysis (MCA) to select the most relevant rules according to user preferences. The developed system is flexible and allows intuitive creation and execution of different algorithms for an extensive range of advanced data analysis topics. Furthermore, we demonstrate a case study of association rule mining and ranking on road accident data.","publish_time":1585008000000,"author_summary":" Ait-Mlouk, Addi; Jiang, Lili","abstract_summary":" In this demo, we introduce an interactive<br>system, which effectively applies multiple criteria<br>analysis to rank association rules. We first use<br>association rules techniques to explore the correlations<br>between variables in given data (i.e., database and<br>linked data (LD)), and secondly apply multiple<br>criteria analysis (MCA) to select the most relevant<br>rules according to user preferences. The developed<br>system is flexible and allows intuitive creation and<br>execution of different algorithms for an extensive range<br>of advanced data analysis topics. Furthermore,<br>we demonstrate a case study of association rule<br>mining and ranking on road accident data.","title_summary":" A Web-Based Platform for Mining and Ranking<br>Association Rules","x":-33.5139083862,"y":35.6141586304,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5139083862,"tsne_y":35.6141586304,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"8fb8ias9","source_x":"PMC","title":"Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on Generalization","doi":"10.1007\/978-3-030-45442-5_48","abstract":"Contextualized embeddings use unsupervised language model pretraining to compute word representations depending on their context. This is intuitively useful for generalization, especially in Named-Entity Recognition where it is crucial to detect mentions never seen during training. However, standard English benchmarks overestimate the importance of lexical over contextual features because of an unrealistic lexical overlap between train and test mentions. In this paper, we perform an empirical analysis of the generalization capabilities of state-of-the-art contextualized embeddings by separating mentions by novelty and with out-of-domain evaluation. We show that they are particularly beneficial for unseen mentions detection, especially out-of-domain. For models trained on CoNLL03, language model contextualization leads to a +1.2% maximal relative micro-F1 score increase in-domain against +13% out-of-domain on the WNUT dataset (The code is available at https:\/\/github.com\/btaille\/contener).","publish_time":1585008000000,"author_summary":" Taill\u00e9, Bruno; Guigue, Vincent; Gallinari,<br>Patrick","abstract_summary":" Contextualized embeddings use unsupervised<br>language model pretraining to compute word<br>representations depending on their context. This is<br>intuitively useful for generalization, especially in<br>Named-Entity Recognition where it is crucial to detect<br>mentions never seen during training. However, standard<br>English benchmarks overestimate the importance of<br>lexical over contextual features because of an<br>unrealistic lexical overlap between train and test<br>mentions. In this paper, we perform an empirical analysis<br>of the generalization capabilities of<br>state-of-the-art contextualized embeddings by separating<br>mentions by novelty and with out-of-domain evaluation.<br>We show that they are particularly beneficial for<br>unseen mentions detection, especially<br>out-of-domain. For models trained...","title_summary":" Contextualized Embeddings in Named-Entity<br>Recognition: An Empirical Study on Generalization","x":-34.2667655945,"y":32.5335502625,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2667655945,"tsne_y":32.5335502625,"subcluster":19,"subcluster_description":"Select Important Context Words","shape":"p"},{"cord_uid":"agpdokb2","source_x":"PMC","title":"Ranking Significant Discrepancies in Clinical Reports","doi":"10.1007\/978-3-030-45442-5_30","abstract":"Medical errors are a major public health concern and a leading cause of death worldwide. Many healthcare centers and hospitals use reporting systems where medical practitioners write a preliminary medical report and the report is later reviewed, revised, and finalized by a more experienced physician. The revisions range from stylistic to corrections of critical errors or misinterpretations of the case. Due to the large quantity of reports written daily, it is often difficult to manually and thoroughly review all the finalized reports to find such errors and learn from them. To address this challenge, we propose a novel ranking approach, consisting of textual and ontological overlaps between the preliminary and final versions of reports. The approach learns to rank the reports based on the degree of discrepancy between the versions. This allows medical practitioners to easily identify and learn from the reports in which their interpretation most substantially differed from that of the attending physician (who finalized the report). This is a crucial step towards uncovering potential errors and helping medical practitioners to learn from such errors, thus improving patient-care in the long run. We evaluate our model on a dataset of radiology reports and show that our approach outperforms both previously-proposed approaches and more recent language models by 4.5% to 15.4%.","publish_time":1585008000000,"author_summary":" MacAvaney, Sean; Cohan, Arman; Goharian,<br>Nazli; Filice, Ross","abstract_summary":" Medical errors are a major public health<br>concern and a leading cause of death worldwide. Many<br>healthcare centers and hospitals use reporting systems<br>where medical practitioners write a preliminary<br>medical report and the report is later reviewed,<br>revised, and finalized by a more experienced physician.<br>The revisions range from stylistic to corrections<br>of critical errors or misinterpretations of the<br>case. Due to the large quantity of reports written<br>daily, it is often difficult to manually and<br>thoroughly review all the finalized reports to find such<br>errors and learn from them. To address this challenge,<br>we propose a novel ranking approach,...","title_summary":" Ranking Significant Discrepancies in<br>Clinical Reports","x":-31.9495868683,"y":31.9419879913,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.9495868683,"tsne_y":31.9419879913,"subcluster":8,"subcluster_description":"Multi-Label Medical Text Classificationexperiencer","shape":"p"},{"cord_uid":"zovf9rk4","source_x":"PMC","title":"Bibliometric-Enhanced Legal Information Retrieval","doi":"10.1007\/978-3-030-45442-5_83","abstract":"This research project addresses user-focused ranking in legal information retrieval (IR). It studies the perception of relevance of search results for users of Dutch legal IR systems, the employment of usage and citation variables to improve the ranking of search results (bibliometric-enhanced information retrieval), and user-centred evaluation for ranking improvements. The goal of this project is improve the ranking in legal IR systems. Ultimately, this will help legal professionals find relevant information faster.","publish_time":1585008000000,"author_summary":" Wiggers, Gineke","abstract_summary":" This research project addresses user-focused<br>ranking in legal information retrieval (IR). It<br>studies the perception of relevance of search results<br>for users of Dutch legal IR systems, the employment<br>of usage and citation variables to improve the<br>ranking of search results (bibliometric-enhanced<br>information retrieval), and user-centred evaluation for<br>ranking improvements. The goal of this project is<br>improve the ranking in legal IR systems. Ultimately,<br>this will help legal professionals find relevant<br>information faster.","title_summary":" Bibliometric-Enhanced Legal Information<br>Retrieval","x":-34.5652046204,"y":34.9144210815,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.5652046204,"tsne_y":34.9144210815,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"8fipt7c4","source_x":"PMC","title":"Neural Embedding-Based Metrics for Pre-retrieval Query Performance Prediction","doi":"10.1007\/978-3-030-45442-5_10","abstract":"Query Performance Prediction (QPP) is concerned with estimating the effectiveness of a query within the context of a retrieval model. It allows for operations such as query routing and segmentation, leading to improved retrieval performance. Pre-retrieval QPP methods are oblivious to the performance of the retrieval model as they predict query difficulty prior to observing the set of documents retrieved for the query. Since neural embedding-based models are showing wider adoption in the Information Retrieval (IR) community, we propose a set of pre-retrieval QPP metrics based on the properties of pre-trained neural embeddings and show that such metrics are more effective for query performance prediction compared to the widely known QPP metrics such as SCQ, PMI and SCS. We report our findings based on Robust04, ClueWeb09 and Gov2 corpora and their associated TREC topics.","publish_time":1585008000000,"author_summary":" Arabzadeh, Negar; Zarrinkalam, Fattane;<br>Jovanovic, Jelena; Bagheri, Ebrahim","abstract_summary":" Query Performance Prediction (QPP) is<br>concerned with estimating the effectiveness of a query<br>within the context of a retrieval model. It allows for<br>operations such as query routing and segmentation,<br>leading to improved retrieval performance.<br>Pre-retrieval QPP methods are oblivious to the performance of<br>the retrieval model as they predict query<br>difficulty prior to observing the set of documents<br>retrieved for the query. Since neural embedding-based<br>models are showing wider adoption in the Information<br>Retrieval (IR) community, we propose a set of<br>pre-retrieval QPP metrics based on the properties of<br>pre-trained neural embeddings and show that such metrics<br>are more...","title_summary":" Neural Embedding-Based Metrics for<br>Pre-retrieval Query Performance Prediction","x":-33.8241424561,"y":34.0498847961,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8241424561,"tsne_y":34.0498847961,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"d1bykvkg","source_x":"PMC","title":"BioASQ at CLEF2020: Large-Scale Biomedical Semantic Indexing and Question Answering","doi":"10.1007\/978-3-030-45442-5_71","abstract":"This paper describes the eighth edition of the BioASQ Challenge, which will run as an evaluation Lab in the context of CLEF2020. The aim of BioASQ is the promotion of systems and methods for highly precise biomedical information access. This is done through the organization of a series of challenges (shared tasks) on large-scale biomedical semantic indexing and question answering, where different teams develop systems that compete on the same demanding benchmark datasets that represent the real information needs of biomedical experts. In order to facilitate this information finding process, the BioASQ challenge introduced two complementary tasks: (a) the automated indexing of large volumes of unlabelled data, primarily scientific articles, with biomedical concepts, (b) the processing of biomedical questions and the generation of comprehensible answers. Rewarding the most competitive systems that outperform the state of the art, BioASQ manages to push the research frontier towards ensuring that the biomedical experts will have direct access to valuable knowledge.","publish_time":1585008000000,"author_summary":" Krallinger, Martin; Krithara, Anastasia;<br>Nentidis, Anastasios; Paliouras, Georgios; Villegas,<br>Marta","abstract_summary":" This paper describes the eighth edition of the<br>BioASQ Challenge, which will run as an evaluation Lab<br>in the context of CLEF2020. The aim of BioASQ is the<br>promotion of systems and methods for highly precise<br>biomedical information access. This is done through the<br>organization of a series of challenges (shared tasks) on<br>large-scale biomedical semantic indexing and question<br>answering, where different teams develop systems that<br>compete on the same demanding benchmark datasets that<br>represent the real information needs of biomedical<br>experts. In order to facilitate this information<br>finding process, the BioASQ challenge introduced two<br>complementary tasks: (a) the automated...","title_summary":" BioASQ at CLEF2020: Large-Scale Biomedical<br>Semantic Indexing and Question Answering","x":-32.1292877197,"y":32.1795120239,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.1292877197,"tsne_y":32.1795120239,"subcluster":9,"subcluster_description":"Large-Scale Biomedical Semantic Indexing","shape":"p"},{"cord_uid":"zh5an0p5","source_x":"PMC","title":"On Biomedical Named Entity Recognition: Experiments in Interlingual Transfer for Clinical and Social Media Texts","doi":"10.1007\/978-3-030-45442-5_35","abstract":"Although deep neural networks yield state-of-the-art performance in biomedical named entity recognition (bioNER), much research shares one limitation: models are usually trained and evaluated on English texts from a single domain. In this work, we present a fine-grained evaluation intended to understand the efficiency of multilingual BERT-based models for bioNER of drug and disease mentions across two domains in two languages, namely clinical data and user-generated texts on drug therapy in English and Russian. We investigate the role of transfer learning (TL) strategies between four corpora to reduce the number of examples that have to be manually annotated. Evaluation results demonstrate that multi-BERT shows the best transfer capabilities in the zero-shot setting when training and test sets are either in the same language or in the same domain. TL reduces the amount of labeled data needed to achieve high performance on three out of four corpora: pretrained models reach 98\u201399% of the full dataset performance on both types of entities after training on 10\u201325% of sentences. We demonstrate that pretraining on data with one or both types of transfer can be effective.","publish_time":1585008000000,"author_summary":" Miftahutdinov, Zulfat; Alimova, Ilseyar;<br>Tutubalina, Elena","abstract_summary":" Although deep neural networks yield<br>state-of-the-art performance in biomedical named entity<br>recognition (bioNER), much research shares one<br>limitation: models are usually trained and evaluated on<br>English texts from a single domain. In this work, we<br>present a fine-grained evaluation intended to<br>understand the efficiency of multilingual BERT-based<br>models for bioNER of drug and disease mentions across<br>two domains in two languages, namely clinical data<br>and user-generated texts on drug therapy in<br>English and Russian. We investigate the role of<br>transfer learning (TL) strategies between four corpora<br>to reduce the number of examples that have to be<br>manually annotated. Evaluation results...","title_summary":" On Biomedical Named Entity Recognition:<br>Experiments in Interlingual Transfer for Clinical and<br>Social Media Texts","x":-32.1538391113,"y":31.8249416351,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.1538391113,"tsne_y":31.8249416351,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"g4f0kstn","source_x":"PMC","title":"Finding Old Answers to New Math Questions: The ARQMath Lab at CLEF 2020","doi":"10.1007\/978-3-030-45442-5_73","abstract":"The ARQMath Lab at CLEF 2020 considers the problem of finding answers to new mathematical questions among posted answers on a community question answering site (Math Stack Exchange). Queries are question postings held out from the test collection, each containing both text and at least one formula. We expect this to be a challenging task, as both math and text may be needed to find relevant answer posts. While several models have been proposed for text question answering, math question answering is in an earlier stage of development. To advance math-aware search and mathematical question answering systems, we will create a standard test collection for researchers to use for benchmarking. ARQMath will also include a formula retrieval sub-task: individual formulas from question posts are used to locate formulas in earlier answer posts, with relevance determined by narrative fields created based on the original question. We will use these narrative fields to explore diverse information needs for formula search (e.g., alternative notation, applications in specific fields or definition).","publish_time":1585008000000,"author_summary":" Mansouri, Behrooz; Agarwal, Anurag; Oard,<br>Douglas; Zanibbi, Richard","abstract_summary":" The ARQMath Lab at CLEF 2020 considers the<br>problem of finding answers to new mathematical<br>questions among posted answers on a community question<br>answering site (Math Stack Exchange). Queries are<br>question postings held out from the test collection,<br>each containing both text and at least one formula.<br>We expect this to be a challenging task, as both<br>math and text may be needed to find relevant answer<br>posts. While several models have been proposed for<br>text question answering, math question answering<br>is in an earlier stage of development. To advance<br>math-aware search and mathematical question answering<br>systems, we will create...","title_summary":" Finding Old Answers to New Math Questions: The<br>ARQMath Lab at CLEF 2020","x":-35.3998413086,"y":32.9798355103,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.3998413086,"tsne_y":32.9798355103,"subcluster":12,"subcluster_description":"Assistancefooling Automatic Short Answer","shape":"p"},{"cord_uid":"kt4g3ysk","source_x":"PMC","title":"Personalized Video Summarization Based Exclusively on User Preferences","doi":"10.1007\/978-3-030-45442-5_38","abstract":"We propose a recommender system to detect personalized video summaries, that make visual content interesting for the subjective criteria of the user. In order to provide accurate video summarization, the video segmentation provided by the users and the features of the video segments\u2019 duration are combined using a Synthetic Coordinate based Recommendation system.","publish_time":1585008000000,"author_summary":" Panagiotakis, Costas; Papadakis, Harris;<br>Fragopoulou, Paraskevi","abstract_summary":" We propose a recommender system to detect<br>personalized video summaries, that make visual content<br>interesting for the subjective criteria of the user. In<br>order to provide accurate video summarization, the<br>video segmentation provided by the users and the<br>features of the video segments\u2019 duration are combined<br>using a Synthetic Coordinate based Recommendation<br>system.","title_summary":" Personalized Video Summarization Based<br>Exclusively on User Preferences","x":-31.1648921967,"y":33.9760322571,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.1648921967,"tsne_y":33.9760322571,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"1v6dcmt3","source_x":"PMC","title":"On the Replicability of Combining Word Embeddings and Retrieval Models","doi":"10.1007\/978-3-030-45442-5_7","abstract":"We replicate recent experiments attempting to demonstrate an attractive hypothesis about the use of the Fisher kernel framework and mixture models for aggregating word embeddings towards document representations and the use of these representations in document classification, clustering, and retrieval. Specifically, the hypothesis was that the use of a mixture model of von Mises-Fisher (VMF) distributions instead of Gaussian distributions would be beneficial because of the focus on cosine distances of both VMF and the vector space model traditionally used in information retrieval. Previous experiments had validated this hypothesis. Our replication was not able to validate it, despite a large parameter scan space.","publish_time":1585008000000,"author_summary":" Papariello, Luca; Bampoulidis, Alexandros;<br>Lupu, Mihai","abstract_summary":" We replicate recent experiments attempting to<br>demonstrate an attractive hypothesis about the use of the<br>Fisher kernel framework and mixture models for<br>aggregating word embeddings towards document<br>representations and the use of these representations in<br>document classification, clustering, and retrieval.<br>Specifically, the hypothesis was that the use of a mixture<br>model of von Mises-Fisher (VMF) distributions<br>instead of Gaussian distributions would be beneficial<br>because of the focus on cosine distances of both VMF and<br>the vector space model traditionally used in<br>information retrieval. Previous experiments had<br>validated this hypothesis. Our replication was not able<br>to validate it, despite a large...","title_summary":" On the Replicability of Combining Word<br>Embeddings and Retrieval Models","x":-34.2299804688,"y":33.189540863,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2299804688,"tsne_y":33.189540863,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"frr8xba6","source_x":"PMC","title":"DAKE: Document-Level Attention for Keyphrase Extraction","doi":"10.1007\/978-3-030-45442-5_49","abstract":"Keyphrases provide a concise representation of the topical content of a document and they are helpful in various downstream tasks. Previous approaches for keyphrase extraction model it as a sequence labelling task and use local contextual information to understand the semantics of the input text but they fail when the local context is ambiguous or unclear. We present a new framework to improve keyphrase extraction by utilizing additional supporting contextual information. We retrieve this additional information from other sentences within the same document. To this end, we propose Document-level Attention for Keyphrase Extraction (DAKE), which comprises Bidirectional Long Short-Term Memory networks that capture hidden semantics in text, a document-level attention mechanism to incorporate document level contextual information, gating mechanisms which help to determine the influence of additional contextual information on the fusion with local contextual information, and Conditional Random Fields which capture output label dependencies. Our experimental results on a dataset of research papers show that the proposed model outperforms previous state-of-the-art approaches for keyphrase extraction.","publish_time":1585008000000,"author_summary":" Santosh, Tokala Yaswanth Sri Sai; Sanyal,<br>Debarshi Kumar; Bhowmick, Plaban Kumar; Das, Partha<br>Pratim","abstract_summary":" Keyphrases provide a concise representation<br>of the topical content of a document and they are<br>helpful in various downstream tasks. Previous<br>approaches for keyphrase extraction model it as a sequence<br>labelling task and use local contextual information to<br>understand the semantics of the input text but they fail<br>when the local context is ambiguous or unclear. We<br>present a new framework to improve keyphrase<br>extraction by utilizing additional supporting<br>contextual information. We retrieve this additional<br>information from other sentences within the same document.<br>To this end, we propose Document-level Attention<br>for Keyphrase Extraction (DAKE), which comprises<br>Bidirectional Long Short-Term Memory...","title_summary":" DAKE: Document-Level Attention for Keyphrase<br>Extraction","x":-33.8314361572,"y":32.6471061707,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8314361572,"tsne_y":32.6471061707,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"6fuuvwua","source_x":"PMC","title":"Influence of Random Walk Parametrization on Graph Embeddings","doi":"10.1007\/978-3-030-45442-5_8","abstract":"Network or graph embedding has gained increasing attention in the research community during the last years. In particular, many methods to create graph embeddings using random walk based approaches have been developed. node2vec [10] introduced means to control the random walk behavior, guiding the walks. We aim to reproduce parts of their work and introduce two additional modifications (jump probabilities and attention to hubs), in order to investigate how guiding and modifying the walks influences the learned embeddings. The reproduction includes the case study illustrating homophily and structural equivalence subject to the chosen strategy and a node classification task. We were not able to illustrate structural equivalence and further results show that modifications of the walks only slightly improve node classification, if at all.","publish_time":1585008000000,"author_summary":" Schliski, Fabian; Schl\u00f6tterer, J\u00f6rg;<br>Granitzer, Michael","abstract_summary":" Network or graph embedding has gained<br>increasing attention in the research community during the<br>last years. In particular, many methods to create<br>graph embeddings using random walk based approaches<br>have been developed. node2vec [10] introduced<br>means to control the random walk behavior, guiding<br>the walks. We aim to reproduce parts of their work<br>and introduce two additional modifications (jump<br>probabilities and attention to hubs), in order to investigate<br>how guiding and modifying the walks influences the<br>learned embeddings. The reproduction includes the<br>case study illustrating homophily and structural<br>equivalence subject to the chosen strategy and a node<br>classification task. We...","title_summary":" Influence of Random Walk Parametrization on<br>Graph Embeddings","x":-31.1131534576,"y":35.9015579224,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.1131534576,"tsne_y":35.9015579224,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"8go3wflu","source_x":"PMC","title":"ImageCLEF 2020: Multimedia Retrieval in Lifelogging, Medical, Nature, and Internet Applications","doi":"10.1007\/978-3-030-45442-5_69","abstract":"This paper presents an overview of the 2020 ImageCLEF lab that will be organized as part of the Conference and Labs of the Evaluation Forum\u2014CLEF Labs 2020 in Thessaloniki, Greece. ImageCLEF is an ongoing evaluation initiative (run since 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval of visual data with the aim of providing information access to large collections of images in various usage scenarios and domains. In 2020, the 18th edition of ImageCLEF will organize four main tasks: (i) a Lifelog task (videos, images and other sources) about daily activity understanding, retrieval and summarization, (ii) a Medical task that groups three previous tasks (caption analysis, tuberculosis prediction, and medical visual question answering) with new data and adapted tasks, (iii) a Coral task about segmenting and labeling collections of coral images for 3D modeling, and a new (iv) Web user interface task addressing the problems of detecting and recognizing hand drawn website UIs (User Interfaces) for generating automatic code. The strong participation, with over 235 research groups registering and 63 submitting over 359 runs for the tasks in 2019 shows an important interest in this benchmarking campaign. We expect the new tasks to attract at least as many researchers for 2020.","publish_time":1585008000000,"author_summary":" Ionescu, Bogdan; M\u00fcller, Henning; P\u00e9teri,<br>Renaud; Dang-Nguyen, Duc-Tien; Zhou, Liting; Piras,<br>Luca; Riegler, Michael; Halvorsen, P\u00e5l; Tran,<br>Minh-Triet; Lux, Mathias; Gurrin, Cathal; Chamberlain,<br>Jon; Clark, Adrian; Campello, Antonio; Seco de<br>Herrera, Alba G.; Ben Abacha, Asma; Datla, Vivek; Hasan,<br>Sadid A.; Liu, Joey; Demner-Fushman, Dina; Pelka,<br>Obioma; Friedrich, Christoph M.; Dicente Cid, Yashin;<br>Kozlovski, Serge; Liauchuk, Vitali; Kovalev, Vassili;<br>Berari, Raul; Brie, Paul; Fichou, Dimitri; Dogariu,<br>Mihai; Stefan, Liviu Daniel; Constantin, Mihai<br>Gabriel","abstract_summary":" This paper presents an overview of the 2020<br>ImageCLEF lab that will be organized as part of the<br>Conference and Labs of the Evaluation Forum\u2014CLEF Labs 2020<br>in Thessaloniki, Greece. ImageCLEF is an ongoing<br>evaluation initiative (run since 2003) that promotes the<br>evaluation of technologies for annotation, indexing and<br>retrieval of visual data with the aim of providing<br>information access to large collections of images in<br>various usage scenarios and domains. In 2020, the 18th<br>edition of ImageCLEF will organize four main tasks: (i)<br>a Lifelog task (videos, images and other<br>sources) about daily activity understanding,<br>retrieval and summarization, (ii)...","title_summary":" ImageCLEF 2020: Multimedia Retrieval in<br>Lifelogging, Medical, Nature, and Internet Applications","x":-32.6748695374,"y":33.0343780518,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.6748695374,"tsne_y":33.0343780518,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"28xz762w","source_x":"PMC","title":"Document Network Projection in Pretrained Word Embedding Space","doi":"10.1007\/978-3-030-45442-5_19","abstract":"We present Regularized Linear Embedding (RLE), a novel method that projects a collection of linked documents (e.g., citation network) into a pretrained word embedding space. In addition to the textual content, we leverage a matrix of pairwise similarities providing complementary information (e.g., the network proximity of two documents in a citation graph). We first build a simple word vector average for each document, and we use the similarities to alter this average representation. The document representations can help to solve many information retrieval tasks, such as recommendation, classification and clustering. We demonstrate that our approach outperforms or matches existing document network embedding methods on node classification and link prediction tasks. Furthermore, we show that it helps identifying relevant keywords to describe document classes.","publish_time":1585008000000,"author_summary":" Gourru, Antoine; Guille, Adrien; Velcin,<br>Julien; Jacques, Julien","abstract_summary":" We present Regularized Linear Embedding<br>(RLE), a novel method that projects a collection of<br>linked documents (e.g., citation network) into a<br>pretrained word embedding space. In addition to the<br>textual content, we leverage a matrix of pairwise<br>similarities providing complementary information (e.g.,<br>the network proximity of two documents in a<br>citation graph). We first build a simple word vector<br>average for each document, and we use the similarities<br>to alter this average representation. The<br>document representations can help to solve many<br>information retrieval tasks, such as recommendation,<br>classification and clustering. We demonstrate that our<br>approach outperforms or matches existing document...","title_summary":" Document Network Projection in Pretrained<br>Word Embedding Space","x":-32.6476325989,"y":34.2804870605,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.6476325989,"tsne_y":34.2804870605,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"7rlpcs64","source_x":"PMC","title":"NLPExplorer: Exploring the Universe of NLP Papers","doi":"10.1007\/978-3-030-45442-5_61","abstract":"Understanding the current research trends, problems, and their innovative solutions remains a bottleneck due to the ever-increasing volume of scientific articles. In this paper, we propose NLPExplorer, a completely automatic portal for indexing, searching, and visualizing Natural Language Processing (NLP) research volume. NLPExplorer presents interesting insights from papers, authors, venues, and topics. In contrast to previous topic modelling based approaches, we manually curate five course-grained non-exclusive topical categories namely Linguistic Target (Syntax, Discourse, etc.), Tasks (Tagging, Summarization, etc.), Approaches (unsupervised, supervised, etc.), Languages (English, Chinese, etc.) and Dataset types (news, clinical notes, etc.). Some of the novel features include a list of young popular authors, popular URLs and datasets, list of topically diverse papers and recent popular papers. Also, it provides temporal statistics such as yearwise popularity of topics, datasets, and seminal papers. To facilitate future research and system development, we make all the processed dataset accessible through API calls. The current system is available at http:\/\/nlpexplorer.org.","publish_time":1585008000000,"author_summary":" Parmar, Monarch; Jain, Naman; Jain, Pranjali;<br>Jayakrishna Sahit, P.; Pachpande, Soham; Singh, Shruti;<br>Singh, Mayank","abstract_summary":" Understanding the current research trends,<br>problems, and their innovative solutions remains a<br>bottleneck due to the ever-increasing volume of<br>scientific articles. In this paper, we propose<br>NLPExplorer, a completely automatic portal for indexing,<br>searching, and visualizing Natural Language Processing<br>(NLP) research volume. NLPExplorer presents<br>interesting insights from papers, authors, venues, and<br>topics. In contrast to previous topic modelling based<br>approaches, we manually curate five course-grained<br>non-exclusive topical categories namely Linguistic Target<br>(Syntax, Discourse, etc.), Tasks (Tagging,<br>Summarization, etc.), Approaches (unsupervised,<br>supervised, etc.), Languages (English, Chinese, etc.) and<br>Dataset types (news, clinical notes, etc.). Some of the<br>novel features include a...","title_summary":" NLPExplorer: Exploring the Universe of NLP<br>Papers","x":-32.958442688,"y":32.6093673706,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.958442688,"tsne_y":32.6093673706,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"wv2ep1u4","source_x":"PMC","title":"Bibliometric-Enhanced Information Retrieval 10th Anniversary Workshop Edition","doi":"10.1007\/978-3-030-45442-5_85","abstract":"The Bibliometric-enhanced Information Retrieval workshop series (BIR) was launched at ECIR in 2014 [19] and it was held at ECIR each year since then. This year we organize the 10th iteration of BIR. The workshop series at ECIR and JCDL\/SIGIR tackles issues related to academic search, at the crossroads between Information Retrieval, Natural Language Processing and Bibliometrics. In this overview paper, we summarize the past workshops, present the workshop topics for 2020 and reflect on some future steps for this workshop series.","publish_time":1585008000000,"author_summary":" Cabanac, Guillaume; Frommholz, Ingo; Mayr,<br>Philipp","abstract_summary":" The Bibliometric-enhanced Information<br>Retrieval workshop series (BIR) was launched at ECIR in<br>2014 [19] and it was held at ECIR each year since then.<br>This year we organize the 10th iteration of BIR. The<br>workshop series at ECIR and JCDL\/SIGIR tackles issues<br>related to academic search, at the crossroads between<br>Information Retrieval, Natural Language Processing and<br>Bibliometrics. In this overview paper, we summarize the past<br>workshops, present the workshop topics for 2020 and<br>reflect on some future steps for this workshop series.","title_summary":" Bibliometric-Enhanced Information<br>Retrieval 10th Anniversary Workshop Edition","x":-34.4584312439,"y":34.7999839783,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.4584312439,"tsne_y":34.7999839783,"subcluster":20,"subcluster_description":"Bibliometric-Enhanced Information Retrieval 10Th","shape":"p"},{"cord_uid":"tlks96di","source_x":"PMC","title":"A Mixed Semantic Features Model for Chinese NER with Characters and Words","doi":"10.1007\/978-3-030-45439-5_24","abstract":"Named Entity Recognition (NER) is an essential part of many natural language processing (NLP) tasks. The existing Chinese NER methods are mostly based on word segmentation, or use the character sequences as input. However, using a single granularity representation would suffer from the problems of out-of-vocabulary and word segmentation errors, and the semantic content is relatively simple. In this paper, we introduce the self-attention mechanism into the BiLSTM-CRF neural network structure for Chinese named entity recognition with two embedding. Different from other models, our method combines character and word features at the sequence level, and the attention mechanism computes similarity on the total sequence consisted of characters and words. The character semantic information and the structure of words work together to improve the accuracy of word boundary segmentation and solve the problem of long-phrase combination. We validate our model on MSRA and Weibo corpora, and experiments demonstrate that our model can significantly improve the performance of the Chinese NER task.","publish_time":1584403200000,"author_summary":" Chang, Ning; Zhong, Jiang; Li, Qing; Zhu, Jiang","abstract_summary":" Named Entity Recognition (NER) is an essential<br>part of many natural language processing (NLP)<br>tasks. The existing Chinese NER methods are mostly<br>based on word segmentation, or use the character<br>sequences as input. However, using a single granularity<br>representation would suffer from the problems of<br>out-of-vocabulary and word segmentation errors, and the semantic<br>content is relatively simple. In this paper, we<br>introduce the self-attention mechanism into the<br>BiLSTM-CRF neural network structure for Chinese named<br>entity recognition with two embedding. Different<br>from other models, our method combines character<br>and word features at the sequence level, and the<br>attention mechanism computes similarity...","title_summary":" A Mixed Semantic Features Model for Chinese NER<br>with Characters and Words","x":-34.1887130737,"y":32.0354347229,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1887130737,"tsne_y":32.0354347229,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"i9cezsuc","source_x":"PMC","title":"Interactive Learning for Multimedia at Large","doi":"10.1007\/978-3-030-45439-5_33","abstract":"Interactive learning has been suggested as a key method for addressing analytic multimedia tasks arising in several domains. Until recently, however, methods to maintain interactive performance at the scale of today\u2019s media collections have not been addressed. We propose an interactive learning approach that builds on and extends the state of the art in user relevance feedback systems and high-dimensional indexing for multimedia. We report on a detailed experimental study using the ImageNet and YFCC100M collections, containing 14 million and 100 million images respectively. The proposed approach outperforms the relevant state-of-the-art approaches in terms of interactive performance, while improving suggestion relevance in some cases. In particular, even on YFCC100M, our approach requires less than 0.3 s per interaction round to generate suggestions, using a single computing core and less than 7 GB of main memory.","publish_time":1584403200000,"author_summary":" Khan, Omar Shahbaz; J\u00f3nsson, Bj\u00f6rn \u00de\u00f3r;<br>Rudinac, Stevan; Zah\u00e1lka, Jan; Ragnarsd\u00f3ttir, Hanna;<br>\u00deorleiksd\u00f3ttir, \u00de\u00f3rhildur; Gu\u00f0mundsson, Gylfi \u00de\u00f3r; Amsaleg,<br>Laurent; Worring, Marcel","abstract_summary":" Interactive learning has been suggested as a<br>key method for addressing analytic multimedia<br>tasks arising in several domains. Until recently,<br>however, methods to maintain interactive performance<br>at the scale of today\u2019s media collections have not<br>been addressed. We propose an interactive learning<br>approach that builds on and extends the state of the art in<br>user relevance feedback systems and<br>high-dimensional indexing for multimedia. We report on a<br>detailed experimental study using the ImageNet and<br>YFCC100M collections, containing 14 million and 100<br>million images respectively. The proposed approach<br>outperforms the relevant state-of-the-art approaches in<br>terms of interactive performance, while improving<br>suggestion...","title_summary":" Interactive Learning for Multimedia at Large","x":-33.4261245728,"y":33.6322669983,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4261245728,"tsne_y":33.6322669983,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"f667icyt","source_x":"PMC","title":"Semantic Path-Based Learning for Review Volume Prediction","doi":"10.1007\/978-3-030-45439-5_54","abstract":"Graphs offer a natural abstraction for modeling complex real-world systems where entities are represented as nodes and edges encode relations between them. In such networks, entities may share common or similar attributes and may be connected by paths through multiple attribute modalities. In this work, we present an approach that uses semantically meaningful, bimodal random walks on real-world heterogeneous networks to extract correlations between nodes and bring together nodes with shared or similar attributes. An attention-based mechanism is used to combine multiple attribute-specific representations in a late fusion setup. We focus on a real-world network formed by restaurants and their shared attributes and evaluate performance on predicting the number of reviews a restaurant receives, a strong proxy for popularity. Our results demonstrate the rich expressiveness of such representations in predicting review volume and the ability of an attention-based model to selectively combine individual representations for maximum predictive power on the chosen downstream task.","publish_time":1584403200000,"author_summary":" Sharma, Ujjwal; Rudinac, Stevan; Worring,<br>Marcel; Demmers, Joris; van Dolen, Willemijn","abstract_summary":" Graphs offer a natural abstraction for<br>modeling complex real-world systems where entities are<br>represented as nodes and edges encode relations between<br>them. In such networks, entities may share common or<br>similar attributes and may be connected by paths<br>through multiple attribute modalities. In this work,<br>we present an approach that uses semantically<br>meaningful, bimodal random walks on real-world<br>heterogeneous networks to extract correlations between<br>nodes and bring together nodes with shared or similar<br>attributes. An attention-based mechanism is used to<br>combine multiple attribute-specific<br>representations in a late fusion setup. We focus on a real-world<br>network formed by restaurants and their...","title_summary":" Semantic Path-Based Learning for Review<br>Volume Prediction","x":-31.9228076935,"y":35.1755180359,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.9228076935,"tsne_y":35.1755180359,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"tesitlwh","source_x":"PMC","title":"Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths","doi":"10.1007\/978-3-030-45439-5_2","abstract":"Knowledge graphs\u2019 incompleteness has motivated many researchers to propose methods to automatically infer missing facts in knowledge graphs. Knowledge graph embedding has been an active research area for knowledge graph completion, with great improvement from the early TransE to the current state-of-the-art ConvKB. ConvKB considers a knowledge graph as a set of triples, and employs a convolutional neural network to capture global relationships and transitional characteristics between entities and relations in the knowledge graph. However, it only utilizes the triple information, and ignores the rich information contained in relation paths. In fact, a path of one relation describes the relation from some aspect in a fine-grained way. Therefore, it is beneficial to take relation paths into consideration for knowledge graph embedding. In this paper, we present a novel convolutional neural network-based embedding model PConvKB, which improves knowledge graph embedding by incorporating relation paths locally and globally. Specifically, we introduce attention mechanism to measure the local importance of relation paths. Moreover, we propose a simple yet effective measure DIPF to compute the global importance of relation paths. Experimental results show that our model achieves substantial improvements against state-of-the-art methods.","publish_time":1584403200000,"author_summary":" Jia, Ningning; Cheng, Xiang; Su, Sen","abstract_summary":" Knowledge graphs\u2019 incompleteness has<br>motivated many researchers to propose methods to<br>automatically infer missing facts in knowledge graphs.<br>Knowledge graph embedding has been an active research<br>area for knowledge graph completion, with great<br>improvement from the early TransE to the current<br>state-of-the-art ConvKB. ConvKB considers a knowledge graph as a<br>set of triples, and employs a convolutional neural<br>network to capture global relationships and<br>transitional characteristics between entities and<br>relations in the knowledge graph. However, it only<br>utilizes the triple information, and ignores the rich<br>information contained in relation paths. In fact, a path of<br>one relation describes the relation...","title_summary":" Improving Knowledge Graph Embedding Using<br>Locally and Globally Attentive Relation Paths","x":-31.7123661041,"y":35.4718170166,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.7123661041,"tsne_y":35.4718170166,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"5i5pa5ld","source_x":"PMC","title":"Recognizing Semantic Relations: Attention-Based Transformers vs. Recurrent Models","doi":"10.1007\/978-3-030-45439-5_37","abstract":"Automatically recognizing an existing semantic relation (such as \u201cis a\u201d, \u201cpart of\u201d, \u201cproperty of\u201d, \u201copposite of\u201d etc.) between two arbitrary words (phrases, concepts, etc.) is an important task affecting many information retrieval and artificial intelligence tasks including query expansion, common-sense reasoning, question answering, and database federation. Currently, two classes of approaches exist to classify a relation between words (concepts) X and Y: (1) path-based and (2) distributional. While the path-based approaches look at word-paths connecting X and Y in text, the distributional approaches look at statistical properties of X and Y separately, not necessary in the proximity of each other. Here, we suggest how both types can be improved and empirically compare them using several standard benchmarking datasets. For our distributional approach, we are suggesting using an attention-based transformer. While they are known to be capable of supporting knowledge transfer between different tasks, and recently set a number of benchmarking records in various applications, we are the first to successfully apply them to the task of recognizing semantic relations. To improve a path-based approach, we are suggesting our original neural word path model that combines useful properties of convolutional and recurrent networks, and thus addressing several shortcomings from the prior path-based models. Both our models significantly outperforms the state-of-the-art within its type accordingly. Our transformer-based approach outperforms current state-of-the-art by 1\u201312% points on 4 out of 6 standard benchmarking datasets. This results in 15\u201340% error reduction and is closing the gap between the automated and human performance by up to 50%. It also needs much less training data than prior approaches. For the ease of re-producing our results, we make our source code and trained models publicly available.","publish_time":1584403200000,"author_summary":" Roussinov, Dmitri; Sharoff, Serge; Puchnina,<br>Nadezhda","abstract_summary":" Automatically recognizing an existing<br>semantic relation (such as \u201cis a\u201d, \u201cpart of\u201d, \u201cproperty<br>of\u201d, \u201copposite of\u201d etc.) between two arbitrary<br>words (phrases, concepts, etc.) is an important task<br>affecting many information retrieval and artificial<br>intelligence tasks including query expansion,<br>common-sense reasoning, question answering, and database<br>federation. Currently, two classes of approaches exist to<br>classify a relation between words (concepts) X and Y: (1)<br>path-based and (2) distributional. While the path-based<br>approaches look at word-paths connecting X and Y in text,<br>the distributional approaches look at<br>statistical properties of X and Y separately, not necessary<br>in the proximity of each other....","title_summary":" Recognizing Semantic Relations:<br>Attention-Based Transformers vs. Recurrent Models","x":-33.8555145264,"y":33.4366531372,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8555145264,"tsne_y":33.4366531372,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ov2lzus4","source_x":"PMC","title":"Learning to Rank Images with Cross-Modal Graph Convolutions","doi":"10.1007\/978-3-030-45439-5_39","abstract":"We are interested in the problem of cross-modal retrieval for web image search, where the goal is to retrieve images relevant to a text query. While most of the current approaches for cross-modal retrieval revolve around learning how to represent text and images in a shared latent space, we take a different direction: we propose to generalize the cross-modal relevance feedback mechanism, a simple yet effective unsupervised method, that relies on standard information retrieval heuristics and the choice of a few hyper-parameters. We show that we can cast it as a supervised representation learning problem on graphs, using graph convolutions operating jointly over text and image features, namely cross-modal graph convolutions. The proposed architecture directly learns how to combine image and text features for the ranking task, while taking into account the context given by all the other elements in the set of images to be (re-)ranked. We validate our approach on two datasets: a public dataset from a MediaEval challenge, and a small sample of proprietary image search query logs, referred as WebQ. Our experiments demonstrate that our model improves over standard baselines.","publish_time":1584403200000,"author_summary":" Formal, Thibault; Clinchant, St\u00e9phane;<br>Renders, Jean-Michel; Lee, Sooyeol; Cho, Geun Hee","abstract_summary":" We are interested in the problem of cross-modal<br>retrieval for web image search, where the goal is to<br>retrieve images relevant to a text query. While most of<br>the current approaches for cross-modal retrieval<br>revolve around learning how to represent text and<br>images in a shared latent space, we take a different<br>direction: we propose to generalize the cross-modal<br>relevance feedback mechanism, a simple yet effective<br>unsupervised method, that relies on standard information<br>retrieval heuristics and the choice of a few<br>hyper-parameters. We show that we can cast it as a supervised<br>representation learning problem on graphs, using graph<br>convolutions...","title_summary":" Learning to Rank Images with Cross-Modal Graph<br>Convolutions","x":-32.813167572,"y":33.9613418579,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.813167572,"tsne_y":33.9613418579,"subcluster":21,"subcluster_description":"Cross-Modal Retrievalgraph-Based Image Retrieval","shape":"p"},{"cord_uid":"aw465igx","source_x":"PMC","title":"Inductive Document Network Embedding with Topic-Word Attention","doi":"10.1007\/978-3-030-45439-5_22","abstract":"Document network embedding aims at learning representations for a structured text corpus i.e. when documents are linked to each other. Recent algorithms extend network embedding approaches by incorporating the text content associated with the nodes in their formulations. In most cases, it is hard to interpret the learned representations. Moreover, little importance is given to the generalization to new documents that are not observed within the network. In this paper, we propose an interpretable and inductive document network embedding method. We introduce a novel mechanism, the Topic-Word Attention (TWA), that generates document representations based on the interplay between word and topic representations. We train these word and topic vectors through our general model, Inductive Document Network Embedding (IDNE), by leveraging the connections in the document network. Quantitative evaluations show that our approach achieves state-of-the-art performance on various networks and we qualitatively show that our model produces meaningful and interpretable representations of the words, topics and documents.","publish_time":1584403200000,"author_summary":" Brochier, Robin; Guille, Adrien; Velcin,<br>Julien","abstract_summary":" Document network embedding aims at learning<br>representations for a structured text corpus i.e. when<br>documents are linked to each other. Recent algorithms<br>extend network embedding approaches by<br>incorporating the text content associated with the nodes in<br>their formulations. In most cases, it is hard to<br>interpret the learned representations. Moreover,<br>little importance is given to the generalization to<br>new documents that are not observed within the<br>network. In this paper, we propose an interpretable and<br>inductive document network embedding method. We<br>introduce a novel mechanism, the Topic-Word Attention<br>(TWA), that generates document representations<br>based on the interplay between word and...","title_summary":" Inductive Document Network Embedding with<br>Topic-Word Attention","x":-32.7197151184,"y":33.9640960693,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.7197151184,"tsne_y":33.9640960693,"subcluster":21,"subcluster_description":"Cross-Modal Retrievalgraph-Based Image Retrieval","shape":"p"},{"cord_uid":"lt3m8h41","source_x":"PMC","title":"KvGR: A Graph-Based Interface for Explorative Sequential Question Answering on Heterogeneous Information Sources","doi":"10.1007\/978-3-030-45439-5_50","abstract":"Exploring a knowledge base is often an iterative process: initially vague information needs are refined by interaction. We propose a novel approach for such interaction that supports sequential question answering (SQA) on knowledge graphs. As opposed to previous work, we focus on exploratory settings, which we support with a visual representation of graph structures, helping users to better understand relationships. In addition, our approach keeps track of context \u2013 an important challenge in SQA \u2013 by allowing users to make their focus explicit via subgraph selection. Our results show that the interaction principle is either understood immediately or picked up very quickly \u2013 and that the possibility of exploring the information space iteratively is appreciated.","publish_time":1584403200000,"author_summary":" Witschel, Hans Friedrich; Riesen, Kaspar;<br>Grether, Loris","abstract_summary":" Exploring a knowledge base is often an<br>iterative process: initially vague information needs<br>are refined by interaction. We propose a novel<br>approach for such interaction that supports sequential<br>question answering (SQA) on knowledge graphs. As<br>opposed to previous work, we focus on exploratory<br>settings, which we support with a visual representation<br>of graph structures, helping users to better<br>understand relationships. In addition, our approach<br>keeps track of context \u2013 an important challenge in SQA<br>\u2013 by allowing users to make their focus explicit<br>via subgraph selection. Our results show that the<br>interaction principle is either understood immediately or<br>picked up...","title_summary":" KvGR: A Graph-Based Interface for Explorative<br>Sequential Question Answering on Heterogeneous<br>Information Sources","x":-32.9801292419,"y":35.3803825378,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.9801292419,"tsne_y":35.3803825378,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"w13bh9ia","source_x":"PMC","title":"Domain-Independent Extraction of Scientific Concepts from Research Articles","doi":"10.1007\/978-3-030-45439-5_17","abstract":"We examine the novel task of domain-independent scientific concept extraction from abstracts of scholarly articles and present two contributions. First, we suggest a set of generic scientific concepts that have been identified in a systematic annotation process. This set of concepts is utilised to annotate a corpus of scientific abstracts from 10 domains of Science, Technology and Medicine at the phrasal level in a joint effort with domain experts. The resulting dataset is used in a set of benchmark experiments to (a) provide baseline performance for this task, (b) examine the transferability of concepts between domains. Second, we present a state-of-the-art deep learning baseline. Further, we propose the active learning strategy for an optimal selection of instances from among the various domains in our data. The experimental results show that (1) a substantial agreement is achievable by non-experts after consultation with domain experts, (2) the baseline system achieves a fairly high F1 score, (3) active learning enables us to nearly halve the amount of required training data.","publish_time":1584403200000,"author_summary":" Brack, Arthur; D\u2019Souza, Jennifer; Hoppe,<br>Anett; Auer, S\u00f6ren; Ewerth, Ralph","abstract_summary":" We examine the novel task of<br>domain-independent scientific concept extraction from abstracts<br>of scholarly articles and present two<br>contributions. First, we suggest a set of generic scientific<br>concepts that have been identified in a systematic<br>annotation process. This set of concepts is utilised to<br>annotate a corpus of scientific abstracts from 10<br>domains of Science, Technology and Medicine at the<br>phrasal level in a joint effort with domain experts. The<br>resulting dataset is used in a set of benchmark<br>experiments to (a) provide baseline performance for this<br>task, (b) examine the transferability of concepts<br>between domains. Second, we present a...","title_summary":" Domain-Independent Extraction of Scientific<br>Concepts from Research Articles","x":-32.6942710876,"y":32.5443649292,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.6942710876,"tsne_y":32.5443649292,"subcluster":6,"subcluster_description":"Pubmed Knowledge Graphbuilding","shape":"p"},{"cord_uid":"8g5i7r05","source_x":"PMC","title":"Answering Event-Related Questions over Long-Term News Article Archives","doi":"10.1007\/978-3-030-45439-5_51","abstract":"Long-term news article archives are valuable resources about our past, allowing people to know detailed information of events that occurred at specific time points. To make better use of such heritage collections, this work considers the task of large scale question answering on long-term news article archives. Questions on such archives are often event-related. In addition, they usually exhibit strong temporal aspects and can be roughly categorized into two types: (1) ones containing explicit temporal expressions, and (2) ones only implicitly associated with particular time periods. We focus on the latter type as such questions are more difficult to be answered, and we propose a retriever-reader model with an additional module for reranking articles by exploiting temporal information from different angles. Experimental results on carefully constructed test set show that our model outperforms the existing question answering systems, thanks to an additional module that finds more relevant documents.","publish_time":1584403200000,"author_summary":" Wang, Jiexin; Jatowt, Adam; F\u00e4rber, Michael;<br>Yoshikawa, Masatoshi","abstract_summary":" Long-term news article archives are valuable<br>resources about our past, allowing people to know<br>detailed information of events that occurred at<br>specific time points. To make better use of such heritage<br>collections, this work considers the task of large scale<br>question answering on long-term news article archives.<br>Questions on such archives are often event-related. In<br>addition, they usually exhibit strong temporal aspects<br>and can be roughly categorized into two types: (1)<br>ones containing explicit temporal expressions,<br>and (2) ones only implicitly associated with<br>particular time periods. We focus on the latter type as such<br>questions are more difficult to be...","title_summary":" Answering Event-Related Questions over<br>Long-Term News Article Archives","x":-33.9712944031,"y":33.6381797791,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9712944031,"tsne_y":33.6381797791,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"h5q6rx5a","source_x":"PMC","title":"A Hierarchical Model for Data-to-Text Generation","doi":"10.1007\/978-3-030-45439-5_5","abstract":"Transcribing structured data into natural language descriptions has emerged as a challenging task, referred to as \u201cdata-to-text\u201d. These structures generally regroup multiple elements, as well as their attributes. Most attempts rely on translation encoder-decoder methods which linearize elements into a sequence. This however loses most of the structure contained in the data. In this work, we propose to overpass this limitation with a hierarchical model that encodes the data-structure at the element-level and the structure level. Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative and quantitative metrics.","publish_time":1584403200000,"author_summary":" Rebuffel, Cl\u00e9ment; Soulier, Laure;<br>Scoutheeten, Geoffrey; Gallinari, Patrick","abstract_summary":" Transcribing structured data into natural<br>language descriptions has emerged as a challenging<br>task, referred to as \u201cdata-to-text\u201d. These<br>structures generally regroup multiple elements, as well<br>as their attributes. Most attempts rely on<br>translation encoder-decoder methods which linearize<br>elements into a sequence. This however loses most of the<br>structure contained in the data. In this work, we propose<br>to overpass this limitation with a hierarchical<br>model that encodes the data-structure at the<br>element-level and the structure level. Evaluations on<br>RotoWire show the effectiveness of our model w.r.t.<br>qualitative and quantitative metrics.","title_summary":" A Hierarchical Model for Data-to-Text<br>Generation","x":-34.7625923157,"y":32.3575668335,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.7625923157,"tsne_y":32.3575668335,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"xo4gqpyp","source_x":"PMC","title":"You Can Teach an Old Dog New Tricks: Rank Fusion applied to Coordination Level Matching for Ranking in Systematic Reviews","doi":"10.1007\/978-3-030-45439-5_27","abstract":"Coordination level matching is a ranking method originally proposed to rank documents given Boolean queries that is now several decades old. Rank fusion is a relatively recent method for combining runs from multiple systems into a single ranking, and has been shown to significantly improve the ranking. This paper presents a novel extension to coordination level matching, by applying rank fusion to each sub-clause of a Boolean query. We show that, for the tasks of systematic review screening prioritisation and stopping estimation, our method significantly outperforms the state-of-the-art learning to rank and bag-of-words-based systems for this domain. Our fully automatic, unsupervised method has (i) the potential for significant real-world cost savings (ii) does not rely on any intervention from the user, and (iii) is significantly better at ranking documents given only a Boolean query in the context of systematic reviews when compared to other approaches.","publish_time":1584403200000,"author_summary":" Scells, Harrisen; Zuccon, Guido; Koopman,<br>Bevan","abstract_summary":" Coordination level matching is a ranking<br>method originally proposed to rank documents given<br>Boolean queries that is now several decades old. Rank<br>fusion is a relatively recent method for combining<br>runs from multiple systems into a single ranking,<br>and has been shown to significantly improve the<br>ranking. This paper presents a novel extension to<br>coordination level matching, by applying rank fusion to each<br>sub-clause of a Boolean query. We show that, for the tasks of<br>systematic review screening prioritisation and stopping<br>estimation, our method significantly outperforms the<br>state-of-the-art learning to rank and bag-of-words-based<br>systems for this domain. Our fully automatic,...","title_summary":" You Can Teach an Old Dog New Tricks: Rank Fusion<br>applied to Coordination Level Matching for Ranking in<br>Systematic Reviews","x":-32.909122467,"y":34.1648406982,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.909122467,"tsne_y":34.1648406982,"subcluster":21,"subcluster_description":"Cross-Modal Retrievalgraph-Based Image Retrieval","shape":"p"},{"cord_uid":"j7uhmysr","source_x":"PMC","title":"Joint Word and Entity Embeddings for Entity Retrieval from a Knowledge Graph","doi":"10.1007\/978-3-030-45439-5_10","abstract":"Recent years have witnessed the emergence of novel models for ad-hoc entity search in knowledge graphs of varying complexity. Since these models are based on direct term matching, their accuracy can suffer from a mismatch between vocabularies used in queries and entity descriptions. Although successful applications of word embeddings and knowledge graph entity embeddings to address the issues of vocabulary mismatch in ad-hoc document retrieval and knowledge graph noisiness and incompleteness, respectively, have been reported in recent literature, the utility of joint word and entity embeddings for entity search in knowledge graphs has been relatively unexplored. In this paper, we propose Knowledge graph Entity and Word Embedding for Retrieval (KEWER), a novel method to embed entities and words into the same low-dimensional vector space, which takes into account a knowledge graph\u2019s local structure and structural components, such as entities, attributes, and categories, and is designed specifically for entity search. KEWER is based on random walks over the knowledge graph and can be considered as a hybrid of word and network embedding methods. Similar to word embedding methods, KEWER utilizes contextual co-occurrences as training data, however, it treats words and entities as different objects. Similar to network embedding methods, KEWER takes into account knowledge graph\u2019s local structure, however, it also differentiates between structural components. Experiments on publicly available entity search benchmarks and state-of-the-art word and joint word and entity embedding methods indicate that a combination of KEWER and BM25F results in a consistent improvement in retrieval accuracy over BM25F alone.","publish_time":1584403200000,"author_summary":" Nikolaev, Fedor; Kotov, Alexander","abstract_summary":" Recent years have witnessed the emergence of<br>novel models for ad-hoc entity search in knowledge<br>graphs of varying complexity. Since these models are<br>based on direct term matching, their accuracy can<br>suffer from a mismatch between vocabularies used in<br>queries and entity descriptions. Although successful<br>applications of word embeddings and knowledge graph entity<br>embeddings to address the issues of vocabulary mismatch in<br>ad-hoc document retrieval and knowledge graph<br>noisiness and incompleteness, respectively, have been<br>reported in recent literature, the utility of joint word<br>and entity embeddings for entity search in<br>knowledge graphs has been relatively unexplored. In this<br>paper, we...","title_summary":" Joint Word and Entity Embeddings for Entity<br>Retrieval from a Knowledge Graph","x":-33.0520324707,"y":34.6026344299,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.0520324707,"tsne_y":34.6026344299,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"aew8xr6n","source_x":"PMC","title":"TransRev: Modeling Reviews as Translations from Users to Items","doi":"10.1007\/978-3-030-45439-5_16","abstract":"The text of a review expresses the sentiment a customer has towards a particular product. This is exploited in sentiment analysis where machine learning models are used to predict the review score from the text of the review. Furthermore, the products costumers have purchased in the past are indicative of the products they will purchase in the future. This is what recommender systems exploit by learning models from purchase information to predict the items a customer might be interested in. The underlying structure of this problem setting is a bipartite graph, wherein customer nodes are connected to product nodes via \u2018review\u2019 links. This is reminiscent of knowledge bases, with \u2018review\u2019 links replacing relation types. We propose TransRev, an approach to the product recommendation problem that integrates ideas from recommender systems, sentiment analysis, and multi-relational learning into a joint learning objective. TransRev learns vector representations for users, items, and reviews. The embedding of a review is learned such that (a) it performs well as input feature of a regression model for sentiment prediction; and (b) it always translates the reviewer embedding to the embedding of the reviewed item. This is reminiscent of TransE [5], a popular embedding method for link prediction in knowledge bases. This allows TransRev to approximate a review embedding at test time as the difference of the embedding of each item and the user embedding. The approximated review embedding is then used with the regression model to predict the review score for each item. TransRev outperforms state of the art recommender systems on a large number of benchmark data sets. Moreover, it is able to retrieve, for each user and item, the review text from the training set whose embedding is most similar to the approximated review embedding.","publish_time":1584403200000,"author_summary":" Garc\u00eda-Dur\u00e1n, Alberto; Gonz\u00e1lez, Roberto;<br>O\u00f1oro-Rubio, Daniel; Niepert, Mathias; Li, Hui","abstract_summary":" The text of a review expresses the sentiment a<br>customer has towards a particular product. This is<br>exploited in sentiment analysis where machine learning<br>models are used to predict the review score from the<br>text of the review. Furthermore, the products<br>costumers have purchased in the past are indicative of the<br>products they will purchase in the future. This is what<br>recommender systems exploit by learning models from<br>purchase information to predict the items a customer<br>might be interested in. The underlying structure of<br>this problem setting is a bipartite graph, wherein<br>customer nodes are connected to product nodes via...","title_summary":" TransRev: Modeling Reviews as Translations<br>from Users to Items","x":-33.8080101013,"y":30.9824714661,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8080101013,"tsne_y":30.9824714661,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"1voqlrvl","source_x":"PMC","title":"Joint Geographical and Temporal Modeling Based on Matrix Factorization for Point-of-Interest Recommendation","doi":"10.1007\/978-3-030-45439-5_14","abstract":"With the popularity of Location-based Social Networks, Point-of-Interest (POI) recommendation has become an important task, which learns the users\u2019 preferences and mobility patterns to recommend POIs. Previous studies show that incorporating contextual information such as geographical and temporal influences is necessary to improve POI recommendation by addressing the data sparsity problem. However, existing methods model the geographical influence based on the physical distance between POIs and users, while ignoring the temporal characteristics of such geographical influences. In this paper, we perform a study on the user mobility patterns where we find out that users\u2019 check-ins happen around several centers depending on their current temporal state. Next, we propose a spatio-temporal activity-centers algorithm to model users\u2019 behavior more accurately. Finally, we demonstrate the effectiveness of our proposed contextual model by incorporating it into the matrix factorization model under two different settings: (i) static and (ii) temporal. To show the effectiveness of our proposed method, which we refer to as STACP, we conduct experiments on two well-known real-world datasets acquired from Gowalla and Foursquare LBSNs. Experimental results show that the STACP model achieves a statistically significant performance improvement, compared to the state-of-the-art techniques. Also, we demonstrate the effectiveness of capturing geographical and temporal information for modeling users\u2019 activity centers and the importance of modeling them jointly.","publish_time":1584403200000,"author_summary":" Rahmani, Hossein A.; Aliannejadi, Mohammad;<br>Baratchi, Mitra; Crestani, Fabio","abstract_summary":" With the popularity of Location-based Social<br>Networks, Point-of-Interest (POI) recommendation has<br>become an important task, which learns the users\u2019<br>preferences and mobility patterns to recommend POIs.<br>Previous studies show that incorporating contextual<br>information such as geographical and temporal influences<br>is necessary to improve POI recommendation by<br>addressing the data sparsity problem. However, existing<br>methods model the geographical influence based on the<br>physical distance between POIs and users, while<br>ignoring the temporal characteristics of such<br>geographical influences. In this paper, we perform a study on<br>the user mobility patterns where we find out that<br>users\u2019 check-ins happen around several centers<br>depending...","title_summary":" Joint Geographical and Temporal Modeling<br>Based on Matrix Factorization for<br>Point-of-Interest Recommendation","x":-30.5652275085,"y":34.336139679,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.5652275085,"tsne_y":34.336139679,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"qt0ly5d0","source_x":"PMC","title":"What Can Task Teach Us About Query Reformulations?","doi":"10.1007\/978-3-030-45439-5_42","abstract":"A significant amount of prior research has been devoted to understanding query reformulations. The majority of these works rely on time-based sessions which are sequences of contiguous queries segmented using time threshold on users\u2019 activities. However, queries are generally issued by users having in mind a particular task, and time-based sessions unfortunately fail in revealing such tasks. In this paper, we are interested in revealing in which extent time-based sessions vs. task-based sessions represent significantly different background contexts to be used in the perspective of better understanding users\u2019 query reformulations. Using insights from large-scale search logs, our findings clearly show that task is an additional relevant search unit that helps better understanding user\u2019s query reformulation patterns and predicting the next user\u2019s query. The findings from our analyses provide potential implications for model design of task-based search engines.","publish_time":1584403200000,"author_summary":" Tamine, Lynda; Melgarejo, Jes\u00fas Lov\u00f3n;<br>Pinel-Sauvagnat, Karen","abstract_summary":" A significant amount of prior research has been<br>devoted to understanding query reformulations. The<br>majority of these works rely on time-based sessions<br>which are sequences of contiguous queries segmented<br>using time threshold on users\u2019 activities. However,<br>queries are generally issued by users having in mind a<br>particular task, and time-based sessions unfortunately<br>fail in revealing such tasks. In this paper, we are<br>interested in revealing in which extent time-based<br>sessions vs. task-based sessions represent<br>significantly different background contexts to be used in the<br>perspective of better understanding users\u2019 query<br>reformulations. Using insights from large-scale search logs,<br>our findings clearly show...","title_summary":" What Can Task Teach Us About Query<br>Reformulations?","x":-34.2114486694,"y":34.1167259216,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2114486694,"tsne_y":34.1167259216,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"x3o3a45b","source_x":"PMC","title":"Relevance Ranking Based on Query-Aware Context Analysis","doi":"10.1007\/978-3-030-45439-5_30","abstract":"Word mismatch between queries and documents is a long-standing challenge in information retrieval. Recent advances in distributed word representations address the word mismatch problem by enabling semantic matching. However, most existing models rank documents based on semantic matching between query and document terms without an explicit understanding of the relationship of the match to relevance. To consider semantic matching between query and document, we propose an unsupervised semantic matching model by simulating a user who makes relevance decisions. The primary goal of the proposed model is to combine the exact and semantic matching between query and document terms, which has been shown to produce effective performance in information retrieval. As semantic matching between queries and entire documents is computationally expensive, we propose to use local contexts of query terms in documents for semantic matching. Matching with smaller query-related contexts of documents stems from the relevance judgment process recorded by human observers. The most relevant part of a document is then recognized and used to rank documents with respect to the query. Experimental results on several representative retrieval models and standard datasets show that our proposed semantic matching model significantly outperforms competitive baselines in all measures.","publish_time":1584403200000,"author_summary":" Montazeralghaem, Ali; Rahimi, Razieh; Allan,<br>James","abstract_summary":" Word mismatch between queries and documents is<br>a long-standing challenge in information<br>retrieval. Recent advances in distributed word<br>representations address the word mismatch problem by enabling<br>semantic matching. However, most existing models rank<br>documents based on semantic matching between query and<br>document terms without an explicit understanding of the<br>relationship of the match to relevance. To consider semantic<br>matching between query and document, we propose an<br>unsupervised semantic matching model by simulating a user<br>who makes relevance decisions. The primary goal of<br>the proposed model is to combine the exact and<br>semantic matching between query and document terms,<br>which has...","title_summary":" Relevance Ranking Based on Query-Aware<br>Context Analysis","x":-33.8003730774,"y":34.5368003845,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8003730774,"tsne_y":34.5368003845,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"gw8i6tkn","source_x":"PMC","title":"An Attention Model of Customer Expectation to Improve Review Helpfulness Prediction","doi":"10.1007\/978-3-030-45439-5_55","abstract":"Many people browse reviews online before making purchasing decisions. It is essential to identify the subset of helpful reviews from the large number of reviews of varying quality. This paper aims to build a model to predict review helpfulness automatically. Our work is inspired by the observation that a customer\u2019s expectation of a review can be greatly affected by review sentiment and the degree to which the customer is aware of pertinent product information. Consequently, a customer may pay more attention to that specific content of a review which contributes more to its helpfulness from their perspective. To model such customer expectations and capture important information from a review text, we propose a novel neural network which leverages review sentiment and product information. Specifically, we encode the sentiment of a review through an attention module, to get sentiment-driven information from review text. We also introduce a product attention layer that fuses information from both the target product and related products, in order to capture the product related information from review text. Our experimental results show an AUC improvement of 5.4% and 1.5% over the previous state of the art model on Amazon and Yelp data sets, respectively.","publish_time":1584403200000,"author_summary":" Qu, Xianshan; Li, Xiaopeng; Farkas, Csilla;<br>Rose, John","abstract_summary":" Many people browse reviews online before<br>making purchasing decisions. It is essential to<br>identify the subset of helpful reviews from the large<br>number of reviews of varying quality. This paper aims<br>to build a model to predict review helpfulness<br>automatically. Our work is inspired by the observation that a<br>customer\u2019s expectation of a review can be greatly affected<br>by review sentiment and the degree to which the<br>customer is aware of pertinent product information.<br>Consequently, a customer may pay more attention to that<br>specific content of a review which contributes more to<br>its helpfulness from their perspective. To model<br>such...","title_summary":" An Attention Model of Customer Expectation to<br>Improve Review Helpfulness Prediction","x":-33.8167076111,"y":30.9231414795,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8167076111,"tsne_y":30.9231414795,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"zdqatxo7","source_x":"PMC","title":"Diagnosing BERT with Retrieval Heuristics","doi":"10.1007\/978-3-030-45439-5_40","abstract":"Word embeddings, made widely popular in 2013 with the release of word2vec, have become a mainstay of NLP engineering pipelines. Recently, with the release of BERT, word embeddings have moved from the term-based embedding space to the contextual embedding space\u2014each term is no longer represented by a single low-dimensional vector but instead each term and its context determine the vector weights. BERT\u2019s setup and architecture have been shown to be general enough to be applicable to many natural language tasks. Importantly for Information Retrieval (IR), in contrast to prior deep learning solutions to IR problems which required significant tuning of neural net architectures and training regimes, \u201cvanilla BERT\u201d has been shown to outperform existing retrieval algorithms by a wide margin, including on tasks and corpora that have long resisted retrieval effectiveness gains over traditional IR baselines (such as Robust04). In this paper, we employ the recently proposed axiomatic dataset analysis technique\u2014that is, we create diagnostic datasets that each fulfil a retrieval heuristic (both term matching and semantic-based)\u2014to explore what BERT is able to learn. In contrast to our expectations, we find BERT, when applied to a recently released large-scale web corpus with ad-hoc topics, to not adhere to any of the explored axioms. At the same time, BERT outperforms the traditional query likelihood retrieval model by 40%. This means that the axiomatic approach to IR (and its extension of diagnostic datasets created for retrieval heuristics) may in its current form not be applicable to large-scale corpora. Additional\u2014different\u2014axioms are needed.","publish_time":1584403200000,"author_summary":" C\u00e2mara, Arthur; Hauff, Claudia","abstract_summary":" Word embeddings, made widely popular in 2013<br>with the release of word2vec, have become a mainstay<br>of NLP engineering pipelines. Recently, with the<br>release of BERT, word embeddings have moved from the<br>term-based embedding space to the contextual embedding<br>space\u2014each term is no longer represented by a single<br>low-dimensional vector but instead each term and its context<br>determine the vector weights. BERT\u2019s setup and<br>architecture have been shown to be general enough to be<br>applicable to many natural language tasks. Importantly<br>for Information Retrieval (IR), in contrast to<br>prior deep learning solutions to IR problems which<br>required significant tuning of...","title_summary":" Diagnosing BERT with Retrieval Heuristics","x":-34.3115196228,"y":32.7740135193,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.3115196228,"tsne_y":32.7740135193,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"oe77eupc","source_x":"PMC","title":"Leveraging Schema Labels to Enhance Dataset Search","doi":"10.1007\/978-3-030-45439-5_18","abstract":"A search engine\u2019s ability to retrieve desirable datasets is important for data sharing and reuse. Existing dataset search engines typically rely on matching queries to dataset descriptions. However, a user may not have enough prior knowledge to write a query using terms that match with description text. We propose a novel schema label generation model which generates possible schema labels based on dataset table content. We incorporate the generated schema labels into a mixed ranking model which not only considers the relevance between the query and dataset metadata but also the similarity between the query and generated schema labels. To evaluate our method on real-world datasets, we create a new benchmark specifically for the dataset retrieval task. Experiments show that our approach can effectively improve the precision and NDCG scores of the dataset retrieval task compared with baseline methods. We also test on a collection of Wikipedia tables to show that the features generated from schema labels can improve the unsupervised and supervised web table retrieval task as well.","publish_time":1584403200000,"author_summary":" Chen, Zhiyu; Jia, Haiyan; Heflin, Jeff;<br>Davison, Brian D.","abstract_summary":" A search engine\u2019s ability to retrieve<br>desirable datasets is important for data sharing and<br>reuse. Existing dataset search engines typically<br>rely on matching queries to dataset descriptions.<br>However, a user may not have enough prior knowledge to<br>write a query using terms that match with description<br>text. We propose a novel schema label generation<br>model which generates possible schema labels based<br>on dataset table content. We incorporate the<br>generated schema labels into a mixed ranking model which<br>not only considers the relevance between the query<br>and dataset metadata but also the similarity<br>between the query and generated schema labels. To...","title_summary":" Leveraging Schema Labels to Enhance Dataset<br>Search","x":-33.5467033386,"y":34.6809158325,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5467033386,"tsne_y":34.6809158325,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"v9an24rg","source_x":"PMC","title":"Reinforced Rewards Framework for Text Style Transfer","doi":"10.1007\/978-3-030-45439-5_36","abstract":"Style transfer deals with the algorithms to transfer the stylistic properties of a piece of text into that of another while ensuring that the core content is preserved. There has been a lot of interest in the field of text style transfer due to its wide application to tailored text generation. Existing works evaluate the style transfer models based on content preservation and transfer strength. In this work, we propose a reinforcement learning based framework that directly rewards the framework on these target metrics yielding a better transfer of the target style. We show the improved performance of our proposed framework based on automatic and human evaluation on three independent tasks: wherein we transfer the style of text from formal to informal, high excitement to low excitement, modern English to Shakespearean English, and vice-versa in all the three cases. Improved performance of the proposed framework over existing state-of-the-art frameworks indicates the viability of the approach.","publish_time":1584403200000,"author_summary":" Sancheti, Abhilasha; Krishna, Kundan;<br>Srinivasan, Balaji Vasan; Natarajan, Anandhavelu","abstract_summary":" Style transfer deals with the algorithms to<br>transfer the stylistic properties of a piece of text into<br>that of another while ensuring that the core content<br>is preserved. There has been a lot of interest in<br>the field of text style transfer due to its wide<br>application to tailored text generation. Existing works<br>evaluate the style transfer models based on content<br>preservation and transfer strength. In this work, we propose<br>a reinforcement learning based framework that<br>directly rewards the framework on these target metrics<br>yielding a better transfer of the target style. We show<br>the improved performance of our proposed...","title_summary":" Reinforced Rewards Framework for Text Style<br>Transfer","x":-35.1739349365,"y":31.9395866394,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.1739349365,"tsne_y":31.9395866394,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"ma9h10qx","source_x":"PMC","title":"Utilising Information Foraging Theory for User Interaction with Image Query Auto-Completion","doi":"10.1007\/978-3-030-45439-5_44","abstract":"Query Auto-completion (QAC) is a prominently used feature in search engines, where user interaction with such explicit feature is facilitated by the possible automatic suggestion of queries based on a prefix typed by the user. Existing QAC models have pursued a little on user interaction and cannot capture a user\u2019s information need (IN) context. In this work, we devise a new task of QAC applied on an image for estimating patch (one of the key components of Information Foraging Theory) probabilities for query suggestion. Our work supports query completion by extending a user query prefix (one or two characters) to a complete query utilising a foraging-based probabilistic patch selection model. We present iBERT, to fine-tune the BERT (Bidirectional Encoder Representations from Transformers) model, which leverages combined textual-image queries for a solution to image QAC by computing probabilities of a large set of image patches. The reflected patch probabilities are used for selection while being agnostic to changing information need or contextual mechanisms. Experimental results show that query auto-completion using both natural language queries and images is more effective than using only language-level queries. Also, our fine-tuned iBERT model allows to efficiently rank patches in the image.","publish_time":1584403200000,"author_summary":" Jaiswal, Amit Kumar; Liu, Haiming; Frommholz,<br>Ingo","abstract_summary":" Query Auto-completion (QAC) is a prominently<br>used feature in search engines, where user<br>interaction with such explicit feature is facilitated by<br>the possible automatic suggestion of queries<br>based on a prefix typed by the user. Existing QAC<br>models have pursued a little on user interaction and<br>cannot capture a user\u2019s information need (IN)<br>context. In this work, we devise a new task of QAC applied<br>on an image for estimating patch (one of the key<br>components of Information Foraging Theory)<br>probabilities for query suggestion. Our work supports query<br>completion by extending a user query prefix (one or two<br>characters) to...","title_summary":" Utilising Information Foraging Theory for<br>User Interaction with Image Query Auto-Completion","x":-33.4127388,"y":33.9569168091,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4127388,"tsne_y":33.9569168091,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"tbq7okmj","source_x":"PMC","title":"Variational Recurrent Sequence-to-Sequence Retrieval for Stepwise Illustration","doi":"10.1007\/978-3-030-45439-5_4","abstract":"We address and formalise the task of sequence-to-sequence (seq2seq) cross-modal retrieval. Given a sequence of text passages as query, the goal is to retrieve a sequence of images that best describes and aligns with the query. This new task extends the traditional cross-modal retrieval, where each image-text pair is treated independently ignoring broader context. We propose a novel variational recurrent seq2seq (VRSS) retrieval model for this seq2seq task. Unlike most cross-modal methods, we generate an image vector corresponding to the latent topic obtained from combining the text semantics and context. This synthetic image embedding point associated with every text embedding point can then be employed for either image generation or image retrieval as desired. We evaluate the model for the application of stepwise illustration of recipes, where a sequence of relevant images are retrieved to best match the steps described in the text. To this end, we build and release a new Stepwise Recipe dataset for research purposes, containing 10K recipes (sequences of image-text pairs) having a total of 67K image-text pairs. To our knowledge, it is the first publicly available dataset to offer rich semantic descriptions in a focused category such as food or recipes. Our model is shown to outperform several competitive and relevant baselines in the experiments. We also provide qualitative analysis of how semantically meaningful the results produced by our model are through human evaluation and comparison with relevant existing methods.","publish_time":1584403200000,"author_summary":" Batra, Vishwash; Haldar, Aparajita; He,<br>Yulan; Ferhatosmanoglu, Hakan; Vogiatzis, George;<br>Guha, Tanaya","abstract_summary":" We address and formalise the task of<br>sequence-to-sequence (seq2seq) cross-modal retrieval. Given a<br>sequence of text passages as query, the goal is to<br>retrieve a sequence of images that best describes and<br>aligns with the query. This new task extends the<br>traditional cross-modal retrieval, where each image-text<br>pair is treated independently ignoring broader<br>context. We propose a novel variational recurrent<br>seq2seq (VRSS) retrieval model for this seq2seq task.<br>Unlike most cross-modal methods, we generate an image<br>vector corresponding to the latent topic obtained<br>from combining the text semantics and context. This<br>synthetic image embedding point associated with every<br>text embedding...","title_summary":" Variational Recurrent Sequence-to-Sequence<br>Retrieval for Stepwise Illustration","x":-33.5027351379,"y":33.4444580078,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5027351379,"tsne_y":33.4444580078,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"q9juxggc","source_x":"PMC","title":"Learning Advanced Similarities and Training Features for Toponym Interlinking","doi":"10.1007\/978-3-030-45439-5_8","abstract":"Interlinking of spatio-textual entities is an open and quite challenging research problem, with application in several commercial fields, including geomarketing, navigation and social networks. It comprises the process of identifying, between different data sources, entity descriptions that refer to the same real-world entity. In this work, we focus on toponym interlinking, that is we handle spatio-textual entities that are exclusively represented by their name; additional properties, such as categories, coordinates, etc. are considered as either absent or of too low quality to be exploited in this setting. Toponyms are inherently heterogeneous entities; quite often several alternative names exist for the same toponym, with varying degrees of similarity between these names. State of the art approaches adopt mostly generic, domain-agnostic similarity functions and use them as is, or incorporate them as training features within classifiers for performing toponym interlinking. We claim that capturing the specificities of toponyms and exploiting them into elaborate meta-similarity functions and derived training features can significantly increase the effectiveness of interlinking methods. To this end, we propose the LGM-Sim meta-similarity function and a series of novel, similarity-based and statistical training features that can be utilized in similarity-based and classification-based interlinking settings respectively. We demonstrate that the proposed methods achieve large increases in accuracy, in both settings, compared to several methods from the literature in the widely used Geonames toponym dataset.","publish_time":1584403200000,"author_summary":" Giannopoulos, Giorgos; Kaffes, Vassilis;<br>Kostoulas, Georgios","abstract_summary":" Interlinking of spatio-textual entities is an<br>open and quite challenging research problem, with<br>application in several commercial fields, including<br>geomarketing, navigation and social networks. It comprises<br>the process of identifying, between different<br>data sources, entity descriptions that refer to the<br>same real-world entity. In this work, we focus on<br>toponym interlinking, that is we handle<br>spatio-textual entities that are exclusively represented by<br>their name; additional properties, such as<br>categories, coordinates, etc. are considered as either<br>absent or of too low quality to be exploited in this<br>setting. Toponyms are inherently heterogeneous<br>entities; quite often several alternative names exist<br>for the...","title_summary":" Learning Advanced Similarities and Training<br>Features for Toponym Interlinking","x":-33.1911659241,"y":34.5919494629,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.1911659241,"tsne_y":34.5919494629,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"7p37m92a","source_x":"PMC","title":"A Framework for Argument Retrieval: Ranking Argument Clusters by Frequency and Specificity","doi":"10.1007\/978-3-030-45439-5_29","abstract":"Computational argumentation has recently become a fast growing field of research. An argument consists of a claim, such as \u201cWe should abandon fossil fuels\u201d, which is supported or attacked by at least one premise, for example \u201cBurning fossil fuels is one cause for global warming\u201d. From an information retrieval perspective, an interesting task within this setting is finding the best supporting and attacking premises for a given query claim from a large corpus of arguments. Since the same logical premise can be formulated differently, the system needs to avoid retrieving duplicate results and thus needs to use some form of clustering. In this paper we propose a principled probabilistic ranking framework for premises based on the idea of tf-idf that, given a query claim, first identifies highly similar claims in the corpus, and then clusters and ranks their premises, taking clusters of claims as well as the stances of query and premises into account. We compare our approach to a baseline system that uses BM25F which we outperform even with a primitive implementation of our framework utilising BERT.","publish_time":1584403200000,"author_summary":" Dumani, Lorik; Neumann, Patrick J.; Schenkel,<br>Ralf","abstract_summary":" Computational argumentation has recently<br>become a fast growing field of research. An argument<br>consists of a claim, such as \u201cWe should abandon fossil<br>fuels\u201d, which is supported or attacked by at least one<br>premise, for example \u201cBurning fossil fuels is one cause<br>for global warming\u201d. From an information<br>retrieval perspective, an interesting task within this<br>setting is finding the best supporting and attacking<br>premises for a given query claim from a large corpus of<br>arguments. Since the same logical premise can be<br>formulated differently, the system needs to avoid<br>retrieving duplicate results and thus needs to use some<br>form of...","title_summary":" A Framework for Argument Retrieval: Ranking<br>Argument Clusters by Frequency and Specificity","x":-34.1460227966,"y":34.4546012878,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1460227966,"tsne_y":34.4546012878,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"3w3fy9jn","source_x":"PMC","title":"Semantic Modelling of Citation Contexts for Context-Aware Citation Recommendation","doi":"10.1007\/978-3-030-45439-5_15","abstract":"New research is being published at a rate, at which it is infeasible for many scholars to read and assess everything possibly relevant to their work. In pursuit of a remedy, efforts towards automated processing of publications, like semantic modelling of papers to facilitate their digital handling, and the development of information filtering systems, are an active area of research. In this paper, we investigate the benefits of semantically modelling citation contexts for the purpose of citation recommendation. For this, we develop semantic models of citation contexts based on entities and claim structures. To assess the effectiveness and conceptual soundness of our models, we perform a large offline evaluation on several data sets and furthermore conduct a user study. Our findings show that the models can outperform a non-semantic baseline model and do, indeed, capture the kind of information they\u2019re conceptualized for.","publish_time":1584403200000,"author_summary":" Saier, Tarek; F\u00e4rber, Michael","abstract_summary":" New research is being published at a rate, at<br>which it is infeasible for many scholars to read and<br>assess everything possibly relevant to their work. In<br>pursuit of a remedy, efforts towards automated<br>processing of publications, like semantic modelling of<br>papers to facilitate their digital handling, and the<br>development of information filtering systems, are an<br>active area of research. In this paper, we investigate<br>the benefits of semantically modelling citation<br>contexts for the purpose of citation recommendation.<br>For this, we develop semantic models of citation<br>contexts based on entities and claim structures. To<br>assess the effectiveness and conceptual soundness...","title_summary":" Semantic Modelling of Citation Contexts for<br>Context-Aware Citation Recommendation","x":-33.7604370117,"y":34.8662910461,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7604370117,"tsne_y":34.8662910461,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"ds0cf78u","source_x":"PMC","title":"Seed-Guided Deep Document Clustering","doi":"10.1007\/978-3-030-45439-5_1","abstract":"Different users may be interested in different clustering views underlying a given collection (e.g., topic and writing style in documents). Enabling them to provide constraints reflecting their needs can then help obtain tailored clustering results. For document clustering, constraints can be provided in the form of seed words, each cluster being characterized by a small set of words. This seed-guided constrained document clustering problem was recently addressed through topic modeling approaches. In this paper, we jointly learn deep representations and bias the clustering results through the seed words, leading to a Seed-guided Deep Document Clustering approach. Its effectiveness is demonstrated on five public datasets.","publish_time":1584403200000,"author_summary":" Fard, Mazar Moradi; Thonet, Thibaut;<br>Gaussier, Eric","abstract_summary":" Different users may be interested in different<br>clustering views underlying a given collection (e.g.,<br>topic and writing style in documents). Enabling them<br>to provide constraints reflecting their needs<br>can then help obtain tailored clustering results.<br>For document clustering, constraints can be<br>provided in the form of seed words, each cluster being<br>characterized by a small set of words. This seed-guided<br>constrained document clustering problem was recently<br>addressed through topic modeling approaches. In this<br>paper, we jointly learn deep representations and bias<br>the clustering results through the seed words,<br>leading to a Seed-guided Deep Document Clustering<br>approach. Its effectiveness is demonstrated...","title_summary":" Seed-Guided Deep Document Clustering","x":-32.3304748535,"y":34.2253990173,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.3304748535,"tsne_y":34.2253990173,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"w031hm7f","source_x":"PMC","title":"Multi-components System for Automatic Arabic Diacritization","doi":"10.1007\/978-3-030-45439-5_23","abstract":"In this paper, we propose an approach to tackle the problem of the automatic restoration of Arabic diacritics that includes three components stacked in a pipeline: a deep learning model which is a multi-layer recurrent neural network with LSTM and Dense layers, a character-level rule-based corrector which applies deterministic operations to prevent some errors, and a word-level statistical corrector which uses the context and the distance information to fix some diacritization issues. This approach is novel in a way that combines methods of different types and adds edit distance based corrections. We used a large public dataset containing raw diacritized Arabic text (Tashkeela) for training and testing our system after cleaning and normalizing it. On a newly-released benchmark test set, our system outperformed all the tested systems by achieving DER of 3.39% and WER of 9.94% when taking all Arabic letters into account, DER of 2.61% and WER of 5.83% when ignoring the diacritization of the last letter of every word.","publish_time":1584403200000,"author_summary":" Abbad, Hamza; Xiong, Shengwu","abstract_summary":" In this paper, we propose an approach to tackle<br>the problem of the automatic restoration of Arabic<br>diacritics that includes three components stacked in a<br>pipeline: a deep learning model which is a multi-layer<br>recurrent neural network with LSTM and Dense layers, a<br>character-level rule-based corrector which applies<br>deterministic operations to prevent some errors, and a<br>word-level statistical corrector which uses the context<br>and the distance information to fix some<br>diacritization issues. This approach is novel in a way that<br>combines methods of different types and adds edit<br>distance based corrections. We used a large public<br>dataset containing raw diacritized...","title_summary":" Multi-components System for Automatic Arabic<br>Diacritization","x":-34.622177124,"y":31.6043891907,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.622177124,"tsne_y":31.6043891907,"subcluster":11,"subcluster_description":"Automatic Arabic Diacritizationenhancing","shape":"p"},{"cord_uid":"056bvngu","source_x":"PMC","title":"Evaluating the Effectiveness of the Standard Insights Extraction Pipeline for Bantu Languages","doi":"10.1007\/978-3-030-45439-5_11","abstract":"Extracting insights from data obtained from the web in order to identify people\u2019s views and opinions on various topics is a growing practice. The standard insights extraction pipeline is typically an unsupervised machine learning task composed of processes that preprocess the text, visualize it, cluster and identify the topics and sentiment in each cluster, and then graph the network. Given the increasing amount of data being generated on the internet in Africa today, and the multilingual state of African countries, we evaluated how well the standard pipeline works when applied to text wholly or partially written in indigenous African languages, specifically Bantu languages. We carried out an exploratory investigation using Twitter data and compared the outputs from each step of the pipeline for an English dataset and a mixed Bantu language dataset. We found that for Bantu languages, due to their complex grammatical structure, extra preprocessing steps such as part-of-speech tagging and morphological analysis are required during data cleaning, threshold values should be adjusted during topic modeling, and semantic analysis should be performed before completing text preprocessing.","publish_time":1584403200000,"author_summary":" Nchabeleng, Mathibele; Byamugisha, Joan","abstract_summary":" Extracting insights from data obtained from<br>the web in order to identify people\u2019s views and<br>opinions on various topics is a growing practice. The<br>standard insights extraction pipeline is typically an<br>unsupervised machine learning task composed of processes<br>that preprocess the text, visualize it, cluster and<br>identify the topics and sentiment in each cluster, and<br>then graph the network. Given the increasing amount<br>of data being generated on the internet in Africa<br>today, and the multilingual state of African<br>countries, we evaluated how well the standard pipeline<br>works when applied to text wholly or partially<br>written in indigenous African languages,...","title_summary":" Evaluating the Effectiveness of the Standard<br>Insights Extraction Pipeline for Bantu Languages","x":-33.5691757202,"y":31.8867454529,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5691757202,"tsne_y":31.8867454529,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"f3r659yv","source_x":"PMC","title":"Visual Re-Ranking via Adaptive Collaborative Hypergraph Learning for Image Retrieval","doi":"10.1007\/978-3-030-45439-5_34","abstract":"Visual re-ranking has received considerable attention in recent years. It aims to enhance the performance of text-based image retrieval by boosting the rank of relevant images using visual information. Hypergraph has been widely used for relevance estimation, where textual results are taken as vertices and the re-ranking problem is formulated as a transductive learning on the hypergraph. The potential of the hypergraph learning is essentially determined by the hypergraph construction scheme. To this end, in this paper, we introduce a novel data representation technique named adaptive collaborative representation for hypergraph learning. Compared to the conventional collaborative representation, we consider the data locality to adaptively select relevant and close samples for a test sample and discard irrelevant and faraway ones. Moreover, at the feature level, we impose a weight matrix on the representation errors to adaptively highlight the important features and reduce the effect of redundant\/noisy ones. Finally, we also add a nonnegativity constraint on the representation coefficients to enhance the hypergraph interpretability. These attractive properties allow constructing a more informative and quality hypergraph, thereby achieving better retrieval performance than other hypergraph models. Extensive experiments on the public MediaEval benchmarks demonstrate that our re-ranking method achieves consistently superior results, compared to state-of-the-art methods.","publish_time":1584403200000,"author_summary":" Bouhlel, Noura; Feki, Ghada; Amar, Chokri Ben","abstract_summary":" Visual re-ranking has received considerable<br>attention in recent years. It aims to enhance the<br>performance of text-based image retrieval by boosting the<br>rank of relevant images using visual information.<br>Hypergraph has been widely used for relevance estimation,<br>where textual results are taken as vertices and the<br>re-ranking problem is formulated as a transductive<br>learning on the hypergraph. The potential of the<br>hypergraph learning is essentially determined by the<br>hypergraph construction scheme. To this end, in this<br>paper, we introduce a novel data representation<br>technique named adaptive collaborative representation<br>for hypergraph learning. Compared to the<br>conventional collaborative representation, we consider<br>the...","title_summary":" Visual Re-Ranking via Adaptive Collaborative<br>Hypergraph Learning for Image Retrieval","x":-32.7798500061,"y":33.9815788269,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.7798500061,"tsne_y":33.9815788269,"subcluster":21,"subcluster_description":"Cross-Modal Retrievalgraph-Based Image Retrieval","shape":"p"},{"cord_uid":"2m0oc86w","source_x":"PMC","title":"VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification","doi":"10.1007\/978-3-030-45439-5_25","abstract":"Much progress has been made recently on text classification with methods based on neural networks. In particular, models using attention mechanism such as BERT have shown to have the capability of capturing the contextual information within a sentence or document. However, their ability of capturing the global information about the vocabulary of a language is more limited. This latter is the strength of Graph Convolutional Networks (GCN). In this paper, we propose VGCN-BERT model which combines the capability of BERT with a Vocabulary Graph Convolutional Network (VGCN). Local information and global information interact through different layers of BERT, allowing them to influence mutually and to build together a final representation for classification. In our experiments on several text classification datasets, our approach outperforms BERT and GCN alone, and achieve higher effectiveness than that reported in previous studies.","publish_time":1584403200000,"author_summary":" Lu, Zhibin; Du, Pan; Nie, Jian-Yun","abstract_summary":" Much progress has been made recently on text<br>classification with methods based on neural networks. In<br>particular, models using attention mechanism such as BERT<br>have shown to have the capability of capturing the<br>contextual information within a sentence or document.<br>However, their ability of capturing the global<br>information about the vocabulary of a language is more<br>limited. This latter is the strength of Graph<br>Convolutional Networks (GCN). In this paper, we propose<br>VGCN-BERT model which combines the capability of BERT<br>with a Vocabulary Graph Convolutional Network<br>(VGCN). Local information and global information<br>interact through different layers of BERT, allowing<br>them...","title_summary":" VGCN-BERT: Augmenting BERT with Graph<br>Embedding for Text Classification","x":-31.7621593475,"y":35.0967597961,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.7621593475,"tsne_y":35.0967597961,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"w2glplgl","source_x":"PMC","title":"Multimodal Entity Linking for Tweets","doi":"10.1007\/978-3-030-45439-5_31","abstract":"In many information extraction applications, entity linking (EL) has emerged as a crucial task that allows leveraging information about named entities from a knowledge base. In this paper, we address the task of multimodal entity linking (MEL), an emerging research field in which textual and visual information is used to map an ambiguous mention to an entity in a knowledge base (KB). First, we propose a method for building a fully annotated Twitter dataset for MEL, where entities are defined in a Twitter KB. Then, we propose a model for jointly learning a representation of both mentions and entities from their textual and visual contexts. We demonstrate the effectiveness of the proposed model by evaluating it on the proposed dataset and highlight the importance of leveraging visual information when it is available.","publish_time":1584403200000,"author_summary":" Adjali, Omar; Besan\u00e7on, Romaric; Ferret,<br>Olivier; Le Borgne, Herv\u00e9; Grau, Brigitte","abstract_summary":" In many information extraction applications,<br>entity linking (EL) has emerged as a crucial task that<br>allows leveraging information about named entities<br>from a knowledge base. In this paper, we address the<br>task of multimodal entity linking (MEL), an<br>emerging research field in which textual and visual<br>information is used to map an ambiguous mention to an entity<br>in a knowledge base (KB). First, we propose a<br>method for building a fully annotated Twitter dataset<br>for MEL, where entities are defined in a Twitter KB.<br>Then, we propose a model for jointly learning a<br>representation of both mentions and entities from...","title_summary":" Multimodal Entity Linking for Tweets","x":-33.4712791443,"y":34.1507720947,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4712791443,"tsne_y":34.1507720947,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"v6wyg170","source_x":"PMC","title":"Generation of Synthetic Query Auto Completion Logs","doi":"10.1007\/978-3-030-45439-5_41","abstract":"Privacy concerns can prohibit research access to large-scale commercial query logs. Here we focus on generation of a synthetic log from a publicly available dataset, suitable for evaluation of query auto completion (QAC) systems. The synthetic log contains plausible string sequences reflecting how users enter their queries in a QAC interface. Properties that would influence experimental outcomes are compared between a synthetic log and a real QAC log through a set of side-by-side experiments, and confirm the applicability of the generated log for benchmarking the performance of QAC methods.","publish_time":1584403200000,"author_summary":" Krishnan, Unni; Moffat, Alistair; Zobel,<br>Justin; Billerbeck, Bodo","abstract_summary":" Privacy concerns can prohibit research access<br>to large-scale commercial query logs. Here we<br>focus on generation of a synthetic log from a publicly<br>available dataset, suitable for evaluation of query auto<br>completion (QAC) systems. The synthetic log contains<br>plausible string sequences reflecting how users enter<br>their queries in a QAC interface. Properties that<br>would influence experimental outcomes are compared<br>between a synthetic log and a real QAC log through a set of<br>side-by-side experiments, and confirm the applicability of<br>the generated log for benchmarking the<br>performance of QAC methods.","title_summary":" Generation of Synthetic Query Auto Completion<br>Logs","x":-34.6013565063,"y":34.488483429,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6013565063,"tsne_y":34.488483429,"subcluster":20,"subcluster_description":"Bibliometric-Enhanced Information Retrieval 10Th","shape":"p"},{"cord_uid":"pd2fltwr","source_x":"PMC","title":"Recommending Music Curators: A Neural Style-Aware Approach","doi":"10.1007\/978-3-030-45439-5_13","abstract":"We propose a framework for personalized music curator recommendation to connect users with curators who have matching curation style. Three unique features of the proposed framework are: (i) models of curation style to capture the coverage of music and curator\u2019s individual style in assigning tracks to playlists; (ii) a curation-based embedding approach to capture inter-track agreement, beyond the audio features, resulting in models of music tracks that pair well together; and (iii) a novel neural pairwise ranking model for personalized music curator recommendation that naturally incorporates both curator style models and track embeddings. Experiments over a Spotify dataset show significant improvements in precision, recall, and F1 versus state-of-the-art.","publish_time":1584403200000,"author_summary":" Wang, Jianling; Caverlee, James","abstract_summary":" We propose a framework for personalized music<br>curator recommendation to connect users with curators<br>who have matching curation style. Three unique<br>features of the proposed framework are: (i) models of<br>curation style to capture the coverage of music and<br>curator\u2019s individual style in assigning tracks to<br>playlists; (ii) a curation-based embedding approach to<br>capture inter-track agreement, beyond the audio<br>features, resulting in models of music tracks that pair<br>well together; and (iii) a novel neural pairwise<br>ranking model for personalized music curator<br>recommendation that naturally incorporates both curator<br>style models and track embeddings. Experiments over<br>a Spotify dataset show significant...","title_summary":" Recommending Music Curators: A Neural<br>Style-Aware Approach","x":-31.5909156799,"y":34.0181846619,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.5909156799,"tsne_y":34.0181846619,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"pxi0sadj","source_x":"PMC","title":"Temporal Latent Space Modeling for Community Prediction","doi":"10.1007\/978-3-030-45439-5_49","abstract":"We propose a temporal latent space model for user community prediction in social networks, whose goal is to predict future emerging user communities based on past history of users\u2019 topics of interest. Our model assumes that each user lies within an unobserved latent space, and similar users in the latent space representation are more likely to be members of the same user community. The model allows each user to adjust its location in the latent space as her topics of interest evolve over time. Empirically, we demonstrate that our model, when evaluated on a Twitter dataset, outperforms existing approaches under two application scenarios, namely news recommendation and user prediction on a host of metrics such as mrr, ndcg as well as precision and f-measure.","publish_time":1584403200000,"author_summary":" Fani, Hossein; Bagheri, Ebrahim; Du, Weichang","abstract_summary":" We propose a temporal latent space model for<br>user community prediction in social networks,<br>whose goal is to predict future emerging user<br>communities based on past history of users\u2019 topics of<br>interest. Our model assumes that each user lies within an<br>unobserved latent space, and similar users in the latent<br>space representation are more likely to be members of<br>the same user community. The model allows each user<br>to adjust its location in the latent space as her<br>topics of interest evolve over time. Empirically, we<br>demonstrate that our model, when evaluated on a Twitter<br>dataset, outperforms existing approaches under...","title_summary":" Temporal Latent Space Modeling for Community<br>Prediction","x":-30.5534420013,"y":34.3568077087,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.5534420013,"tsne_y":34.3568077087,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"qksoii1o","source_x":"PMC","title":"Graph-Embedding Empowered Entity Retrieval","doi":"10.1007\/978-3-030-45439-5_7","abstract":"In this research, we improve upon the current state of the art in entity retrieval by re-ranking the result list using graph embeddings. The paper shows that graph embeddings are useful for entity-oriented search tasks. We demonstrate empirically that encoding information from the knowledge graph into (graph) embeddings contributes to a higher increase in effectiveness of entity retrieval results than using plain word embeddings. We analyze the impact of the accuracy of the entity linker on the overall retrieval effectiveness. Our analysis further deploys the cluster hypothesis to explain the observed advantages of graph embeddings over the more widely used word embeddings, for user tasks involving ranking entities.","publish_time":1584403200000,"author_summary":" Gerritse, Emma J.; Hasibi, Faegheh; de Vries,<br>Arjen P.","abstract_summary":" In this research, we improve upon the current<br>state of the art in entity retrieval by re-ranking the<br>result list using graph embeddings. The paper shows<br>that graph embeddings are useful for<br>entity-oriented search tasks. We demonstrate empirically that<br>encoding information from the knowledge graph into<br>(graph) embeddings contributes to a higher increase in<br>effectiveness of entity retrieval results than using plain<br>word embeddings. We analyze the impact of the<br>accuracy of the entity linker on the overall retrieval<br>effectiveness. Our analysis further deploys the cluster<br>hypothesis to explain the observed advantages of graph<br>embeddings over the more widely used...","title_summary":" Graph-Embedding Empowered Entity Retrieval","x":-32.8460693359,"y":34.69190979,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.8460693359,"tsne_y":34.69190979,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"a21dwip2","source_x":"PMC","title":"Curriculum Learning Strategies for IR: An Empirical Study on Conversation Response Ranking","doi":"10.1007\/978-3-030-45439-5_46","abstract":"Neural ranking models are traditionally trained on a series of random batches, sampled uniformly from the entire training set. Curriculum learning has recently been shown to improve neural models\u2019 effectiveness by sampling batches non-uniformly, going from easy to difficult instances during training. In the context of neural Information Retrieval (IR) curriculum learning has not been explored yet, and so it remains unclear (1) how to measure the difficulty of training instances and (2) how to transition from easy to difficult instances during training. To address both challenges and determine whether curriculum learning is beneficial for neural ranking models, we need large-scale datasets and a retrieval task that allows us to conduct a wide range of experiments. For this purpose, we resort to the task of conversation response ranking: ranking responses given the conversation history. In order to deal with challenge (1), we explore scoring functions to measure the difficulty of conversations based on different input spaces. To address challenge (2) we evaluate different pacing functions, which determine the velocity in which we go from easy to difficult instances. We find that, overall, by just intelligently sorting the training data (i.e., by performing curriculum learning) we can improve the retrieval effectiveness by up to 2% (The source code is available at https:\/\/github.com\/Guzpenha\/transformers_cl.).","publish_time":1584403200000,"author_summary":" Penha, Gustavo; Hauff, Claudia","abstract_summary":" Neural ranking models are traditionally<br>trained on a series of random batches, sampled<br>uniformly from the entire training set. Curriculum<br>learning has recently been shown to improve neural<br>models\u2019 effectiveness by sampling batches<br>non-uniformly, going from easy to difficult instances during<br>training. In the context of neural Information Retrieval<br>(IR) curriculum learning has not been explored yet,<br>and so it remains unclear (1) how to measure the<br>difficulty of training instances and (2) how to transition<br>from easy to difficult instances during training.<br>To address both challenges and determine whether<br>curriculum learning is beneficial for neural ranking<br>models, we need...","title_summary":" Curriculum Learning Strategies for IR: An<br>Empirical Study on Conversation Response Ranking","x":-35.6200332642,"y":31.9938564301,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.6200332642,"tsne_y":31.9938564301,"subcluster":13,"subcluster_description":"Multi-Turn Response Selection Models","shape":"p"},{"cord_uid":"89c7rijg","source_x":"PMC","title":"Counterfactual Online Learning to Rank","doi":"10.1007\/978-3-030-45439-5_28","abstract":"Exploiting users\u2019 implicit feedback, such as clicks, to learn rankers is attractive as it does not require editorial labelling effort, and adapts to users\u2019 changing preferences, among other benefits. However, directly learning a ranker from implicit data is challenging, as users\u2019 implicit feedback usually contains bias (e.g., position bias, selection bias) and noise (e.g., clicking on irrelevant but attractive snippets, adversarial clicks). Two main methods have arisen for optimizing rankers based on implicit feedback: counterfactual learning to rank (CLTR), which learns a ranker from the historical click-through data collected from a deployed, logging ranker; and online learning to rank (OLTR), where a ranker is updated by recording user interaction with a result list produced by multiple rankers (usually via interleaving). In this paper, we propose a counterfactual online learning to rank algorithm (COLTR) that combines the key components of both CLTR and OLTR. It does so by replacing the online evaluation required by traditional OLTR methods with the counterfactual evaluation common in CLTR. Compared to traditional OLTR approaches based on interleaving, COLTR can evaluate a large number of candidate rankers in a more efficient manner. Our empirical results show that COLTR significantly outperforms traditional OLTR methods. Furthermore, COLTR can reach the same effectiveness of the current state-of-the-art, under noisy click settings, and has room for future extensions.","publish_time":1584403200000,"author_summary":" Zhuang, Shengyao; Zuccon, Guido","abstract_summary":" Exploiting users\u2019 implicit feedback, such as<br>clicks, to learn rankers is attractive as it does not<br>require editorial labelling effort, and adapts to<br>users\u2019 changing preferences, among other benefits.<br>However, directly learning a ranker from implicit data<br>is challenging, as users\u2019 implicit feedback<br>usually contains bias (e.g., position bias, selection<br>bias) and noise (e.g., clicking on irrelevant but<br>attractive snippets, adversarial clicks). Two main<br>methods have arisen for optimizing rankers based on<br>implicit feedback: counterfactual learning to rank<br>(CLTR), which learns a ranker from the historical<br>click-through data collected from a deployed, logging<br>ranker; and online learning to rank (OLTR),...","title_summary":" Counterfactual Online Learning to Rank","x":-31.5905017853,"y":33.9971237183,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.5905017853,"tsne_y":33.9971237183,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"yxzug44x","source_x":"PMC","title":"Context-Guided Learning to Rank Entities","doi":"10.1007\/978-3-030-45439-5_6","abstract":"We propose a method for learning entity orders, for example, safety, popularity, and livability orders of countries. We train linear functions by using samples of ordered entities as training data, and attributes of entities as features. An example of such functions is f(Entity) [Formula: see text] (Police budget) [Formula: see text] (Crime rate), for ordering countries in terms of safety. As the size of training data is typically small in this task, we propose a machine learning method referred to as context-guided learning (CGL) to overcome the over-fitting problem. Exploiting a large amount of contexts regarding relations between the labeling criteria (e.g. safety) and attributes, CGL guides learning in the correct direction by estimating a roughly appropriate weight for each attribute by the contexts. This idea was implemented by a regularization approach similar to support vector machines. Experiments were conducted with 158 kinds of orders in three datasets. The experimental results showed high effectiveness of the contextual guidance over existing ranking methods.","publish_time":1584403200000,"author_summary":" Kato, Makoto P.; Imrattanatrai, Wiradee;<br>Yamamoto, Takehiro; Ohshima, Hiroaki; Tanaka, Katsumi","abstract_summary":" We propose a method for learning entity orders,<br>for example, safety, popularity, and livability<br>orders of countries. We train linear functions by<br>using samples of ordered entities as training data,<br>and attributes of entities as features. An example<br>of such functions is f(Entity) [Formula: see<br>text] (Police budget) [Formula: see text] (Crime<br>rate), for ordering countries in terms of safety. As<br>the size of training data is typically small in this<br>task, we propose a machine learning method referred<br>to as context-guided learning (CGL) to overcome<br>the over-fitting problem. Exploiting a large<br>amount of contexts regarding relations between the<br>labeling...","title_summary":" Context-Guided Learning to Rank Entities","x":-33.0617523193,"y":33.0581054688,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.0617523193,"tsne_y":33.0581054688,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"vu91dbqs","source_x":"PMC","title":"Using Image Captions and Multitask Learning for Recommending Query Reformulations","doi":"10.1007\/978-3-030-45439-5_45","abstract":"Interactive search sessions often contain multiple queries, where the user submits a reformulated version of the previous query in response to the original results. We aim to enhance the query recommendation experience for a commercial image search engine. Our proposed methodology incorporates current state-of-the-art practices from relevant literature \u2013 the use of generation-based sequence-to-sequence models that capture session context, and a multitask architecture that simultaneously optimizes the ranking of results. We extend this setup by driving the learning of such a model with captions of clicked images as the target, instead of using the subsequent query within the session. Since these captions tend to be linguistically richer, the reformulation mechanism can be seen as assistance to construct more descriptive queries. In addition, via the use of a pairwise loss for the secondary ranking task, we show that the generated reformulations are more diverse.","publish_time":1584403200000,"author_summary":" Verma, Gaurav; Vinay, Vishwa; Bansal, Sahil;<br>Oberoi, Shashank; Sharma, Makkunda; Gupta, Prakhar","abstract_summary":" Interactive search sessions often contain<br>multiple queries, where the user submits a reformulated<br>version of the previous query in response to the<br>original results. We aim to enhance the query<br>recommendation experience for a commercial image search<br>engine. Our proposed methodology incorporates<br>current state-of-the-art practices from relevant<br>literature \u2013 the use of generation-based<br>sequence-to-sequence models that capture session context, and a<br>multitask architecture that simultaneously optimizes<br>the ranking of results. We extend this setup by<br>driving the learning of such a model with captions of<br>clicked images as the target, instead of using the<br>subsequent query within the session. Since...","title_summary":" Using Image Captions and Multitask Learning<br>for Recommending Query Reformulations","x":-33.48777771,"y":33.7589607239,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.48777771,"tsne_y":33.7589607239,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"fymgnv1g","source_x":"PMC","title":"ReadNet: A Hierarchical Transformer Framework for Web Article Readability Analysis","doi":"10.1007\/978-3-030-45439-5_3","abstract":"Analyzing the readability of articles has been an important sociolinguistic task. Addressing this task is necessary to the automatic recommendation of appropriate articles to readers with different comprehension abilities, and it further benefits education systems, web information systems, and digital libraries. Current methods for assessing readability employ empirical measures or statistical learning techniques that are limited by their ability to characterize complex patterns such as article structures and semantic meanings of sentences. In this paper, we propose a new and comprehensive framework which uses a hierarchical self-attention model to analyze document readability. In this model, measurements of sentence-level difficulty are captured along with the semantic meanings of each sentence. Additionally, the sentence-level features are incorporated to characterize the overall readability of an article with consideration of article structures. We evaluate our proposed approach on three widely-used benchmark datasets against several strong baseline approaches. Experimental results show that our proposed method achieves the state-of-the-art performance on estimating the readability for various web articles and literature.","publish_time":1584403200000,"author_summary":" Meng, Changping; Chen, Muhao; Mao, Jie;<br>Neville, Jennifer","abstract_summary":" Analyzing the readability of articles has been<br>an important sociolinguistic task. Addressing<br>this task is necessary to the automatic<br>recommendation of appropriate articles to readers with<br>different comprehension abilities, and it further<br>benefits education systems, web information systems,<br>and digital libraries. Current methods for<br>assessing readability employ empirical measures or<br>statistical learning techniques that are limited by their<br>ability to characterize complex patterns such as<br>article structures and semantic meanings of<br>sentences. In this paper, we propose a new and<br>comprehensive framework which uses a hierarchical<br>self-attention model to analyze document readability. In this<br>model, measurements of sentence-level difficulty<br>are...","title_summary":" ReadNet: A Hierarchical Transformer<br>Framework for Web Article Readability Analysis","x":-34.2975349426,"y":32.3755455017,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2975349426,"tsne_y":32.3755455017,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"o5scqiyk","source_x":"PMC","title":"Accelerating Substructure Similarity Search for Formula Retrieval","doi":"10.1007\/978-3-030-45439-5_47","abstract":"Formula retrieval systems using substructure matching are effective, but suffer from slow retrieval times caused by the complexity of structure matching. We present a specialized inverted index and rank-safe dynamic pruning algorithm for faster substructure retrieval. Formulas are indexed from their Operator Tree (OPT) representations. Our model is evaluated using the NTCIR-12 Wikipedia Formula Browsing Task and a new formula corpus produced from Math StackExchange posts. Our approach preserves the effectiveness of structure matching while allowing queries to be executed in real-time.","publish_time":1584403200000,"author_summary":" Zhong, Wei; Rohatgi, Shaurya; Wu, Jian; Giles,<br>C. Lee; Zanibbi, Richard","abstract_summary":" Formula retrieval systems using substructure<br>matching are effective, but suffer from slow retrieval<br>times caused by the complexity of structure<br>matching. We present a specialized inverted index and<br>rank-safe dynamic pruning algorithm for faster<br>substructure retrieval. Formulas are indexed from their<br>Operator Tree (OPT) representations. Our model is<br>evaluated using the NTCIR-12 Wikipedia Formula Browsing<br>Task and a new formula corpus produced from Math<br>StackExchange posts. Our approach preserves the<br>effectiveness of structure matching while allowing queries<br>to be executed in real-time.","title_summary":" Accelerating Substructure Similarity Search<br>for Formula Retrieval","x":-34.3030281067,"y":34.6799049377,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.3030281067,"tsne_y":34.6799049377,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"pujijsj7","source_x":"PMC","title":"Patch-Based Identification of Lexical Semantic Relations","doi":"10.1007\/978-3-030-45439-5_9","abstract":"The identification of lexical semantic relations is of the utmost importance to enhance reasoning capacities of Natural Language Processing and Information Retrieval systems. Within this context, successful results have been achieved based on the distributional hypothesis and\/or the paradigmatic assumption. However, both strategies solely rely on the input words to predict the lexical semantic relation. In this paper, we make the hypothesis that the decision process should not only rely on the input words but also on their K closest neighbors in some semantic space. For that purpose, we present different binary and multi-task classification strategies that include two distinct attention mechanisms based on PageRank. Evaluation results over four gold-standard datasets show that average improvements of 10.6% for binary and 8% for multi-task classification can be achieved over baseline approaches in terms of F[Formula: see text]. The code and the datasets are available upon demand.","publish_time":1584403200000,"author_summary":" Bannour, Nesrine; Dias, Ga\u00ebl; Chahir,<br>Youssef; Akhmouch, Houssam","abstract_summary":" The identification of lexical semantic<br>relations is of the utmost importance to enhance<br>reasoning capacities of Natural Language Processing and<br>Information Retrieval systems. Within this context,<br>successful results have been achieved based on the<br>distributional hypothesis and\/or the paradigmatic<br>assumption. However, both strategies solely rely on the<br>input words to predict the lexical semantic<br>relation. In this paper, we make the hypothesis that the<br>decision process should not only rely on the input words<br>but also on their K closest neighbors in some<br>semantic space. For that purpose, we present different<br>binary and multi-task classification strategies<br>that include two distinct...","title_summary":" Patch-Based Identification of Lexical<br>Semantic Relations","x":-34.1008758545,"y":33.7064323425,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1008758545,"tsne_y":33.7064323425,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"hhd6gjzg","source_x":"PMC","title":"A Regularised Intent Model for Discovering Multiple Intents in E-Commerce Tail Queries","doi":"10.1007\/978-3-030-45439-5_43","abstract":"A substantial portion of the query volume for e-commerce search engines consists of infrequent queries and identifying user intent in such tail queries is critical in retrieving relevant products. The intent of a query is defined as a labelling of its tokens with the product attributes whose values are matched against the query tokens during retrieval. Tail queries in e-commerce search tend to have multiple correct attribute labels for their tokens due to multiple valid matches in the product catalog. In this paper, we propose a latent variable generative model along with a novel data dependent regularisation technique for identifying multiple intents in such queries. We demonstrate the superior performance of our proposed model against several strong baseline models on an editorially labelled data set as well as in a large scale online A\/B experiment at Flipkart, a major Indian e-commerce company.","publish_time":1584403200000,"author_summary":" Maji, Subhadeep; Patel, Priyank; Thakarar,<br>Bharat; Kumar, Mohit; Tripathi, Krishna Azad","abstract_summary":" A substantial portion of the query volume for<br>e-commerce search engines consists of infrequent queries<br>and identifying user intent in such tail queries is<br>critical in retrieving relevant products. The intent of<br>a query is defined as a labelling of its tokens<br>with the product attributes whose values are<br>matched against the query tokens during retrieval.<br>Tail queries in e-commerce search tend to have<br>multiple correct attribute labels for their tokens due<br>to multiple valid matches in the product catalog.<br>In this paper, we propose a latent variable<br>generative model along with a novel data dependent<br>regularisation technique for identifying...","title_summary":" A Regularised Intent Model for Discovering<br>Multiple Intents in E-Commerce Tail Queries","x":-33.8471450806,"y":34.1513328552,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8471450806,"tsne_y":34.1513328552,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"sisn6eys","source_x":"PMC","title":"Moving from Formal Towards Coherent Concept Analysis: Why, When and How","doi":"10.1007\/978-3-030-45439-5_19","abstract":"Formal concept analysis has been largely applied to explore taxonomic relationships and derive ontologies from text collections. Despite its recognized relevance, it generally misses relevant concept associations and suffers from the need to learn from Boolean space models. Biclustering, the discovery of coherent concept associations (subsets of documents correlated on subsets of terms and topics), is here suggested to address the aforementioned problems. This work proposes a structured view on why, when and how to apply biclustering for concept analysis, a subject remaining largely unexplored up to date. Gathered results from a large text collection confirm the relevance of biclustering to find less-trivial, yet actionable and statistically significant concept associations.","publish_time":1584403200000,"author_summary":" Kovalchuk, Pavlo; Proen\u00e7a, Diogo; Borbinha,<br>Jos\u00e9; Henriques, Rui","abstract_summary":" Formal concept analysis has been largely<br>applied to explore taxonomic relationships and derive<br>ontologies from text collections. Despite its recognized<br>relevance, it generally misses relevant concept<br>associations and suffers from the need to learn from Boolean<br>space models. Biclustering, the discovery of<br>coherent concept associations (subsets of documents<br>correlated on subsets of terms and topics), is here<br>suggested to address the aforementioned problems. This<br>work proposes a structured view on why, when and how<br>to apply biclustering for concept analysis, a<br>subject remaining largely unexplored up to date.<br>Gathered results from a large text collection confirm<br>the relevance of biclustering...","title_summary":" Moving from Formal Towards Coherent Concept<br>Analysis: Why, When and How","x":-33.8069458008,"y":35.4403343201,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8069458008,"tsne_y":35.4403343201,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"k1upc1xu","source_x":"PMC","title":"Axiomatic Analysis of Contact Recommendation Methods in Social Networks: An IR Perspective","doi":"10.1007\/978-3-030-45439-5_12","abstract":"Contact recommendation is an important functionality in many social network scenarios including Twitter and Facebook, since they can help grow the social networks of users by suggesting, to a given user, people they might wish to follow. Recently, it has been shown that classical information retrieval (IR) weighting models \u2013 such as BM25 \u2013 can be adapted to effectively recommend new social contacts to a given user. However, the exact properties that make such adapted contact recommendation models effective at the task are as yet unknown. In this paper, inspired by new advances in the axiomatic theory of IR, we study the existing IR axioms for the contact recommendation task. Our theoretical analysis and empirical findings show that while the classical axioms related to term frequencies and term discrimination seem to have a positive impact on the recommendation effectiveness, those related to length normalization tend to be not desirable for the task.","publish_time":1584403200000,"author_summary":" Sanz-Cruzado, Javier; Macdonald, Craig;<br>Ounis, Iadh; Castells, Pablo","abstract_summary":" Contact recommendation is an important<br>functionality in many social network scenarios including<br>Twitter and Facebook, since they can help grow the<br>social networks of users by suggesting, to a given<br>user, people they might wish to follow. Recently, it<br>has been shown that classical information<br>retrieval (IR) weighting models \u2013 such as BM25 \u2013 can be<br>adapted to effectively recommend new social contacts<br>to a given user. However, the exact properties<br>that make such adapted contact recommendation<br>models effective at the task are as yet unknown. In this<br>paper, inspired by new advances in the axiomatic<br>theory of IR, we...","title_summary":" Axiomatic Analysis of Contact Recommendation<br>Methods in Social Networks: An IR Perspective","x":-30.8272590637,"y":34.1622810364,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8272590637,"tsne_y":34.1622810364,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"r8mi42fu","source_x":"PMC","title":"Using of Open-Source Technologies for the Design and Development of a Speech Processing System Based on Stemming Methods","doi":"10.1007\/978-3-030-47240-5_10","abstract":"This article discusses the idea of developing an intelligent and customizable automated system for real-time text and voice dialogs with the user. This system can be used for almost any subject area, for example, to create an automated robot - a call center operator or smart chat bots, assistants, and so on. This article presents the developed flexible architecture of the proposed system. The system has many independent submodules. These modules work as interacting microservices and use several speech recognition schemes, including a decision support submodule, third-party speech recognition systems and a post-processing subsystem. In this paper, the post-processing module of the recognized text is presented in detail on the example of Russian and English dictionary models. The proposed submodule also uses several processing steps, including the use of various stemming methods, the use of word stop-lists or other lexical structures, the use of stochastic keyword ranking using a weight table, etc.","publish_time":1588636800000,"author_summary":" Tarasiev, Andrey; Filippova, Margarita;<br>Aksyonov, Konstantin; Aksyonova, Olga; Antonova, Anna","abstract_summary":" This article discusses the idea of developing<br>an intelligent and customizable automated<br>system for real-time text and voice dialogs with the<br>user. This system can be used for almost any subject<br>area, for example, to create an automated robot - a<br>call center operator or smart chat bots,<br>assistants, and so on. This article presents the developed<br>flexible architecture of the proposed system. The<br>system has many independent submodules. These<br>modules work as interacting microservices and use<br>several speech recognition schemes, including a<br>decision support submodule, third-party speech<br>recognition systems and a post-processing subsystem. In<br>this paper, the post-processing module...","title_summary":" Using of Open-Source Technologies for the<br>Design and Development of a Speech Processing System<br>Based on Stemming Methods","x":-36.9761695862,"y":30.8026218414,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.9761695862,"tsne_y":30.8026218414,"subcluster":0,"subcluster_description":"Blocks World Dialogue System","shape":"p"},{"cord_uid":"xift5l9c","source_x":"PMC","title":"Using FLOSS for Storing, Processing and Linking Corpus Data","doi":"10.1007\/978-3-030-47240-5_17","abstract":"Corpus data is widely used to solve different linguistic, educational and applied problems. The Tatar corpus management system (http:\/\/tugantel.tatar) is specifically developed for Turkic languages. The functionality of our corpus management system includes a search of lexical units, morphological and lexical search, a search of syntactic units, a search of N-grams and others. The search is performed using open source tools (database management system MariaDB, Redis data store). This article describes the process of choosing FLOSS for the main components of our system and also processing a search query and building a linked open dataset based on corpus data.","publish_time":1588636800000,"author_summary":" Mukhamedshin, Damir; Nevzorova, Olga;<br>Kirillovich, Alexander","abstract_summary":" Corpus data is widely used to solve different<br>linguistic, educational and applied problems. The Tatar<br>corpus management system (http:\/\/tugantel.tatar)<br>is specifically developed for Turkic languages.<br>The functionality of our corpus management system<br>includes a search of lexical units, morphological and<br>lexical search, a search of syntactic units, a search of<br>N-grams and others. The search is performed using open<br>source tools (database management system MariaDB,<br>Redis data store). This article describes the<br>process of choosing FLOSS for the main components of our<br>system and also processing a search query and building<br>a linked open dataset based on corpus data.","title_summary":" Using FLOSS for Storing, Processing and<br>Linking Corpus Data","x":-34.6273498535,"y":34.726360321,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6273498535,"tsne_y":34.726360321,"subcluster":20,"subcluster_description":"Bibliometric-Enhanced Information Retrieval 10Th","shape":"p"},{"cord_uid":"2akq7e56","source_x":"PMC","title":"An Empirical Investigation of Sentiment Analysis of the Bug Tracking Process in Libre Office Open Source Software","doi":"10.1007\/978-3-030-47240-5_4","abstract":"In this work we are studying the sentiment in Open Source Software projects and more specifically in the process of bug reporting, to investigate the human factor, namely, the feedback from the community (end-users, developers, testers, etc.). One of the characteristics for which Open Source Software has gained attention, over the years, is the fact that it is continuously being tested and maintained by its community of volunteers. Sentiment analysis, a rapidly growing field, can enrich software evaluation with a social aspect. Results suggest that FLOSS projects\u2019 bug reports can potentially constitute a rich emotionally - imbued information source.","publish_time":1588636800000,"author_summary":" Kritikos, Apostolos; Venetis, Theodoros;<br>Stamelos, Ioannis","abstract_summary":" In this work we are studying the sentiment in<br>Open Source Software projects and more<br>specifically in the process of bug reporting, to investigate<br>the human factor, namely, the feedback from the<br>community (end-users, developers, testers, etc.). One<br>of the characteristics for which Open Source<br>Software has gained attention, over the years, is the<br>fact that it is continuously being tested and<br>maintained by its community of volunteers. Sentiment<br>analysis, a rapidly growing field, can enrich software<br>evaluation with a social aspect. Results suggest that<br>FLOSS projects\u2019 bug reports can potentially<br>constitute a rich emotionally - imbued information<br>source.","title_summary":" An Empirical Investigation of Sentiment<br>Analysis of the Bug Tracking Process in Libre Office Open<br>Source Software","x":-33.7901496887,"y":29.3837604523,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7901496887,"tsne_y":29.3837604523,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"5gcs9zr7","source_x":"PMC","title":"An XQuery Specification for Requests with Preferences on XML Databases","doi":"10.1007\/978-3-030-47240-5_12","abstract":"An exact query is a query in which the user specifies precisely what to retrieve from a database (XML or relational database). For these queries only data that strictly respect all user\u2019s conditions is returned. XML documents are generally semi-structured. Due to the non-existence or lack of knowledge of the model of the document being queried, when exact queries are used, there is a high risk of obtaining an empty result (in the case of too specific queries) or too large (in the case of too vague queries). In contrast to exact queries, requests with preferences aim to return only the most relevant results in order to avoid empty or too important results as much as possible. To achieve this goal, requests with preferences generally consist of two parts: the first part is used to express strict constraints and the second part to express preferences or wishes. The satisfaction of both parts increases the relevance of the corresponding results. This paper presents XQuery preference, an extension of the XQuery language, that allows to express requests with preferences relating to both the values and the structure of an XML document. A representation model of such requests based on the Generalized Tree Pattern (GTP) model is also proposed in order to allow an evaluation of these requests through a tree pattern matching process. Integration of the proposed language in open source implementations of XQuery like BaseX, Berkeley DB XML, eXist-db, Galax and much more, will allow users to get much more relevant responses to their concerns.","publish_time":1588636800000,"author_summary":" Tchoup\u00e9 Tchendji, Maurice; Kenfack, Patrik<br>Joslin","abstract_summary":" An exact query is a query in which the user<br>specifies precisely what to retrieve from a database (XML<br>or relational database). For these queries only<br>data that strictly respect all user\u2019s conditions is<br>returned. XML documents are generally semi-structured.<br>Due to the non-existence or lack of knowledge of the<br>model of the document being queried, when exact<br>queries are used, there is a high risk of obtaining an<br>empty result (in the case of too specific queries) or<br>too large (in the case of too vague queries). In<br>contrast to exact queries, requests with preferences<br>aim to return only...","title_summary":" An XQuery Specification for Requests with<br>Preferences on XML Databases","x":-33.9422950745,"y":35.8418998718,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9422950745,"tsne_y":35.8418998718,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"gxh17nf8","source_x":"PMC","title":"GAMMA: A Graph and Multi-view Memory Attention Mechanism for Top-N Heterogeneous Recommendation","doi":"10.1007\/978-3-030-47426-3_3","abstract":"Exploiting heterogeneous information networks (HIN) to top-N recommendation has been shown to alleviate the data sparsity problem present in recommendation systems. This requires careful effort in extracting relevant knowledge from HIN. However, existing models in this setting have the following shortcomings. Mainly, they are not end-to-end, which puts the burden on the system to first learn similarity or commuting matrix offline using some manually selected meta-paths before we train for the top-N recommendation objective. Further, they do not attentively extract user-specific information from HIN, which is essential for personalization. To address these challenges, we propose an end-to-end neural network model \u2013 GAMMA (Graph and Multi-view Memory Attention mechanism). We aim to replace the offline meta-path based similarity or commuting matrix computation with a graph attention mechanism. Besides, with different semantics of items in HIN, we propose a multi-view memory attention mechanism to learn more profound user-specific item views. Experiments on three real-world datasets demonstrate the effectiveness of our model for top-N recommendation setting.","publish_time":1587081600000,"author_summary":" Vijaikumar, M.; Shevade, Shirish; Narasimha<br>Murty, M.","abstract_summary":" Exploiting heterogeneous information<br>networks (HIN) to top-N recommendation has been shown to<br>alleviate the data sparsity problem present in<br>recommendation systems. This requires careful effort in<br>extracting relevant knowledge from HIN. However,<br>existing models in this setting have the following<br>shortcomings. Mainly, they are not end-to-end, which puts the<br>burden on the system to first learn similarity or<br>commuting matrix offline using some manually selected<br>meta-paths before we train for the top-N recommendation<br>objective. Further, they do not attentively extract<br>user-specific information from HIN, which is essential for<br>personalization. To address these challenges, we propose an<br>end-to-end neural network model...","title_summary":" GAMMA: A Graph and Multi-view Memory Attention<br>Mechanism for Top-N Heterogeneous Recommendation","x":-31.2268180847,"y":35.0725440979,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2268180847,"tsne_y":35.0725440979,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"6l5lv89s","source_x":"PMC","title":"Learning Multigraph Node Embeddings Using Guided L\u00e9vy Flights","doi":"10.1007\/978-3-030-47426-3_41","abstract":"Learning efficient representation of graphs has recently been studied extensively for simple networks to facilitate various downstream applications. In this paper, we deal with a more generalized graph structure, called multigraph (multiple edges of different types connecting a pair of nodes) and propose Multigraph2Vec, a random walk based framework for learning multigraph network representation. Multigraph2Vec samples a heterogeneous neighborhood structure for each node by preserving the inter-layer interactions. It employs L\u00e9vy flight random walk strategy, which allows the random walker to travel across multiple layers and reach far-off nodes in a single step. The transition probabilities are learned in a supervised fashion as a function of node attributes (metadata based and\/or network structure based). We compare Multigraph2Vec with four state-of-the-art baselines after suitably adopting to our setting on four datasets. Multigraph2Vec outperforms others in the task of link prediction, by beating the best baseline with 5.977% higher AUC score; while in the multi-class node classification task, it beats the best baseline with 5.28% higher accuracy. We also deployed Multigraph2Vec for friend recommendation on Hike Messenger.","publish_time":1587081600000,"author_summary":" Roy, Aman; Kumar, Vinayak; Mukherjee,<br>Debdoot; Chakraborty, Tanmoy","abstract_summary":" Learning efficient representation of graphs<br>has recently been studied extensively for simple<br>networks to facilitate various downstream<br>applications. In this paper, we deal with a more generalized<br>graph structure, called multigraph (multiple edges<br>of different types connecting a pair of nodes) and<br>propose Multigraph2Vec, a random walk based framework<br>for learning multigraph network representation.<br>Multigraph2Vec samples a heterogeneous neighborhood<br>structure for each node by preserving the inter-layer<br>interactions. It employs L\u00e9vy flight random walk strategy,<br>which allows the random walker to travel across<br>multiple layers and reach far-off nodes in a single step.<br>The transition probabilities are learned in a...","title_summary":" Learning Multigraph Node Embeddings Using<br>Guided L\u00e9vy Flights","x":-31.153213501,"y":35.7388458252,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.153213501,"tsne_y":35.7388458252,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"2vhb6pq0","source_x":"PMC","title":"Case-Sensitive Neural Machine Translation","doi":"10.1007\/978-3-030-47426-3_51","abstract":"Even as an important lexical information for Latin languages, word case is often ignored in machine translation. According to observations, the translation performance drops significantly when we introduce case-sensitive evaluation metrics. In this paper, we introduce two types of case-sensitive neural machine translation (NMT) approaches to alleviate the above problems: i) adding case tokens into the decoding sequence, and ii) adopting case prediction to the conventional NMT. Our proposed approaches incorporate case information to the NMT decoder by jointly learning target word generation and word case prediction. We compare our approaches with multiple kinds of baselines including NMT with naive case-restoration methods and analyze the impacts of various setups on our approaches. Experimental results on three typical translation tasks (Zh-En, En-Fr, En-De) show that our proposed methods lead to the improvements up to 2.5, 1.0 and 0.5 in case-sensitive BLEU scores respectively. Further analyses also illustrate the inherent reasons why our approaches lead to different improvements on different translation tasks.","publish_time":1587081600000,"author_summary":" Shi, Xuewen; Huang, Heyan; Jian, Ping; Tang,<br>Yi-Kun","abstract_summary":" Even as an important lexical information for<br>Latin languages, word case is often ignored in<br>machine translation. According to observations, the<br>translation performance drops significantly when we<br>introduce case-sensitive evaluation metrics. In this<br>paper, we introduce two types of case-sensitive<br>neural machine translation (NMT) approaches to<br>alleviate the above problems: i) adding case tokens into<br>the decoding sequence, and ii) adopting case<br>prediction to the conventional NMT. Our proposed<br>approaches incorporate case information to the NMT<br>decoder by jointly learning target word generation and<br>word case prediction. We compare our approaches<br>with multiple kinds of baselines including NMT with<br>naive...","title_summary":" Case-Sensitive Neural Machine Translation","x":-34.854511261,"y":32.156993866,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.854511261,"tsne_y":32.156993866,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"r0gbw5j6","source_x":"PMC","title":"Modeling Users\u2019 Multifaceted Interest Correlation for Social Recommendation","doi":"10.1007\/978-3-030-47426-3_10","abstract":"Recommender systems suggest to users the items that are potentially of their interests, by mining users\u2019 feedback data on items. Social relations provide an independent source of information about users and can be exploited for improving recommendation performance. Most of existing recommendation methods exploit social influence by refining social relations into a scalar indicator to either directly recommend friends\u2019 visited items to users or constrain that friends\u2019 embeddings are similar. However, a scalar indicator cannot express the multifaceted interest correlations between users, since each user\u2019s interest is distributed across multiple dimensions. To address this issue, we propose a new embedding-based framework, which exploits users\u2019 multifaceted interest correlation for social recommendation. We design a dimension-wise attention mechanism to learn a correlation vector to characterize the interest correlation between a pair of friends, capturing the high variation of users\u2019 interest correlation on multiple dimensions. Moreover, we use friends\u2019 embeddings to smooth a user\u2019s own embedding with the correlation vector as weights, building the elaborate unstructured social influence between users. Experimental results on two real-world datasets demonstrate that modeling users\u2019 multifaceted interest correlations can significantly improve recommendation performance.","publish_time":1587081600000,"author_summary":" Wang, Hao; Shen, Huawei; Cheng, Xueqi","abstract_summary":" Recommender systems suggest to users the items<br>that are potentially of their interests, by mining<br>users\u2019 feedback data on items. Social relations<br>provide an independent source of information about<br>users and can be exploited for improving<br>recommendation performance. Most of existing recommendation<br>methods exploit social influence by refining social<br>relations into a scalar indicator to either directly<br>recommend friends\u2019 visited items to users or constrain<br>that friends\u2019 embeddings are similar. However, a<br>scalar indicator cannot express the multifaceted<br>interest correlations between users, since each user\u2019s<br>interest is distributed across multiple dimensions. To<br>address this issue, we propose a new embedding-based...","title_summary":" Modeling Users\u2019 Multifaceted Interest<br>Correlation for Social Recommendation","x":-30.9010429382,"y":34.3528060913,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.9010429382,"tsne_y":34.3528060913,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"1rijkc5s","source_x":"PMC","title":"Relational Metric Learning with Dual Graph Attention Networks for Social Recommendation","doi":"10.1007\/978-3-030-47426-3_9","abstract":"Existing social recommenders typically incorporate all social relations into user preference modeling, while social connections are not always built on common interests. In addition, they often learn a single vector for each user involved in two domains, which is insufficient to reveal user\u2019s complex interests to both items and friends. To tackle the above issues, in this paper, we consider modeling the user-item interactions and social relations simultaneously and propose a novel metric learning-based model called RML-DGATs. Specifically, relations in two domains are modeled as two types of relation vectors, with which each user can be regarded as being translated to both multiple item-aware and social-aware representations. Then we model the relation vectors by neighborhood interactions with two carefully designed dual GATs to fully encode the neighborhood information. Finally, the two parts are jointly trained under a dual metric learning framework. Extensive experiments on two real-world datasets demonstrate that our model outperforms the best baseline by 1.91% to 4.74% on three metrics for top-N recommendation and the performance gains are more significant under the cold-start scenarios.","publish_time":1587081600000,"author_summary":" Wang, Xiaodong; Liu, Zhen; Wang, Nana; Fan,<br>Wentao","abstract_summary":" Existing social recommenders typically<br>incorporate all social relations into user preference<br>modeling, while social connections are not always built<br>on common interests. In addition, they often<br>learn a single vector for each user involved in two<br>domains, which is insufficient to reveal user\u2019s complex<br>interests to both items and friends. To tackle the above<br>issues, in this paper, we consider modeling the<br>user-item interactions and social relations<br>simultaneously and propose a novel metric learning-based<br>model called RML-DGATs. Specifically, relations in<br>two domains are modeled as two types of relation<br>vectors, with which each user can be regarded as being<br>translated...","title_summary":" Relational Metric Learning with Dual Graph<br>Attention Networks for Social Recommendation","x":-31.1605319977,"y":34.6221694946,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.1605319977,"tsne_y":34.6221694946,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"b4vw5r0o","source_x":"PMC","title":"CrowdQM: Learning Aspect-Level User Reliability and Comment Trustworthiness in Discussion Forums","doi":"10.1007\/978-3-030-47426-3_46","abstract":"Community discussion forums are increasingly used to seek advice; however, they often contain conflicting and unreliable information. Truth discovery models estimate source reliability and infer information trustworthiness simultaneously in a mutual reinforcement manner, and can be used to distinguish trustworthy comments with no supervision. However, they do not capture the diversity of word expressions and learn a single reliability score for the user. CrowdQM addresses these limitations by modeling the fine-grained aspect-level reliability of users and incorporate semantic similarity between words to learn a latent trustworthy comment embedding. We apply our latent trustworthy comment for comment ranking for three diverse communities in Reddit and show consistent improvement over non-aspect based approaches. We also show qualitative results on learned reliability scores and word embeddings by our model.","publish_time":1587081600000,"author_summary":" Morales, Alex; Narang, Kanika; Sundaram,<br>Hari; Zhai, Chengxiang","abstract_summary":" Community discussion forums are increasingly<br>used to seek advice; however, they often contain<br>conflicting and unreliable information. Truth discovery<br>models estimate source reliability and infer<br>information trustworthiness simultaneously in a mutual<br>reinforcement manner, and can be used to distinguish<br>trustworthy comments with no supervision. However, they do<br>not capture the diversity of word expressions and<br>learn a single reliability score for the user.<br>CrowdQM addresses these limitations by modeling the<br>fine-grained aspect-level reliability of users and<br>incorporate semantic similarity between words to learn a<br>latent trustworthy comment embedding. We apply our<br>latent trustworthy comment for comment ranking for<br>three diverse...","title_summary":" CrowdQM: Learning Aspect-Level User<br>Reliability and Comment Trustworthiness in Discussion<br>Forums","x":-31.9973640442,"y":33.6999778748,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.9973640442,"tsne_y":33.6999778748,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ae9bu92l","source_x":"PMC","title":"FlowRec: Prototyping Session-Based Recommender Systems in Streaming Mode","doi":"10.1007\/978-3-030-47426-3_6","abstract":"Despite the increasing interest towards session-based and streaming recommender systems, there is still a lack of publicly available evaluation frameworks supporting both these paradigms. To address the gap, we propose FlowRec \u2014 an extension of the streaming framework Scikit-Multiflow, which opens plentiful possibilities for prototyping recommender systems operating on sessionized data streams, thanks to the underlying collection of incremental learners and support for real-time performance tracking. We describe the extended functionalities of the adapted prequential evaluation protocol, and develop a competitive recommendation algorithm on top of Scikit-Multiflow\u2019s implementation of a Hoeffding Tree. We compare our algorithm to other known baselines for the next-item prediction task across three different domains.","publish_time":1587081600000,"author_summary":" Paraschakis, Dimitris; Nilsson, Bengt J.","abstract_summary":" Despite the increasing interest towards<br>session-based and streaming recommender systems, there is<br>still a lack of publicly available evaluation<br>frameworks supporting both these paradigms. To address<br>the gap, we propose FlowRec \u2014 an extension of the<br>streaming framework Scikit-Multiflow, which opens<br>plentiful possibilities for prototyping recommender<br>systems operating on sessionized data streams, thanks<br>to the underlying collection of incremental<br>learners and support for real-time performance<br>tracking. We describe the extended functionalities of<br>the adapted prequential evaluation protocol, and<br>develop a competitive recommendation algorithm on top<br>of Scikit-Multiflow\u2019s implementation of a<br>Hoeffding Tree. We compare our algorithm to other known<br>baselines...","title_summary":" FlowRec: Prototyping Session-Based<br>Recommender Systems in Streaming Mode","x":-30.8507709503,"y":34.1047210693,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8507709503,"tsne_y":34.1047210693,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"r5wnz7rq","source_x":"PMC","title":"SLGAT: Soft Labels Guided Graph Attention Networks","doi":"10.1007\/978-3-030-47426-3_40","abstract":"Graph convolutional neural networks have been widely studied for semi-supervised classification on graph-structured data in recent years. They usually learn node representations by transforming, propagating, aggregating node features and minimizing the prediction loss on labeled nodes. However, the pseudo labels generated on unlabeled nodes are usually overlooked during the learning process. In this paper, we propose a soft labels guided graph attention network (SLGAT) to improve the performance of node representation learning by leveraging generated pseudo labels. Unlike the prior graph attention networks, our SLGAT uses soft labels as guidance to learn different weights for neighboring nodes, which allows SLGAT to pay more attention to the features closely related to the central node labels during the feature aggregation process. We further propose a self-training based optimization method to train SLGAT on both labeled and pseudo labeled nodes. Specifically, we first pre-train SLGAT on labeled nodes and generate pseudo labels for unlabeled nodes. Next, for each iteration, we train SLGAT on the combination of labeled and pseudo labeled nodes, and then generate new pseudo labels for further training. Experimental results on semi-supervised node classification show that SLGAT achieves state-of-the-art performance.","publish_time":1587081600000,"author_summary":" Wang, Yubin; Zhang, Zhenyu; Liu, Tingwen; Guo,<br>Li","abstract_summary":" Graph convolutional neural networks have been<br>widely studied for semi-supervised classification<br>on graph-structured data in recent years. They<br>usually learn node representations by transforming,<br>propagating, aggregating node features and minimizing the<br>prediction loss on labeled nodes. However, the pseudo<br>labels generated on unlabeled nodes are usually<br>overlooked during the learning process. In this paper, we<br>propose a soft labels guided graph attention network<br>(SLGAT) to improve the performance of node<br>representation learning by leveraging generated pseudo<br>labels. Unlike the prior graph attention networks, our<br>SLGAT uses soft labels as guidance to learn different<br>weights for neighboring nodes, which allows SLGAT...","title_summary":" SLGAT: Soft Labels Guided Graph Attention<br>Networks","x":-31.2355709076,"y":35.6966209412,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2355709076,"tsne_y":35.6966209412,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"f90vnog5","source_x":"PMC","title":"Fashion Recommendation with Multi-relational Representation Learning","doi":"10.1007\/978-3-030-47426-3_1","abstract":"Driven by increasing demands of assisting users to dress and match clothing properly, fashion recommendation has attracted wide attention. Its core idea is to model the compatibility among fashion items by jointly projecting embedding into a unified space. However, modeling the item compatibility in such a category-agnostic manner could barely preserve intra-class variance, thus resulting in sub-optimal performance. In this paper, we propose a novel category-aware metric learning framework, which not only learns the cross-category compatibility notions but also preserves the intra-category diversity among items. Specifically, we define a category complementary relation representing a pair of category labels, e.g., tops-bottoms. Given a pair of item embeddings, we first project them to their corresponding relation space, then model the mutual relation of a pair of categories as a relation transition vector to capture compatibility amongst fashion items. We further derive a negative sampling strategy with non-trivial instances to enable the generation of expressive and discriminative item representations. Comprehensive experimental results conducted on two public datasets demonstrate the superiority and feasibility of our proposed approach.","publish_time":1587081600000,"author_summary":" Li, Yang; Luo, Yadan; Huang, Zi","abstract_summary":" Driven by increasing demands of assisting<br>users to dress and match clothing properly, fashion<br>recommendation has attracted wide attention. Its core idea is<br>to model the compatibility among fashion items by<br>jointly projecting embedding into a unified space.<br>However, modeling the item compatibility in such a<br>category-agnostic manner could barely preserve intra-class<br>variance, thus resulting in sub-optimal performance. In<br>this paper, we propose a novel category-aware<br>metric learning framework, which not only learns the<br>cross-category compatibility notions but also preserves the<br>intra-category diversity among items. Specifically, we<br>define a category complementary relation<br>representing a pair of category labels, e.g., tops-bottoms....","title_summary":" Fashion Recommendation with<br>Multi-relational Representation Learning","x":-31.6118965149,"y":34.3825912476,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.6118965149,"tsne_y":34.3825912476,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"syamq5gs","source_x":"PMC","title":"JarKA: Modeling Attribute Interactions for Cross-lingual Knowledge Alignment","doi":"10.1007\/978-3-030-47426-3_65","abstract":"Cross-lingual knowledge alignment is the cornerstone in building a comprehensive knowledge graph (KG), which can benefit various knowledge-driven applications. As the structures of KGs are usually sparse, attributes of entities may play an important role in aligning the entities. However, the heterogeneity of the attributes across KGs prevents from accurately embedding and comparing entities. To deal with the issue, we propose to model the interactions between attributes, instead of globally embedding an entity with all the attributes. We further propose a joint framework to merge the alignments inferred from the attributes and the structures. Experimental results show that the proposed model outperforms the state-of-art baselines by up to 38.48% HitRatio@1. The results also demonstrate that our model can infer the alignments between attributes, relationships and values, in addition to entities.","publish_time":1587081600000,"author_summary":" Chen, Bo; Zhang, Jing; Tang, Xiaobin; Chen,<br>Hong; Li, Cuiping","abstract_summary":" Cross-lingual knowledge alignment is the<br>cornerstone in building a comprehensive knowledge graph<br>(KG), which can benefit various knowledge-driven<br>applications. As the structures of KGs are usually sparse,<br>attributes of entities may play an important role in<br>aligning the entities. However, the heterogeneity of<br>the attributes across KGs prevents from<br>accurately embedding and comparing entities. To deal with<br>the issue, we propose to model the interactions<br>between attributes, instead of globally embedding an<br>entity with all the attributes. We further propose a<br>joint framework to merge the alignments inferred<br>from the attributes and the structures.<br>Experimental results show that the proposed...","title_summary":" JarKA: Modeling Attribute Interactions for<br>Cross-lingual Knowledge Alignment","x":-33.015838623,"y":34.8347129822,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.015838623,"tsne_y":34.8347129822,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"qgbn6oii","source_x":"PMC","title":"Attribute-Driven Capsule Network for Entity Relation Prediction","doi":"10.1007\/978-3-030-47426-3_52","abstract":"Multi-attribute entity relation prediction is a novel data mining application about designing an intelligent system that supports inferencing across attributes information. However, most existing deep learning methods capture the inner structural information between different attributes are far more limited. In this paper, we propose an attribute-driven approach for entity relation prediction task based on capsule networks that have been shown to demonstrate good performance on relation mining. We develop a self-attention routing method to encapsulate multiple attributes semantic representation into relational semantic capsules and using dynamic routing method to generate class capsules for predicting relations. Due to the lack of multi-attribute entity relation data is a major obstacle in this task, we construct a new real-world multi-attribute entity relation dataset in this work. Experimental results show significant superiority of our model, as compared with other baselines.","publish_time":1587081600000,"author_summary":" Chen, Jiayin; Gong, Xiaolong; Chen, Xi; Ma,<br>Zhiyi","abstract_summary":" Multi-attribute entity relation prediction<br>is a novel data mining application about<br>designing an intelligent system that supports<br>inferencing across attributes information. However, most<br>existing deep learning methods capture the inner<br>structural information between different attributes are<br>far more limited. In this paper, we propose an<br>attribute-driven approach for entity relation prediction task<br>based on capsule networks that have been shown to<br>demonstrate good performance on relation mining. We<br>develop a self-attention routing method to<br>encapsulate multiple attributes semantic representation<br>into relational semantic capsules and using<br>dynamic routing method to generate class capsules for<br>predicting relations. Due to the lack of...","title_summary":" Attribute-Driven Capsule Network for Entity<br>Relation Prediction","x":-32.0006256104,"y":35.2769126892,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.0006256104,"tsne_y":35.2769126892,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"u85tjubk","source_x":"PMC","title":"Multi-level Memory Network with CRFs for Keyphrase Extraction","doi":"10.1007\/978-3-030-47426-3_56","abstract":"Keyphrase, that concisely describe the high-level topics discussed in a document, are very useful for a wide range of natural language processing (NLP) tasks. Current popular supervised methods for keyphrase extraction commonly cannot effectively utilize the long-range contextual information in text. In this paper, we focus on how to effectively exploit the long-range contextual information to improve the keyphrase extraction performance. Specifically, we propose a multi-level memory network with the conditional random fields (CRFs), which allows to have unrestricted access to the long-range and local contextual information in text. We first design the multi-level memory network with sentence level and document level to enhance the text representation. Then, we integrate the multi-level memory network with the CRFs, which has an advantage in modeling the local contextual information. Compared with the recent state-of-the-art methods, our model can achieve better results through experiments on two datasets.","publish_time":1587081600000,"author_summary":" Zhou, Tao; Zhang, Yuxiang; Zhu, Haoxiang","abstract_summary":" Keyphrase, that concisely describe the<br>high-level topics discussed in a document, are very useful<br>for a wide range of natural language processing<br>(NLP) tasks. Current popular supervised methods for<br>keyphrase extraction commonly cannot effectively<br>utilize the long-range contextual information in<br>text. In this paper, we focus on how to effectively<br>exploit the long-range contextual information to<br>improve the keyphrase extraction performance.<br>Specifically, we propose a multi-level memory network with<br>the conditional random fields (CRFs), which<br>allows to have unrestricted access to the long-range<br>and local contextual information in text. We first<br>design the multi-level memory network with sentence<br>level and...","title_summary":" Multi-level Memory Network with CRFs for<br>Keyphrase Extraction","x":-33.9067955017,"y":32.6195526123,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9067955017,"tsne_y":32.6195526123,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"qlxqfn2w","source_x":"PMC","title":"Inter-sentence and Implicit Causality Extraction from Chinese Corpus","doi":"10.1007\/978-3-030-47426-3_57","abstract":"Automatically extracting causal relations from texts is a challenging task in Natural Language Processing (NLP). Most existing methods focus on extracting intra-sentence or explicit causality, while neglecting the causal relations that expressed implicitly or hidden in inter-sentences. In this paper, we propose Cascaded multi-Structure Neural Network (CSNN), a novel and unified model that extract inter-sentence or implicit causal relations from Chinese Corpus, without relying on external knowledge. The model employs Convolutional Neural Network (CNN) to capture important features as well as causal structural pattern. Self-attention mechanism is designed to mine semantic and relevant characteristics between different features. The output of CNN and self-attention structure are concatenated as higher-level phrase representations. Then Conditional Random Field (CRF) layer is employed to calculate the label of each word in inter-sentence or implicit causal relation sentences, which improves the performance of inter-sentence or implicit causality extraction. Experimental results show that the proposed model achieves state-of-the-art results, improved on three datasets, when compared with other methods.","publish_time":1587081600000,"author_summary":" Jin, Xianxian; Wang, Xinzhi; Luo, Xiangfeng;<br>Huang, Subin; Gu, Shengwei","abstract_summary":" Automatically extracting causal relations<br>from texts is a challenging task in Natural Language<br>Processing (NLP). Most existing methods focus on<br>extracting intra-sentence or explicit causality, while<br>neglecting the causal relations that expressed<br>implicitly or hidden in inter-sentences. In this paper, we<br>propose Cascaded multi-Structure Neural Network<br>(CSNN), a novel and unified model that extract<br>inter-sentence or implicit causal relations from Chinese<br>Corpus, without relying on external knowledge. The<br>model employs Convolutional Neural Network (CNN) to<br>capture important features as well as causal<br>structural pattern. Self-attention mechanism is<br>designed to mine semantic and relevant characteristics<br>between different features. The output of...","title_summary":" Inter-sentence and Implicit Causality<br>Extraction from Chinese Corpus","x":-33.7243843079,"y":32.1124763489,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7243843079,"tsne_y":32.1124763489,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"tk768wnh","source_x":"PMC","title":"CACRNN: A Context-Aware Attention-Based Convolutional Recurrent Neural Network for Fine-Grained Taxi Demand Prediction","doi":"10.1007\/978-3-030-47426-3_49","abstract":"As taxis are primary public transport in metropolises, accurately predicting fine-grained taxi demands of passengers in real time is important for guiding drivers to plan their routes and reducing the waiting time of passengers. Many efforts have been paid to provide accurate taxi demand prediction, and deep neural networks are leveraged recently. However, existing works are limited in properly incorporating multi-view taxi demand predictions together, by simply assigning fixed weights learned by training to the predictions of each region. To solve this problem, we apply the attention mechanism for leveraging contextual information to assist prediction, and a context-aware attention-based convolutional recurrent neural network (CACRNN) is proposed. Specially, we forecast fine-grained taxi demands with considering multi-view features, including spatial correlations among adjacent regions, short-term periodicity, long-term periodicity, and impacts of external factors. Local convolutional (LC) layers and gated recurrent units (GRUs) are utilized to extract the features from historical records. Moreover, a context-aware attention module is employed to incorporate the predictions of each region with considering different features, which is our novel attempt. This module assigns different weights to the predictions of a region according to its contextual information such as weather, index of time slots, and region function. We conduct comprehensive experiments based on a large-scale real-world dataset from New York City, and the results show that our method outperforms state-of-the-art baselines.","publish_time":1587081600000,"author_summary":" Wu, Wenbin; Liu, Tong; Yang, Jiahao","abstract_summary":" As taxis are primary public transport in<br>metropolises, accurately predicting fine-grained taxi<br>demands of passengers in real time is important for<br>guiding drivers to plan their routes and reducing the<br>waiting time of passengers. Many efforts have been paid<br>to provide accurate taxi demand prediction, and<br>deep neural networks are leveraged recently.<br>However, existing works are limited in properly<br>incorporating multi-view taxi demand predictions together,<br>by simply assigning fixed weights learned by<br>training to the predictions of each region. To solve this<br>problem, we apply the attention mechanism for<br>leveraging contextual information to assist prediction,<br>and a context-aware attention-based<br>convolutional...","title_summary":" CACRNN: A Context-Aware Attention-Based<br>Convolutional Recurrent Neural Network for Fine-Grained<br>Taxi Demand Prediction","x":-33.6982383728,"y":32.0035896301,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6982383728,"tsne_y":32.0035896301,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"8gaeosyr","source_x":"PMC","title":"A Hybrid Recommendation for Music Based on Reinforcement Learning","doi":"10.1007\/978-3-030-47426-3_8","abstract":"The key to personalized recommendation system is the prediction of users\u2019 preferences. However, almost all existing music recommendation approaches only learn listeners\u2019 preferences based on their historical records or explicit feedback, without considering the simulation of interaction process which can capture the minor changes of listeners\u2019 preferences sensitively. In this paper, we propose a personalized hybrid recommendation algorithm for music based on reinforcement learning (PHRR) to recommend song sequences that match listeners\u2019 preferences better. We firstly use weighted matrix factorization (WMF) and convolutional neural network (CNN) to learn and extract the song feature vectors. In order to capture the changes of listeners\u2019 preferences sensitively, we innovatively enhance simulating interaction process of listeners and update the model continuously based on their preferences both for songs and song transitions. The extensive experiments on real-world datasets validate the effectiveness of the proposed PHRR on song sequence recommendation compared with the state-of-the-art recommendation approaches.","publish_time":1587081600000,"author_summary":" Wang, Yu","abstract_summary":" The key to personalized recommendation system<br>is the prediction of users\u2019 preferences.<br>However, almost all existing music recommendation<br>approaches only learn listeners\u2019 preferences based on<br>their historical records or explicit feedback,<br>without considering the simulation of interaction<br>process which can capture the minor changes of<br>listeners\u2019 preferences sensitively. In this paper, we<br>propose a personalized hybrid recommendation<br>algorithm for music based on reinforcement learning<br>(PHRR) to recommend song sequences that match<br>listeners\u2019 preferences better. We firstly use weighted<br>matrix factorization (WMF) and convolutional neural<br>network (CNN) to learn and extract the song feature<br>vectors. In order to capture the changes...","title_summary":" A Hybrid Recommendation for Music Based on<br>Reinforcement Learning","x":-30.7077674866,"y":34.209980011,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.7077674866,"tsne_y":34.209980011,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"yxh25jmd","source_x":"PMC","title":"Collaborative Recommendation of Temporally-Discounted Tag-Based Expertise for Community Question Answering","doi":"10.1007\/978-3-030-47426-3_4","abstract":"We propose an innovative approach to finding experts for community question answering (CQA). The idea is to recommend answerers, who are credited the highest expertise under question tags at routing time. The expertise of answerers under already replied question tags is intuitively discounted by accounting for the observed tags, votes and temporal information of their answers. Instead, the discounted expertise under not yet replied tags is predicted via a latent-factor representation of both answerers and tags. These representations are inferred by means of Gibbs sampling under a new Bayesian probabilistic model of discounted user expertise and asking-answering behavior. The devised model unprecedentedly explains the latter two CQA aspects as the result of a generative process, that seamlessly integrates probabilistic matrix factorization and network behavior characterization. An extensive comparative experimentation over real-world CQA data demonstrates that our approach outperforms several-state-of-the-art competitors in recommendation effectiveness.","publish_time":1587081600000,"author_summary":" Costa, Gianni; Ortale, Riccardo","abstract_summary":" We propose an innovative approach to finding<br>experts for community question answering (CQA). The<br>idea is to recommend answerers, who are credited the<br>highest expertise under question tags at routing time.<br>The expertise of answerers under already replied<br>question tags is intuitively discounted by accounting<br>for the observed tags, votes and temporal<br>information of their answers. Instead, the discounted<br>expertise under not yet replied tags is predicted via a<br>latent-factor representation of both answerers and tags.<br>These representations are inferred by means of Gibbs<br>sampling under a new Bayesian probabilistic model of<br>discounted user expertise and asking-answering<br>behavior. The devised...","title_summary":" Collaborative Recommendation of<br>Temporally-Discounted Tag-Based Expertise for Community Question<br>Answering","x":-32.4289588928,"y":33.9830360413,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.4289588928,"tsne_y":33.9830360413,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"1c3w2cvz","source_x":"PMC","title":"Estimating Descriptors for Large Graphs","doi":"10.1007\/978-3-030-47426-3_60","abstract":"Embedding networks into a fixed dimensional feature space, while preserving its essential structural properties is a fundamental task in graph analytics. These feature vectors (graph descriptors) are used to measure the pairwise similarity between graphs. This enables applying data mining algorithms (e.g classification, clustering, or anomaly detection) on graph-structured data which have numerous applications in multiple domains. State-of-the-art algorithms for computing descriptors require the entire graph to be in memory, entailing a huge memory footprint, and thus do not scale well to increasing sizes of real-world networks. In this work, we propose streaming algorithms to efficiently approximate descriptors by estimating counts of sub-graphs of order [Formula: see text], and thereby devise extensions of two existing graph comparison paradigms: the Graphlet Kernel and NetSimile. Our algorithms require a single scan over the edge stream, have space complexity that is a fraction of the input size, and approximate embeddings via a simple sampling scheme. Our design exploits the trade-off between available memory and estimation accuracy to provide a method that works well for limited memory requirements. We perform extensive experiments on real-world networks and demonstrate that our algorithms scale well to massive graphs.","publish_time":1587081600000,"author_summary":" Hassan, Zohair Raza; Shabbir, Mudassir; Khan,<br>Imdadullah; Abbas, Waseem","abstract_summary":" Embedding networks into a fixed dimensional<br>feature space, while preserving its essential<br>structural properties is a fundamental task in graph<br>analytics. These feature vectors (graph descriptors) are<br>used to measure the pairwise similarity between<br>graphs. This enables applying data mining algorithms<br>(e.g classification, clustering, or anomaly<br>detection) on graph-structured data which have numerous<br>applications in multiple domains. State-of-the-art<br>algorithms for computing descriptors require the entire<br>graph to be in memory, entailing a huge memory<br>footprint, and thus do not scale well to increasing sizes of<br>real-world networks. In this work, we propose streaming<br>algorithms to efficiently approximate descriptors by<br>estimating...","title_summary":" Estimating Descriptors for Large Graphs","x":-31.2536811829,"y":35.9786720276,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2536811829,"tsne_y":35.9786720276,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"tykvzjk0","source_x":"PMC","title":"Relation Embedding for Personalised Translation-Based POI Recommendation","doi":"10.1007\/978-3-030-47426-3_5","abstract":"Point-of-Interest (POI) recommendation is one of the most important location-based services helping people discover interesting venues or services. However, the extreme user-POI matrix sparsity and the varying spatio-temporal context pose challenges for POI systems, which affects the quality of POI recommendations. To this end, we propose a translation-based relation embedding for POI recommendation. Our approach encodes the temporal and geographic information, as well as semantic contents effectively in a low-dimensional relation space by using Knowledge Graph Embedding techniques. To further alleviate the issue of user-POI matrix sparsity, a combined matrix factorization framework is built on a user-POI graph to enhance the inference of dynamic personal interests by exploiting the side-information. Experiments on two real-world datasets demonstrate the effectiveness of our proposed model.","publish_time":1587081600000,"author_summary":" Wang, Xianjing; Salim, Flora D.; Ren, Yongli;<br>Koniusz, Piotr","abstract_summary":" Point-of-Interest (POI) recommendation is<br>one of the most important location-based services<br>helping people discover interesting venues or<br>services. However, the extreme user-POI matrix sparsity<br>and the varying spatio-temporal context pose<br>challenges for POI systems, which affects the quality of<br>POI recommendations. To this end, we propose a<br>translation-based relation embedding for POI recommendation.<br>Our approach encodes the temporal and geographic<br>information, as well as semantic contents effectively in a<br>low-dimensional relation space by using Knowledge Graph<br>Embedding techniques. To further alleviate the issue of<br>user-POI matrix sparsity, a combined matrix<br>factorization framework is built on a user-POI graph to<br>enhance...","title_summary":" Relation Embedding for Personalised<br>Translation-Based POI Recommendation","x":-30.8917827606,"y":34.4472961426,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8917827606,"tsne_y":34.4472961426,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"ganhlq8s","source_x":"PMC","title":"Accurate News Recommendation Coalescing Personal and Global Temporal Preferences","doi":"10.1007\/978-3-030-47426-3_7","abstract":"Given session-based news watch history of users, how can we precisely recommend news articles? Unlike other items for recommendation, the worth of news articles decays quickly and various news sources publish fresh ones every second. Moreover, people frequently select news articles regardless of their personal preferences to understand popular topics at a specific time. Conventional recommendation methods, designed for other recommendation domains, give low performance because of these peculiarities of news articles. In this paper, we propose PGT (News Recommendation Coalescing Personal and Global Temporal Preferences), an accurate news recommendation method designed with consideration of the above characteristics of news articles. PGT extracts latent features from both personal and global temporal preferences to sufficiently reflect users\u2019 behaviors. Furthermore, we propose an attention based architecture to extract adequate coalesced features from both of the preferences. Experimental results show that PGT provides the most accurate news recommendation, giving the state-of-the-art accuracy.","publish_time":1587081600000,"author_summary":" Koo, Bonhun; Jeon, Hyunsik; Kang, U","abstract_summary":" Given session-based news watch history of<br>users, how can we precisely recommend news articles?<br>Unlike other items for recommendation, the worth of<br>news articles decays quickly and various news<br>sources publish fresh ones every second. Moreover,<br>people frequently select news articles regardless of<br>their personal preferences to understand popular<br>topics at a specific time. Conventional<br>recommendation methods, designed for other recommendation<br>domains, give low performance because of these<br>peculiarities of news articles. In this paper, we propose PGT<br>(News Recommendation Coalescing Personal and<br>Global Temporal Preferences), an accurate news<br>recommendation method designed with consideration of the<br>above characteristics of news...","title_summary":" Accurate News Recommendation Coalescing<br>Personal and Global Temporal Preferences","x":-31.246837616,"y":33.6535758972,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.246837616,"tsne_y":33.6535758972,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"b0l8gzke","source_x":"PMC","title":"TemporalGAT: Attention-Based Dynamic Graph Representation Learning","doi":"10.1007\/978-3-030-47426-3_32","abstract":"Learning representations for dynamic graphs is fundamental as it supports numerous graph analytic tasks such as dynamic link prediction, node classification, and visualization. Real-world dynamic graphs are continuously evolved where new nodes and edges are introduced or removed during graph evolution. Most existing dynamic graph representation learning methods focus on modeling dynamic graphs with fixed nodes due to the complexity of modeling dynamic graphs, and therefore, cannot efficiently learn the evolutionary patterns of real-world evolving graphs. Moreover, existing methods generally model the structural information of evolving graphs separately from temporal information. This leads to the loss of important structural and temporal information that could cause the degradation of predictive performance of the model. By employing an innovative neural network architecture based on graph attention networks and temporal convolutions, our framework jointly learns graph representations contemplating evolving graph structure and temporal patterns. We propose a deep attention model to learn low-dimensional feature representations which preserves the graph structure and features among series of graph snapshots over time. Experimental results on multiple real-world dynamic graph datasets show that, our proposed method is competitive against various state-of-the-art methods.","publish_time":1587081600000,"author_summary":" Fathy, Ahmed; Li, Kan","abstract_summary":" Learning representations for dynamic graphs<br>is fundamental as it supports numerous graph<br>analytic tasks such as dynamic link prediction, node<br>classification, and visualization. Real-world dynamic graphs<br>are continuously evolved where new nodes and edges<br>are introduced or removed during graph evolution.<br>Most existing dynamic graph representation<br>learning methods focus on modeling dynamic graphs with<br>fixed nodes due to the complexity of modeling dynamic<br>graphs, and therefore, cannot efficiently learn the<br>evolutionary patterns of real-world evolving graphs.<br>Moreover, existing methods generally model the<br>structural information of evolving graphs separately<br>from temporal information. This leads to the loss of<br>important structural and...","title_summary":" TemporalGAT: Attention-Based Dynamic Graph<br>Representation Learning","x":-31.1352806091,"y":35.8586387634,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.1352806091,"tsne_y":35.8586387634,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"7csmy072","source_x":"PMC","title":"Canonicalizing Knowledge Bases for Recruitment Domain","doi":"10.1007\/978-3-030-47436-2_38","abstract":"Online recruitment industry holds large amount of user-generated content in the form of job postings, resumes etc. This content finds its way in the knowledge bases (KB) causing duplicate and non-standard representations of entities (like company names, institute names, designations, skills etc.) These non-standard entity representations impact various applications such as search, recommendations and information retrieval. Therefore, KB canonicalization i.e, mapping multiple references of same entities into unique clusters is imperative for online recruitment platforms. Research suggests various approaches that use enriched semantic context or external context (from sources like Freebase) to perform KB Canonicalization. In fields where such external sources of context do not exist the problem remains challenging. To address these challenges, we propose a novel deep Siamese architecture with character-based attention and word embeddings that (a) estimates pairwise similarity between all entity mentions, and (b) then uses these similarity (scores) to create canonical clusters representing unique entity in the KB. Our experiments on recruitment domain dataset comprising of 62,288 unique entities of various types such as companies, institutes, skills, and designations demonstrate the effectiveness of our approach. We also provide insights on different network architectures, each of which encapsulate a different set of variation while performing canonicalization.","publish_time":1587081600000,"author_summary":" Fatma, Nausheen; Choudhary, Vijay; Sachdeva,<br>Niharika; Rajput, Nitendra","abstract_summary":" Online recruitment industry holds large<br>amount of user-generated content in the form of job<br>postings, resumes etc. This content finds its way in the<br>knowledge bases (KB) causing duplicate and non-standard<br>representations of entities (like company names, institute<br>names, designations, skills etc.) These<br>non-standard entity representations impact various<br>applications such as search, recommendations and<br>information retrieval. Therefore, KB canonicalization<br>i.e, mapping multiple references of same entities<br>into unique clusters is imperative for online<br>recruitment platforms. Research suggests various<br>approaches that use enriched semantic context or external<br>context (from sources like Freebase) to perform KB<br>Canonicalization. In fields where such external...","title_summary":" Canonicalizing Knowledge Bases for<br>Recruitment Domain","x":-33.0618171692,"y":34.3709106445,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.0618171692,"tsne_y":34.3709106445,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"ik11jmok","source_x":"PMC","title":"Exploiting the Matching Information in the Support Set for Few Shot Event Classification","doi":"10.1007\/978-3-030-47436-2_18","abstract":"The existing event classification (EC) work primarily focuses on the traditional supervised learning setting in which models are unable to extract event mentions of new\/unseen event types. Few-shot learning has not been investigated in this area although it enables EC models to extend their operation to unobserved event types. To fill in this gap, in this work, we investigate event classification under the few-shot learning setting. We propose a novel training method for this problem that extensively exploit the support set during the training process of a few-shot learning model. In particular, in addition to matching the query example with those in the support set for training, we seek to further match the examples within the support set themselves. This method provides more training signals for the models and can be applied to every metric-learning-based few-shot learning methods. Our extensive experiments on two benchmark EC datasets show that the proposed method can improve the best reported few-shot learning models by up to 10% on accuracy for event classification.","publish_time":1587081600000,"author_summary":" Lai, Viet Dac; Dernoncourt, Franck; Nguyen,<br>Thien Huu","abstract_summary":" The existing event classification (EC) work<br>primarily focuses on the traditional supervised<br>learning setting in which models are unable to extract<br>event mentions of new\/unseen event types. Few-shot<br>learning has not been investigated in this area although<br>it enables EC models to extend their operation to<br>unobserved event types. To fill in this gap, in this work, we<br>investigate event classification under the few-shot<br>learning setting. We propose a novel training method for<br>this problem that extensively exploit the support<br>set during the training process of a few-shot<br>learning model. In particular, in addition to matching<br>the query example with...","title_summary":" Exploiting the Matching Information in the<br>Support Set for Few Shot Event Classification","x":-32.9920539856,"y":33.3264694214,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.9920539856,"tsne_y":33.3264694214,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"iaa57hsj","source_x":"PMC","title":"Attention-Based Aggregation Graph Networks for Knowledge Graph Information Transfer","doi":"10.1007\/978-3-030-47436-2_41","abstract":"Knowledge graph completion (KGC) aims to predict missing information in a knowledge graph. Many existing embedding-based KGC models solve the Out-of-knowledge-graph (OOKG) entity problem (also known as zero-shot entity problem) by utilizing textual information resources such as descriptions and types. However, few works utilize the extra structural information to generate embeddings. In this paper, we propose a new zero-shot scenario: how to acquire the embedding vector of a relation that is not observed at training time. Our work uses a convolutional transition and attention-based aggregation graph neural network to solve both the OOKG entity problem and the new OOKG relation problem without retraining, regarding the structural neighbors as the auxiliary information. The experimental results show the effectiveness of our proposed models in solving the OOKG relation problem. For the OOKG entity problem, our model performs better than the previous GNN-based model by 23.9% in NELL-995-Tail dataset.","publish_time":1587081600000,"author_summary":" Zhao, Ming; Jia, Weijia; Huang, Yusheng","abstract_summary":" Knowledge graph completion (KGC) aims to<br>predict missing information in a knowledge graph. Many<br>existing embedding-based KGC models solve the<br>Out-of-knowledge-graph (OOKG) entity problem (also known as zero-shot<br>entity problem) by utilizing textual information<br>resources such as descriptions and types. However, few<br>works utilize the extra structural information to<br>generate embeddings. In this paper, we propose a new<br>zero-shot scenario: how to acquire the embedding vector<br>of a relation that is not observed at training<br>time. Our work uses a convolutional transition and<br>attention-based aggregation graph neural network to solve both<br>the OOKG entity problem and the new OOKG relation...","title_summary":" Attention-Based Aggregation Graph Networks<br>for Knowledge Graph Information Transfer","x":-31.7381458282,"y":35.4134864807,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.7381458282,"tsne_y":35.4134864807,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"mu6w6o24","source_x":"PMC","title":"EMOVA: A Semi-supervised End-to-End Moving-Window Attentive Framework for Aspect Mining","doi":"10.1007\/978-3-030-47436-2_61","abstract":"Aspect mining or extraction is one of the most challenging problems in aspect-level analysis on customer reviews; it aims to extract terms from a review describing aspects of a reviewed entity, e.g., a product or service. As aspect mining can be formulated as the sequence labeling problem, supervised deep sequence learning models have recently achieved the best performance. However, these supervised models require a large amount of labeled data which are usually very costly or unavailable. To this end, we propose a semi-supervised End-to-end MOVing-window Attentive framework (called EMOVA) that has three key features for aspect mining. (1) Two neural layers with Bidirectional Long Short-Term Memory (BiLSTM) are employed to learn representations of reviews. (2) Cross-View Training (CVT) is used to improve the representation learning over a small set of labeled reviews and a large set of unlabeled reviews from the same domain in a unified end-to-end architecture. (3) Since past nearby information in a text provides important semantic contexts for a prediction task in aspect mining, a moving-window attention component is proposed in EMOVA to enhance prediction accuracy. Experimental results over four review datasets from the SemEval workshops show that EMOVA outperforms the state-of-the-art models for aspect mining.","publish_time":1587081600000,"author_summary":" Li, Ning; Chow, Chi-Yin; Zhang, Jia-Dong","abstract_summary":" Aspect mining or extraction is one of the most<br>challenging problems in aspect-level analysis on customer<br>reviews; it aims to extract terms from a review<br>describing aspects of a reviewed entity, e.g., a product or<br>service. As aspect mining can be formulated as the<br>sequence labeling problem, supervised deep sequence<br>learning models have recently achieved the best<br>performance. However, these supervised models require a<br>large amount of labeled data which are usually very<br>costly or unavailable. To this end, we propose a<br>semi-supervised End-to-end MOVing-window Attentive<br>framework (called EMOVA) that has three key features for<br>aspect mining. (1) Two neural...","title_summary":" EMOVA: A Semi-supervised End-to-End<br>Moving-Window Attentive Framework for Aspect Mining","x":-33.7753257751,"y":32.0699920654,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7753257751,"tsne_y":32.0699920654,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"avakkpku","source_x":"PMC","title":"Optimized Transformer Models for FAQ Answering","doi":"10.1007\/978-3-030-47426-3_19","abstract":"Informational chatbots provide a highly effective medium for improving operational efficiency in answering customer queries for any enterprise. Chatbots are also preferred by users\/customers since unlike other alternatives like calling customer care or browsing over FAQ pages, chatbots provide instant responses, are easy to use, are less invasive and are always available. In this paper, we discuss the problem of FAQ answering which is central to designing a retrieval-based informational chatbot. Given a set of FAQ pages s for an enterprise, and a user query, we need to find the best matching question-answer pairs from s. Building such a semantic ranking system that works well across domains for large QA databases with low runtime and model size is challenging. Previous work based on feature engineering or recurrent neural models either provides low accuracy or incurs high runtime costs. We experiment with multiple transformer based deep learning models, and also propose a novel MT-DNN (Multi-task Deep Neural Network)-based architecture, which we call Masked MT-DNN (or MMT-DNN). MMT-DNN significantly outperforms other state-of-the-art transformer models for the FAQ answering task. Further, we propose an improved knowledge distillation component to achieve [Formula: see text]2.4x reduction in model-size and [Formula: see text]7x reduction in runtime while maintaining similar accuracy. On a small benchmark dataset from SemEval 2017 CQA Task 3, we show that our approach provides an NDCG@1 of 83.1. On another large dataset of [Formula: see text]281K instances corresponding to [Formula: see text]30K queries from diverse domains, our distilled 174 MB model provides an NDCG@1 of 75.08 with a CPU runtime of mere 31 ms establishing a new state-of-the-art for FAQ answering.","publish_time":1587081600000,"author_summary":" Damani, Sonam; Narahari, Kedhar Nath;<br>Chatterjee, Ankush; Gupta, Manish; Agrawal, Puneet","abstract_summary":" Informational chatbots provide a highly<br>effective medium for improving operational efficiency<br>in answering customer queries for any<br>enterprise. Chatbots are also preferred by<br>users\/customers since unlike other alternatives like calling<br>customer care or browsing over FAQ pages, chatbots<br>provide instant responses, are easy to use, are less<br>invasive and are always available. In this paper, we<br>discuss the problem of FAQ answering which is central to<br>designing a retrieval-based informational chatbot.<br>Given a set of FAQ pages s for an enterprise, and a user<br>query, we need to find the best matching<br>question-answer pairs from s. Building such a semantic...","title_summary":" Optimized Transformer Models for FAQ<br>Answering","x":-34.9813804626,"y":32.7862014771,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.9813804626,"tsne_y":32.7862014771,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"zaautmhe","source_x":"PMC","title":"Context-Aware Latent Dirichlet Allocation for Topic Segmentation","doi":"10.1007\/978-3-030-47426-3_37","abstract":"We propose a new generative model for topic segmentation based on Latent Dirichlet Allocation. The task is to divide a document into a sequence of topically coherent segments, while preserving long topic change-points (coherency) and keeping short topic segments from getting merged (saliency). Most of the existing models either fuse topic segments by keywords or focus on modeling word co-occurrence patterns without merging. They can hardly achieve both coherency and saliency since many words have high uncertainties in topic assignments due to their polysemous nature. To solve this problem, we introduce topic-specific co-occurrence of word pairs within contexts in modeling, to generate more coherent segments and alleviate the influence of irrelevant words on topic assignment. We also design an optimization algorithm to eliminate redundant items in the generated topic segments. Experimental results show that our proposal produces significant improvements in both topic coherence and topic segmentation.","publish_time":1587081600000,"author_summary":" Li, Wenbo; Matsukawa, Tetsu; Saigo, Hiroto;<br>Suzuki, Einoshin","abstract_summary":" We propose a new generative model for topic<br>segmentation based on Latent Dirichlet Allocation. The task<br>is to divide a document into a sequence of<br>topically coherent segments, while preserving long<br>topic change-points (coherency) and keeping short<br>topic segments from getting merged (saliency). Most<br>of the existing models either fuse topic segments<br>by keywords or focus on modeling word<br>co-occurrence patterns without merging. They can hardly<br>achieve both coherency and saliency since many words<br>have high uncertainties in topic assignments due to<br>their polysemous nature. To solve this problem, we<br>introduce topic-specific co-occurrence of word pairs<br>within contexts in modeling,...","title_summary":" Context-Aware Latent Dirichlet Allocation<br>for Topic Segmentation","x":-32.6444664001,"y":33.7234420776,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.6444664001,"tsne_y":33.7234420776,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"sdk0w6vf","source_x":"PMC","title":"Improving Multi-turn Response Selection Models with Complementary Last-Utterance Selection by Instance Weighting","doi":"10.1007\/978-3-030-47436-2_36","abstract":"Open-domain retrieval-based dialogue systems require a considerable amount of training data to learn their parameters. However, in practice, the negative samples of training data are usually selected from an unannotated conversation data set at random. The generated training data is likely to contain noise and affect the performance of the response selection models. To address this difficulty, we consider utilizing the underlying correlation in the data resource itself to derive different kinds of supervision signals and reduce the influence of noisy data. More specially, we consider a main-complementary task pair. The main task (i.e., our focus) selects the correct response given the last utterance and context, and the complementary task selects the last utterance given the response and context. The key point is that the output of the complementary task is used to set instance weights for the main task. We conduct extensive experiments in two public datasets and obtain significant improvement in both datasets. We also investigate the variant of our approach in multiple aspects, and the results have verified the effectiveness of our approach.","publish_time":1587081600000,"author_summary":" Zhou, Kun; Zhao, Wayne Xin; Zhu, Yutao; Wen,<br>Ji-Rong; Yu, Jingsong","abstract_summary":" Open-domain retrieval-based dialogue<br>systems require a considerable amount of training data<br>to learn their parameters. However, in practice,<br>the negative samples of training data are usually<br>selected from an unannotated conversation data set at<br>random. The generated training data is likely to<br>contain noise and affect the performance of the<br>response selection models. To address this difficulty,<br>we consider utilizing the underlying<br>correlation in the data resource itself to derive different<br>kinds of supervision signals and reduce the<br>influence of noisy data. More specially, we consider a<br>main-complementary task pair. The main task (i.e., our focus)<br>selects the correct response...","title_summary":" Improving Multi-turn Response Selection<br>Models with Complementary Last-Utterance Selection<br>by Instance Weighting","x":-35.6484107971,"y":31.8811130524,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.6484107971,"tsne_y":31.8811130524,"subcluster":13,"subcluster_description":"Multi-Turn Response Selection Models","shape":"p"},{"cord_uid":"t6cyxo6l","source_x":"PMC","title":"Multi-information Source HIN for Medical Concept Embedding","doi":"10.1007\/978-3-030-47436-2_30","abstract":"Learning low-dimensional representations for medical concepts is of great importance in improving public healthcare applications such as computer-aided diagnosis systems. Existing methods rely on Electronic Health Records (EHR) as their only information source and do not make use of abundant available external medical knowledge, and therefore they ignore the correlations between medical concepts. To address this issue, we propose a novel multi-information source Heterogeneous Information Network (HIN) to model EHR while incorporating external medical knowledge including ICD-9-CM and MeSH for an enriched network schema. Our model is well aware of the structure of EHR as well as the correlations between medical concepts it refers to, and learns semantically reflective medical concept embeddings. In experiments, our model outperforms unsupervised baselines in a variety of medical data mining tasks.","publish_time":1587081600000,"author_summary":" Cao, Yuwei; Peng, Hao; Yu, Philip S.","abstract_summary":" Learning low-dimensional representations<br>for medical concepts is of great importance in<br>improving public healthcare applications such as<br>computer-aided diagnosis systems. Existing methods rely on<br>Electronic Health Records (EHR) as their only information<br>source and do not make use of abundant available<br>external medical knowledge, and therefore they ignore<br>the correlations between medical concepts. To<br>address this issue, we propose a novel<br>multi-information source Heterogeneous Information Network<br>(HIN) to model EHR while incorporating external<br>medical knowledge including ICD-9-CM and MeSH for an<br>enriched network schema. Our model is well aware of the<br>structure of EHR as well as the correlations between...","title_summary":" Multi-information Source HIN for Medical<br>Concept Embedding","x":-31.786781311,"y":32.3055152893,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.786781311,"tsne_y":32.3055152893,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"cea8px16","source_x":"PMC","title":"A Relation Learning Hierarchical Framework for Multi-label Charge Prediction","doi":"10.1007\/978-3-030-47436-2_55","abstract":"In legal field, multi-label charge prediction is a popular and foundational task to predict charges (labels) by a case description (a fact). From perspectives of content analysis and label decision, there are two major difficulties. One is content confusion that the case descriptions of some charges are almost identical. The other is dynamic label number that the numbers of labels (label number) of different cases may be different. In this paper, we propose a relation learning hierarchical framework for multi-label charge prediction with two models, i.e., dynamic merging attention (DMA) and number learning network (NLN). Specially, DMA can improve the charge prediction performance by dynamically learning the similarity relation between a fact and external knowledge (provisions) and the difference relation between different provisions, which alleviates the phenomenon of content confusion. NLN mitigates the dynamic label number by learning the co-occurring relation between labels. Moreover, we put the two models into a unified framework to enhance their effects. Conducted on a public large real-world law dataset, experimental results demonstrate that our framework with DMA and NLN outperforms well-known baselines by more than 3%\u201323%.","publish_time":1587081600000,"author_summary":" Duan, Wei; Li, Lin; Yu, Yi","abstract_summary":" In legal field, multi-label charge prediction<br>is a popular and foundational task to predict<br>charges (labels) by a case description (a fact). From<br>perspectives of content analysis and label decision, there<br>are two major difficulties. One is content<br>confusion that the case descriptions of some charges are<br>almost identical. The other is dynamic label number<br>that the numbers of labels (label number) of<br>different cases may be different. In this paper, we<br>propose a relation learning hierarchical framework<br>for multi-label charge prediction with two<br>models, i.e., dynamic merging attention (DMA) and<br>number learning network (NLN). Specially, DMA can<br>improve the...","title_summary":" A Relation Learning Hierarchical Framework<br>for Multi-label Charge Prediction","x":-32.3290519714,"y":33.0224685669,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.3290519714,"tsne_y":33.0224685669,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"4dxs4ry4","source_x":"PMC","title":"Modeling POI-Specific Spatial-Temporal Context for Point-of-Interest Recommendation","doi":"10.1007\/978-3-030-47426-3_11","abstract":"Point-of-Interest (POI) recommendation is a fundamental task in location-based social networks. Different from traditional item recommendation, POI recommendation is highly context-dependent: (1) geographical influence, e.g., users prefer to visit POIs that are not far away; (2) time-sensitivity, e.g., restaurants are preferred in dinner time; (3) dependency in a user\u2019s check-in sequence, e.g., POIs planned in a trip. Yet, existing methods either partially leverage such context information or combine different types of contexts using a global weighting scheme, failing to capture the phenomenon that the importance of each context is also context-dependent rather than the same for all recommendation. In this paper, we propose a model to exploit spatial-temporal contexts in a POI-guided attention mechanism for POI recommendation. Such an attention mechanism offers us high flexibility to capture the POI-specific importance of each context. Experimental results on two real-world datasets collected from Foursquare and Gowalla demonstrate that the POI-specific context importance significantly improves the performance of POI recommendation.","publish_time":1587081600000,"author_summary":" Wang, Hao; Shen, Huawei; Cheng, Xueqi","abstract_summary":" Point-of-Interest (POI) recommendation is a<br>fundamental task in location-based social networks.<br>Different from traditional item recommendation, POI<br>recommendation is highly context-dependent: (1)<br>geographical influence, e.g., users prefer to visit POIs<br>that are not far away; (2) time-sensitivity, e.g.,<br>restaurants are preferred in dinner time; (3) dependency in<br>a user\u2019s check-in sequence, e.g., POIs planned<br>in a trip. Yet, existing methods either partially<br>leverage such context information or combine different<br>types of contexts using a global weighting scheme,<br>failing to capture the phenomenon that the importance<br>of each context is also context-dependent rather<br>than the same for all recommendation. In this...","title_summary":" Modeling POI-Specific Spatial-Temporal<br>Context for Point-of-Interest Recommendation","x":-30.6061248779,"y":34.3192138672,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6061248779,"tsne_y":34.3192138672,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"6rw3atf1","source_x":"PMC","title":"Chinese Sentence Semantic Matching Based on Multi-Granularity Fusion Model","doi":"10.1007\/978-3-030-47436-2_19","abstract":"Sentence semantic matching is the cornerstone of many natural language processing tasks, including Chinese language processing. It is well known that Chinese sentences with different polysemous words or word order may have totally different semantic meanings. Thus, to represent and match the sentence semantic meaning accurately, one challenge that must be solved is how to capture the semantic features from the multi-granularity perspective, e.g., characters and words. To address the above challenge, we propose a novel sentence semantic matching model which is based on the fusion of semantic features from character-granularity and word-granularity, respectively. Particularly, the multi-granularity fusion intends to extract more semantic features to better optimize the downstream sentence semantic matching. In addition, we propose the equilibrium cross-entropy, a novel loss function, by setting mean square error (MSE) as an equilibrium factor of cross-entropy. The experimental results conducted on Chinese open data set demonstrate that our proposed model combined with binary equilibrium cross-entropy loss function is superior to the existing state-of-the-art sentence semantic matching models.","publish_time":1587081600000,"author_summary":" Zhang, Xu; Lu, Wenpeng; Zhang, Guoqiang; Li,<br>Fangfang; Wang, Shoujin","abstract_summary":" Sentence semantic matching is the cornerstone<br>of many natural language processing tasks,<br>including Chinese language processing. It is well known<br>that Chinese sentences with different polysemous<br>words or word order may have totally different<br>semantic meanings. Thus, to represent and match the<br>sentence semantic meaning accurately, one challenge<br>that must be solved is how to capture the semantic<br>features from the multi-granularity perspective,<br>e.g., characters and words. To address the above<br>challenge, we propose a novel sentence semantic matching<br>model which is based on the fusion of semantic<br>features from character-granularity and<br>word-granularity, respectively. Particularly, the<br>multi-granularity fusion intends to...","title_summary":" Chinese Sentence Semantic Matching Based on<br>Multi-Granularity Fusion Model","x":-34.1048202515,"y":32.1106948853,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1048202515,"tsne_y":32.1106948853,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"6lntlkih","source_x":"PMC","title":"[Formula: see text]: Similarity-Aware Multi-modal Fake News Detection","doi":"10.1007\/978-3-030-47436-2_27","abstract":"Effective detection of fake news has recently attracted significant attention. Current studies have made significant contributions to predicting fake news with less focus on exploiting the relationship (similarity) between the textual and visual information in news articles. Attaching importance to such similarity helps identify fake news stories that, for example, attempt to use irrelevant images to attract readers\u2019 attention. In this work, we propose a [Formula: see text]imilarity-[Formula: see text]ware [Formula: see text]ak[Formula: see text] news detection method ([Formula: see text]) which investigates multi-modal (textual and visual) information of news articles. First, neural networks are adopted to separately extract textual and visual features for news representation. We further investigate the relationship between the extracted features across modalities. Such representations of news textual and visual information along with their relationship are jointly learned and used to predict fake news. The proposed method facilitates recognizing the falsity of news articles based on their text, images, or their \u201cmismatches.\u201d We conduct extensive experiments on large-scale real-world data, which demonstrate the effectiveness of the proposed method.","publish_time":1587081600000,"author_summary":" Zhou, Xinyi; Wu, Jindi; Zafarani, Reza","abstract_summary":" Effective detection of fake news has recently<br>attracted significant attention. Current studies have<br>made significant contributions to predicting fake<br>news with less focus on exploiting the relationship<br>(similarity) between the textual and visual information in<br>news articles. Attaching importance to such<br>similarity helps identify fake news stories that, for<br>example, attempt to use irrelevant images to attract<br>readers\u2019 attention. In this work, we propose a [Formula:<br>see text]imilarity-[Formula: see text]ware<br>[Formula: see text]ak[Formula: see text] news detection<br>method ([Formula: see text]) which investigates<br>multi-modal (textual and visual) information of news<br>articles. First, neural networks are adopted to<br>separately extract textual...","title_summary":" [Formula: see text]: Similarity-Aware<br>Multi-modal Fake News Detection","x":-33.3983078003,"y":30.7551517487,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3983078003,"tsne_y":30.7551517487,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"14jlk5tv","source_x":"PMC","title":"SubRank: Subgraph Embeddings via a Subgraph Proximity Measure","doi":"10.1007\/978-3-030-47426-3_38","abstract":"Representation learning for graph data has gained a lot of attention in recent years. However, state-of-the-art research is focused mostly on node embeddings, with little effort dedicated to the closely related task of computing subgraph embeddings. Subgraph embeddings have many applications, such as community detection, cascade prediction, and question answering. In this work, we propose a subgraph to subgraph proximity measure as a building block for a subgraph embedding framework. Experiments on real-world datasets show that our approach, SubRank, outperforms state-of-the-art methods on several important data mining tasks.","publish_time":1587081600000,"author_summary":" Balalau, Oana; Goyal, Sagar","abstract_summary":" Representation learning for graph data has<br>gained a lot of attention in recent years. However,<br>state-of-the-art research is focused mostly on node embeddings,<br>with little effort dedicated to the closely related<br>task of computing subgraph embeddings. Subgraph<br>embeddings have many applications, such as community<br>detection, cascade prediction, and question answering.<br>In this work, we propose a subgraph to subgraph<br>proximity measure as a building block for a subgraph<br>embedding framework. Experiments on real-world<br>datasets show that our approach, SubRank, outperforms<br>state-of-the-art methods on several important data mining<br>tasks.","title_summary":" SubRank: Subgraph Embeddings via a Subgraph<br>Proximity Measure","x":-31.7161769867,"y":35.5477409363,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.7161769867,"tsne_y":35.5477409363,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"nl0gsr0c","source_x":"PMC","title":"MSGE: A Multi-step Gated Model for Knowledge Graph Completion","doi":"10.1007\/978-3-030-47426-3_33","abstract":"Knowledge graph embedding models aim to represent entities and relations in continuous low-dimensional vector space, benefiting many research areas such as knowledge graph completion and web searching. However, previous works do not consider controlling information flow, which makes them hard to obtain useful latent information and limits model performance. Specifically, as human beings, predictions are usually made in multiple steps with every step filtering out irrelevant information and targeting at helpful information. In this paper, we first integrate iterative mechanism into knowledge graph embedding and propose a multi-step gated model which utilizes relations as queries to extract useful information from coarse to fine in multiple steps. First gate mechanism is adopted to control information flow by the interaction between entity and relation with multiple steps. Then we repeat the gate cell for several times to refine the information incrementally. Our model achieves state-of-the-art performance on most benchmark datasets compared to strong baselines. Further analyses demonstrate the effectiveness of our model and its scalability on large knowledge graphs.","publish_time":1587081600000,"author_summary":" Tan, Chunyang; Yang, Kaijia; Dai, Xinyu;<br>Huang, Shujian; Chen, Jiajun","abstract_summary":" Knowledge graph embedding models aim to<br>represent entities and relations in continuous<br>low-dimensional vector space, benefiting many research areas<br>such as knowledge graph completion and web<br>searching. However, previous works do not consider<br>controlling information flow, which makes them hard to<br>obtain useful latent information and limits model<br>performance. Specifically, as human beings, predictions<br>are usually made in multiple steps with every step<br>filtering out irrelevant information and targeting at<br>helpful information. In this paper, we first integrate<br>iterative mechanism into knowledge graph embedding and<br>propose a multi-step gated model which utilizes<br>relations as queries to extract useful information from...","title_summary":" MSGE: A Multi-step Gated Model for Knowledge<br>Graph Completion","x":-31.8919410706,"y":35.3246154785,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.8919410706,"tsne_y":35.3246154785,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"n364q1h8","source_x":"PMC","title":"HIN: Hierarchical Inference Network for Document-Level Relation Extraction","doi":"10.1007\/978-3-030-47426-3_16","abstract":"Document-level RE requires reading, inferring and aggregating over multiple sentences. From our point of view, it is necessary for document-level RE to take advantage of multi-granularity inference information: entity level, sentence level and document level. Thus, how to obtain and aggregate the inference information with different granularity is challenging for document-level RE, which has not been considered by previous work. In this paper, we propose a Hierarchical Inference Network (HIN) to make full use of the abundant information from entity level, sentence level and document level. Translation constraint and bilinear transformation are applied to target entity pair in multiple subspaces to get entity-level inference information. Next, we model the inference between entity-level information and sentence representation to achieve sentence-level inference information. Finally, a hierarchical aggregation approach is adopted to obtain the document-level inference information. In this way, our model can effectively aggregate inference information from these three different granularities. Experimental results show that our method achieves state-of-the-art performance on the large-scale DocRED dataset. We also demonstrate that using BERT representations can further substantially boost the performance.","publish_time":1587081600000,"author_summary":" Tang, Hengzhu; Cao, Yanan; Zhang, Zhenyu; Cao,<br>Jiangxia; Fang, Fang; Wang, Shi; Yin, Pengfei","abstract_summary":" Document-level RE requires reading,<br>inferring and aggregating over multiple sentences. From<br>our point of view, it is necessary for<br>document-level RE to take advantage of multi-granularity<br>inference information: entity level, sentence level and<br>document level. Thus, how to obtain and aggregate the<br>inference information with different granularity is<br>challenging for document-level RE, which has not been<br>considered by previous work. In this paper, we propose a<br>Hierarchical Inference Network (HIN) to make full use of the<br>abundant information from entity level, sentence level<br>and document level. Translation constraint and<br>bilinear transformation are applied to target entity<br>pair in multiple subspaces...","title_summary":" HIN: Hierarchical Inference Network for<br>Document-Level Relation Extraction","x":-33.3353042603,"y":33.4451713562,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3353042603,"tsne_y":33.4451713562,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"zl0pcaty","source_x":"PMC","title":"SGCN: A Graph Sparsifier Based on Graph Convolutional Networks","doi":"10.1007\/978-3-030-47426-3_22","abstract":"Graphs are ubiquitous across the globe and within science and engineering. With graphs growing in size, node classification on large graphs can be space and time consuming, even with powerful classifiers such as Graph Convolutional Networks (GCNs). Hence, some questions are raised, particularly, whether one can keep only some of the edges of a graph while maintaining prediction performance for node classification, or train classifiers on specific subgraphs instead of a whole graph with limited performance loss in node classification. To address these questions, we propose Sparsified Graph Convolutional Network (SGCN), a neural network graph sparsifier that sparsifies a graph by pruning some edges. We formulate sparsification as an optimization problem, which we solve by an Alternating Direction Method of Multipliers (ADMM)-based solution. We show that sparsified graphs provided by SGCN can be used as inputs to GCN, leading to better or comparable node classification performance with that of original graphs in GCN, DeepWalk, and GraphSAGE.","publish_time":1587081600000,"author_summary":" Li, Jiayu; Zhang, Tianyun; Tian, Hao; Jin,<br>Shengmin; Fardad, Makan; Zafarani, Reza","abstract_summary":" Graphs are ubiquitous across the globe and<br>within science and engineering. With graphs growing<br>in size, node classification on large graphs can<br>be space and time consuming, even with powerful<br>classifiers such as Graph Convolutional Networks (GCNs).<br>Hence, some questions are raised, particularly,<br>whether one can keep only some of the edges of a graph<br>while maintaining prediction performance for node<br>classification, or train classifiers on specific subgraphs<br>instead of a whole graph with limited performance loss<br>in node classification. To address these<br>questions, we propose Sparsified Graph Convolutional<br>Network (SGCN), a neural network graph sparsifier that<br>sparsifies a graph...","title_summary":" SGCN: A Graph Sparsifier Based on Graph<br>Convolutional Networks","x":-31.0950012207,"y":35.9921913147,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0950012207,"tsne_y":35.9921913147,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"c7lar3j1","source_x":"PMC","title":"Mining Dynamic Graph Streams for Predictive Queries Under Resource Constraints","doi":"10.1007\/978-3-030-47436-2_3","abstract":"Knowledge graph streams are a data model underlying many online dynamic data applications today. Answering predictive relationship queries over such a stream is very challenging as the heterogeneous graph streams imply complex topological and temporal correlations of knowledge facts, as well as fast dynamic incoming rates and statistical pattern changes over time. We present our approach with two major components: a Count-Fading sketch and an online incremental embedding algorithm. We answer predictive relationship queries using the embedding results. Extensive experiments over real world datasets show that our approach significantly outperforms two baseline approaches, producing accurate query results efficiently with a small memory footprint.","publish_time":1587081600000,"author_summary":" Liu, Xuanming; Ge, Tingjian","abstract_summary":" Knowledge graph streams are a data model<br>underlying many online dynamic data applications today.<br>Answering predictive relationship queries over such a<br>stream is very challenging as the heterogeneous graph<br>streams imply complex topological and temporal<br>correlations of knowledge facts, as well as fast dynamic<br>incoming rates and statistical pattern changes over<br>time. We present our approach with two major<br>components: a Count-Fading sketch and an online<br>incremental embedding algorithm. We answer predictive<br>relationship queries using the embedding results.<br>Extensive experiments over real world datasets show that<br>our approach significantly outperforms two<br>baseline approaches, producing accurate query results<br>efficiently with a...","title_summary":" Mining Dynamic Graph Streams for Predictive<br>Queries Under Resource Constraints","x":-32.5195846558,"y":35.9531135559,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.5195846558,"tsne_y":35.9531135559,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"b3crwr3v","source_x":"PMC","title":"Connecting the Dots: Hypotheses Generation by Leveraging Semantic Shifts","doi":"10.1007\/978-3-030-47436-2_25","abstract":"Literature-based Discovery (LBD) (a.k.a. Hypotheses Generation) is a systematic knowledge discovery process that elicit novel inferences about previously unknown scientific knowledge by rationally connecting complementary and non-interactive literature. Prompt identification of such novel knowledge is beneficial not only for researchers but also for various other stakeholders such as universities, funding bodies and academic publishers. Almost all the prior LBD research suffer from two major limitations. Firstly, the over-reliance of domain-dependent resources which restrict the models\u2019 applicability to certain domains\/problems. In this regard, we propose a generalisable LBD model that supports both cross-domain and cross-lingual knowledge discovery. The second persistent research deficiency is the mere focus of static snapshot of the corpus (i.e. ignoring the temporal evolution of topics) to detect the new knowledge. However, the knowledge in scientific literature changes dynamically and thus relying merely on static snapshot limits the model\u2019s ability in capturing semantically meaningful connections. As a result, we propose a novel temporal model that captures semantic change of topics using diachronic word embeddings to unravel more accurate connections. The model was evaluated using the largest available literature repository to demonstrate the efficiency of the proposed cues towards recommending novel knowledge. ELECTRONIC SUPPLEMENTARY MATERIAL: The online version of this chapter (10.1007\/978-3-030-47436-2_25) contains supplementary material, which is available to authorized users.","publish_time":1587081600000,"author_summary":" Thilakaratne, Menasha; Falkner, Katrina;<br>Atapattu, Thushari","abstract_summary":" Literature-based Discovery (LBD) (a.k.a.<br>Hypotheses Generation) is a systematic knowledge<br>discovery process that elicit novel inferences about<br>previously unknown scientific knowledge by rationally<br>connecting complementary and non-interactive<br>literature. Prompt identification of such novel knowledge<br>is beneficial not only for researchers but also<br>for various other stakeholders such as<br>universities, funding bodies and academic publishers.<br>Almost all the prior LBD research suffer from two major<br>limitations. Firstly, the over-reliance of<br>domain-dependent resources which restrict the models\u2019<br>applicability to certain domains\/problems. In this regard,<br>we propose a generalisable LBD model that<br>supports both cross-domain and cross-lingual<br>knowledge discovery. The second persistent research...","title_summary":" Connecting the Dots: Hypotheses Generation by<br>Leveraging Semantic Shifts","x":-33.7189216614,"y":35.1283798218,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7189216614,"tsne_y":35.1283798218,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"5r9p44uh","source_x":"PMC","title":"Learning to Select Important Context Words for Event Detection","doi":"10.1007\/978-3-030-47436-2_57","abstract":"It is important to locate important context words in the sentences and model them appropriately to perform event detection (ED) effectively. This has been mainly achieved by some fixed word selection strategy in the previous studies for ED. In this work, we propose a novel method that learns to select relevant context words for ED based on the Gumbel-Softmax trick. The extensive experiments demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance for ED over different benchmark datasets and settings.","publish_time":1587081600000,"author_summary":" Ngo, Nghia Trung; Nguyen, Tuan Ngo; Nguyen,<br>Thien Huu","abstract_summary":" It is important to locate important context<br>words in the sentences and model them appropriately<br>to perform event detection (ED) effectively.<br>This has been mainly achieved by some fixed word<br>selection strategy in the previous studies for ED. In this<br>work, we propose a novel method that learns to select<br>relevant context words for ED based on the<br>Gumbel-Softmax trick. The extensive experiments demonstrate<br>the effectiveness of the proposed method, leading<br>to the state-of-the-art performance for ED over<br>different benchmark datasets and settings.","title_summary":" Learning to Select Important Context Words for<br>Event Detection","x":-34.1296043396,"y":32.6338043213,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1296043396,"tsne_y":32.6338043213,"subcluster":19,"subcluster_description":"Select Important Context Words","shape":"p"},{"cord_uid":"843obizt","source_x":"PMC","title":"Robust Attribute and Structure Preserving Graph Embedding","doi":"10.1007\/978-3-030-47436-2_45","abstract":"Graph embedding methods are useful for a wide range of graph analysis tasks including link prediction and node classification. Most graph embedding methods learn only the topological structure of graphs. Nevertheless, it has been shown that the incorporation of node attributes is beneficial in improving the expressive power of node embeddings. However, real-world graphs are often noisy in terms of structure and\/or attributes (missing and\/or erroneous edges\/attributes). Most existing graph embedding methods are susceptible to this noise, as they do not consider uncertainty during the modelling process. In this paper, we introduce RASE, a Robust Attribute and Structure preserving graph Embedding model. RASE is a novel graph representation learning model which effectively preserves both graph structure and node attributes through a unified loss function. To be robust, RASE uses a denoising attribute auto-encoder to deal with node attribute noise, and models uncertainty in the embedding space as Gaussians to cope with graph structure noise. We evaluate the performance of RASE through an extensive experimental study on various real-world datasets. Results demonstrate that RASE outperforms state-of-the-art embedding methods on multiple graph analysis tasks and is robust to both structure and attribute noise.","publish_time":1587081600000,"author_summary":" Hettige, Bhagya; Wang, Weiqing; Li,<br>Yuan-Fang; Buntine, Wray","abstract_summary":" Graph embedding methods are useful for a wide<br>range of graph analysis tasks including link<br>prediction and node classification. Most graph embedding<br>methods learn only the topological structure of<br>graphs. Nevertheless, it has been shown that the<br>incorporation of node attributes is beneficial in improving<br>the expressive power of node embeddings. However,<br>real-world graphs are often noisy in terms of structure<br>and\/or attributes (missing and\/or erroneous<br>edges\/attributes). Most existing graph embedding methods are<br>susceptible to this noise, as they do not consider<br>uncertainty during the modelling process. In this paper, we<br>introduce RASE, a Robust Attribute and Structure<br>preserving graph...","title_summary":" Robust Attribute and Structure Preserving<br>Graph Embedding","x":-31.3326435089,"y":35.7919692993,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.3326435089,"tsne_y":35.7919692993,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"mipi7af6","source_x":"PMC","title":"Balancing Between Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning","doi":"10.1007\/978-3-030-47426-3_13","abstract":"Fairness in recommendation has attracted increasing attention due to bias and discrimination possibly caused by traditional recommenders. In Interactive Recommender Systems (IRS), user preferences and the system\u2019s fairness status are constantly changing over time. Existing fairness-aware recommenders mainly consider fairness in static settings. Directly applying existing methods to IRS will result in poor recommendation. To resolve this problem, we propose a reinforcement learning based framework, FairRec, to dynamically maintain a long-term balance between accuracy and fairness in IRS. User preferences and the system\u2019s fairness status are jointly compressed into the state representation to generate recommendations. FairRec aims at maximizing our designed cumulative reward that combines accuracy and fairness. Extensive experiments validate that FairRec can improve fairness, while preserving good recommendation quality.","publish_time":1587081600000,"author_summary":" Liu, Weiwen; Liu, Feng; Tang, Ruiming; Liao,<br>Ben; Chen, Guangyong; Heng, Pheng Ann","abstract_summary":" Fairness in recommendation has attracted<br>increasing attention due to bias and discrimination<br>possibly caused by traditional recommenders. In<br>Interactive Recommender Systems (IRS), user preferences<br>and the system\u2019s fairness status are constantly<br>changing over time. Existing fairness-aware<br>recommenders mainly consider fairness in static settings.<br>Directly applying existing methods to IRS will result in<br>poor recommendation. To resolve this problem, we<br>propose a reinforcement learning based framework,<br>FairRec, to dynamically maintain a long-term balance<br>between accuracy and fairness in IRS. User preferences<br>and the system\u2019s fairness status are jointly<br>compressed into the state representation to generate<br>recommendations. FairRec aims at maximizing our...","title_summary":" Balancing Between Accuracy and Fairness for<br>Interactive Recommendation with Reinforcement Learning","x":-30.1444339752,"y":34.2640838623,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.1444339752,"tsne_y":34.2640838623,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"c5pq6hwa","source_x":"PMC","title":"Negative Sampling for Hyperlink Prediction in Networks","doi":"10.1007\/978-3-030-47436-2_46","abstract":"While graphs capture pairwise relations between entities, hypergraphs deal with higher-order ones, thereby ensuring losslessness. However, in hyperlink (i.e., higher-order link) prediction, where hyperlinks and non-hyperlinks are treated as \u201cpositive\u201d and \u201cnegative\u201d classes respectively, hypergraphs suffer from the problem of extreme class imbalance. Given this context, \u201cnegative sampling\u201d\u2014under-sampling the negative class of non-hyperlinks\u2014becomes mandatory for performing hyperlink prediction. No prior work on hyperlink prediction deals with this problem. In this work, which is the first of its kind, we deal with this problem in the context of hyperlink prediction. More specifically, we leverage graph sampling techniques for sampling non-hyperlinks in hyperlink prediction. Our analysis clearly establishes the effect of random sampling, which is the norm in both link- as well as hyperlink-prediction. Further, we formalize the notion of \u201chardness\u201d of non-hyperlinks via a measure of density, and analyze its distribution over various negative sampling techniques. We experiment with some real-world hypergraph datasets and provide both qualitative and quantitative results on the effects of negative sampling. We also establish its importance in evaluating hyperlink prediction algorithms.","publish_time":1587081600000,"author_summary":" Patil, Prasanna; Sharma, Govind; Murty, M.<br>Narasimha","abstract_summary":" While graphs capture pairwise relations<br>between entities, hypergraphs deal with higher-order<br>ones, thereby ensuring losslessness. However, in<br>hyperlink (i.e., higher-order link) prediction, where<br>hyperlinks and non-hyperlinks are treated as \u201cpositive\u201d<br>and \u201cnegative\u201d classes respectively,<br>hypergraphs suffer from the problem of extreme class<br>imbalance. Given this context, \u201cnegative<br>sampling\u201d\u2014under-sampling the negative class of non-hyperlinks\u2014becomes<br>mandatory for performing hyperlink prediction. No prior<br>work on hyperlink prediction deals with this<br>problem. In this work, which is the first of its kind, we<br>deal with this problem in the context of hyperlink<br>prediction. More specifically, we leverage graph sampling<br>techniques for sampling non-hyperlinks in...","title_summary":" Negative Sampling for Hyperlink Prediction in<br>Networks","x":-31.6088180542,"y":35.5599594116,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.6088180542,"tsne_y":35.5599594116,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"rn4ul1ql","source_x":"PMC","title":"Bottom-Up and Top-Down Graph Pooling","doi":"10.1007\/978-3-030-47436-2_43","abstract":"Pooling layers are crucial components for efficient deep representation learning. As to graph data, however, it\u2019s not trivial to decide which nodes to retain in order to represent the high-level structure of a graph. Recently many different graph pooling methods have been proposed. However, they all rely on local features to conduct global pooling over all nodes, which contradicts poolings in CNNs that only use local features to conduct local pooling. We analyze why this may hinder the performance of graph pooling, then propose a novel graph pooling method called Bottom-Up and Top-Down graph POOLing (BUTDPool). BUTDPool aims to learn a more fine-grained pooling criterion based on coarse global structure information produced by a bottom-up pooling layer, and can enhance local features with global features. Specifically, we propose to use one or multiple pooling layers with a relatively high retain ratio to produce a coarse high-level graph. Injecting the high-level information back into low-level representation, BUTDPool enhances learning a better pooling criterion. Experiments demonstrate the superior performance of the proposed method over compared methods.","publish_time":1587081600000,"author_summary":" Yang, Jia-Qi; Zhan, De-Chuan; Li, Xin-Chun","abstract_summary":" Pooling layers are crucial components for<br>efficient deep representation learning. As to graph<br>data, however, it\u2019s not trivial to decide which nodes<br>to retain in order to represent the high-level<br>structure of a graph. Recently many different graph<br>pooling methods have been proposed. However, they all<br>rely on local features to conduct global pooling<br>over all nodes, which contradicts poolings in CNNs<br>that only use local features to conduct local<br>pooling. We analyze why this may hinder the performance<br>of graph pooling, then propose a novel graph<br>pooling method called Bottom-Up and Top-Down graph<br>POOLing (BUTDPool). BUTDPool aims to learn...","title_summary":" Bottom-Up and Top-Down Graph Pooling","x":-31.3189468384,"y":35.8953132629,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.3189468384,"tsne_y":35.8953132629,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"xp0yhc2j","source_x":"PMC","title":"Joint Relational Dependency Learning for Sequential Recommendation","doi":"10.1007\/978-3-030-47426-3_14","abstract":"Sequential recommendation leverages the temporal information of users\u2019 transactions as transition dependencies for better inferring user preference, which has become increasingly popular in academic research and practical applications. Short-term transition dependencies contain the information of partial item orders, while long-term transition dependencies infer long-range user preference, the two dependencies are mutually restrictive and complementary. Although some work investigates unifying both long-term and short-term dependencies for better performance, they still neglect the fact that short-term interactions are multi-folds, which are either individual-level interactions or union-level interactions. Existing sequential recommendations mainly focus on user\u2019s individual (i.e., individual-level) interactions but ignore the important collective influence at union-level. Since union-level interactions can reflect that human decisions are made based on multiple items he\/she has already interacted, ignoring such interactions can result in the disability of capturing the collective influence between items. To alleviate this issue, we proposed a Joint Relational Dependency learning (JRD-L) for sequential recommendation that exploits both long-term and short-term preferences at individual-level and union-level. Specifically, JRD-L combines long-term user preferences with short-term interests by measuring short-term pair relations at individual-level and union-level. Moreover, JRD-L can alleviate the sparsity problem of union-level interactions by adding more descriptive details to each item, which is carried by individual-level relations. Extensive numerical experiments demonstrate JRD-L outperforms state-of-the-art baselines for the sequential recommendation.","publish_time":1587081600000,"author_summary":" Wang, Xiangmeng; Li, Qian; Zhang, Wu; Xu,<br>Guandong; Liu, Shaowu; Zhu, Wenhao","abstract_summary":" Sequential recommendation leverages the<br>temporal information of users\u2019 transactions as<br>transition dependencies for better inferring user<br>preference, which has become increasingly popular in<br>academic research and practical applications.<br>Short-term transition dependencies contain the<br>information of partial item orders, while long-term<br>transition dependencies infer long-range user<br>preference, the two dependencies are mutually restrictive<br>and complementary. Although some work<br>investigates unifying both long-term and short-term<br>dependencies for better performance, they still neglect the<br>fact that short-term interactions are<br>multi-folds, which are either individual-level<br>interactions or union-level interactions. Existing<br>sequential recommendations mainly focus on user\u2019s<br>individual (i.e., individual-level) interactions but<br>ignore the important...","title_summary":" Joint Relational Dependency Learning for<br>Sequential Recommendation","x":-30.9437408447,"y":34.4904251099,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.9437408447,"tsne_y":34.4904251099,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"5akx3o58","source_x":"PMC","title":"Quality-Aware Streaming Network Embedding with Memory Refreshing","doi":"10.1007\/978-3-030-47426-3_35","abstract":"Static network embedding has been widely studied to convert sparse structure information into a dense latent space. However, the majority of real networks are continuously evolving, and deriving the whole embedding for every snapshot is computationally intensive. To avoid recomputing the embedding over time, we explore streaming network embedding for two reasons: 1) to efficiently identify the nodes required to update the embeddings under multi-type network changes, and 2) to carefully revise the embeddings to maintain transduction over different parts of the network. Specifically, we propose a new representation learning framework, named Graph Memory Refreshing (GMR), to preserve both global types of structural information efficiently. We prove that GMR maintains the consistency of embeddings (crucial for network analysis) for isomorphic structures better than existing approaches. Experimental results demonstrate that GMR outperforms the baselines with much smaller time.","publish_time":1587081600000,"author_summary":" Chen, Hsi-Wen; Shuai, Hong-Han; Wang,<br>Sheng-De; Yang, De-Nian","abstract_summary":" Static network embedding has been widely<br>studied to convert sparse structure information into a<br>dense latent space. However, the majority of real<br>networks are continuously evolving, and deriving the<br>whole embedding for every snapshot is<br>computationally intensive. To avoid recomputing the embedding<br>over time, we explore streaming network embedding<br>for two reasons: 1) to efficiently identify the<br>nodes required to update the embeddings under<br>multi-type network changes, and 2) to carefully revise the<br>embeddings to maintain transduction over different parts<br>of the network. Specifically, we propose a new<br>representation learning framework, named Graph Memory<br>Refreshing (GMR), to preserve both global...","title_summary":" Quality-Aware Streaming Network Embedding<br>with Memory Refreshing","x":-31.0836238861,"y":35.8731575012,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0836238861,"tsne_y":35.8731575012,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"c7y4c5fm","source_x":"PMC","title":"Strong Baselines for Author Name Disambiguation with and Without Neural Networks","doi":"10.1007\/978-3-030-47426-3_29","abstract":"Author name disambiguation (AND) is one of the most vital problems in scientometrics, which has become a great challenge with the rapid growth of academic digital libraries. Existing approaches for this task substantially rely on complex clustering-like architectures, and they usually assume the number of clusters is known beforehand or predict the number by applying another model, which involve increasingly complex and time-consuming architectures. In this paper, we combine simple neural networks with two sets of heuristic rules to explore strong baselines for the author name disambiguation problem without any priori knowledge or estimation about cluster size, which frees the model from unnecessary complexity. On a popular benchmark dataset AMiner, our solution significantly outperforms several state-of-the-art methods both in performance and efficiency, and it still achieves comparable performance with many complex models when only using a group of rules. Experimental results also indicate that gains from sophisticated deep learning techniques are quite modest in the author name disambiguation problem.","publish_time":1587081600000,"author_summary":" Zhang, Zhenyu; Yu, Bowen; Liu, Tingwen; Wang,<br>Dong","abstract_summary":" Author name disambiguation (AND) is one of the<br>most vital problems in scientometrics, which has<br>become a great challenge with the rapid growth of<br>academic digital libraries. Existing approaches for<br>this task substantially rely on complex<br>clustering-like architectures, and they usually assume the<br>number of clusters is known beforehand or predict the<br>number by applying another model, which involve<br>increasingly complex and time-consuming architectures. In<br>this paper, we combine simple neural networks with<br>two sets of heuristic rules to explore strong<br>baselines for the author name disambiguation problem<br>without any priori knowledge or estimation about<br>cluster size, which frees the...","title_summary":" Strong Baselines for Author Name<br>Disambiguation with and Without Neural Networks","x":-33.9028053284,"y":33.0864868164,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9028053284,"tsne_y":33.0864868164,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"3ocr7481","source_x":"PMC","title":"Role Equivalence Attention for Label Propagation in Graph Neural Networks","doi":"10.1007\/978-3-030-47436-2_42","abstract":"Semi-supervised relational learning methods aim to classify nodes in a partially-labeled graph. While popular, existing methods using Graph Neural Networks (GNN) for semi-supervised relational learning have mainly focused on learning node representations by aggregating nearby attributes, and it is still challenging to leverage inferences about unlabeled nodes with few attributes\u2014particularly when trying to exploit higher-order relationships in the network efficiently. To address this, we propose a Graph Neural Network architecture that incorporates patterns among the available class labels and uses (1) a Role Equivalence attention mechanism and (2) a mini-batch importance sampling method to improve efficiency when learning over high-order paths. In particular, our Role Equivalence attention mechanism is able to use nodes\u2019 roles to learn how to focus on relevant distant neighbors, in order to adaptively reduce the increased noise that occurs when higher-order structures are considered. In experiments on six different real-world datasets, we show that our model (REGNN) achieves significant performance gains compared to other recent state-of-the-art baselines, particularly when higher-order paths are considered in the models.","publish_time":1587081600000,"author_summary":" Park, Hogun; Neville, Jennifer","abstract_summary":" Semi-supervised relational learning methods<br>aim to classify nodes in a partially-labeled<br>graph. While popular, existing methods using Graph<br>Neural Networks (GNN) for semi-supervised<br>relational learning have mainly focused on learning node<br>representations by aggregating nearby attributes, and it is<br>still challenging to leverage inferences about<br>unlabeled nodes with few attributes\u2014particularly when<br>trying to exploit higher-order relationships in the<br>network efficiently. To address this, we propose a<br>Graph Neural Network architecture that<br>incorporates patterns among the available class labels and<br>uses (1) a Role Equivalence attention mechanism and<br>(2) a mini-batch importance sampling method to<br>improve efficiency when learning over high-order...","title_summary":" Role Equivalence Attention for Label<br>Propagation in Graph Neural Networks","x":-31.3311367035,"y":35.5609436035,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.3311367035,"tsne_y":35.5609436035,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"x22g2zg5","source_x":"PMC","title":"Modelling Temporal Dynamics and Repeated Behaviors for Recommendation","doi":"10.1007\/978-3-030-47426-3_15","abstract":"Personalized recommendation has yield immense success in predicting user preference with heterogeneous implicit feedback (HIF), i.e., various user behaviors. However, existing studies consider less about the temporal dynamics and repeated patterns of HIF. They simply suppose: (1) a hard rule among user behaviors (e.g., add-to-cart must come before purchase and after view); (2) merge repeated behaviors into one (e.g., view several times is considered as view once only), thus failing to unveil user preferences from their real behaviors. To ease these issues, we, therefore, propose a novel end-to-end neural framework \u2013 TDRB, which automatically models the Temporal Dynamics and Repeated Behaviors to assist in capturing user preference, thus achieving more accurate recommendations. Empirical studies on three real-world datasets demonstrate the superiority of our proposed TDRB against other state-of-the-arts.","publish_time":1587081600000,"author_summary":" Zhou, Xin; Sun, Zhu; Guo, Guibing; Liu, Yuan","abstract_summary":" Personalized recommendation has yield<br>immense success in predicting user preference with<br>heterogeneous implicit feedback (HIF), i.e., various user<br>behaviors. However, existing studies consider less about<br>the temporal dynamics and repeated patterns of<br>HIF. They simply suppose: (1) a hard rule among user<br>behaviors (e.g., add-to-cart must come before purchase<br>and after view); (2) merge repeated behaviors into<br>one (e.g., view several times is considered as view<br>once only), thus failing to unveil user preferences<br>from their real behaviors. To ease these issues, we,<br>therefore, propose a novel end-to-end neural framework \u2013<br>TDRB, which automatically models the Temporal<br>Dynamics and Repeated...","title_summary":" Modelling Temporal Dynamics and Repeated<br>Behaviors for Recommendation","x":-30.3246307373,"y":34.3156318665,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.3246307373,"tsne_y":34.3156318665,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"ey4ziomv","source_x":"PMC","title":"Fusion-Extraction Network for Multimodal Sentiment Analysis","doi":"10.1007\/978-3-030-47436-2_59","abstract":"Multiple modality data bring new challenges for sentiment analysis, as combining varieties of information in an effective manner is a rigorous task. Previous works do not effectively utilize the relationship and influence between texts and images. This paper proposes a fusion-extraction network model for multimodal sentiment analysis. First, our model uses an interactive information fusion mechanism to interactively learn the visual-specific textual representations and the textual-specific visual representations. Then, we propose an information extraction mechanism to extract valid information and filter redundant parts for the specific textual and visual representations. The experimental results on two public multimodal sentiment datasets show that our model outperforms existing state-of-the-art methods.","publish_time":1587081600000,"author_summary":" Jiang, Tao; Wang, Jiahai; Liu, Zhiyue; Ling,<br>Yingbiao","abstract_summary":" Multiple modality data bring new challenges<br>for sentiment analysis, as combining varieties of<br>information in an effective manner is a rigorous task.<br>Previous works do not effectively utilize the<br>relationship and influence between texts and images. This<br>paper proposes a fusion-extraction network model<br>for multimodal sentiment analysis. First, our<br>model uses an interactive information fusion<br>mechanism to interactively learn the visual-specific<br>textual representations and the textual-specific<br>visual representations. Then, we propose an<br>information extraction mechanism to extract valid<br>information and filter redundant parts for the specific<br>textual and visual representations. The experimental<br>results on two public multimodal sentiment datasets<br>show...","title_summary":" Fusion-Extraction Network for Multimodal<br>Sentiment Analysis","x":-33.9186286926,"y":30.819940567,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9186286926,"tsne_y":30.819940567,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"et6u91gf","source_x":"PMC","title":"AutoSUM: Automating Feature Extraction and Multi-user Preference Simulation for Entity Summarization","doi":"10.1007\/978-3-030-47436-2_44","abstract":"With the growth of knowledge graphs, entity descriptions are becoming extremely lengthy. Entity summarization task, aiming to generate diverse, comprehensive and representative summaries for entities, has received an increasing interest recently. In most previous methods, features are usually extracted by the hand-crafted templates. Then the feature selection and multi-user preference simulation take place, depending too much on human expertise. In this paper, a novel integration method called AutoSUM is proposed for automatic feature extraction and multi-user preference simulation to overcome the drawbacks of previous methods. There are two modules in AutoSUM: extractor and simulator. The extractor module operates automatic feature extraction based on a BiLSTM with a combined input representation including word embeddings and graph embeddings. Meanwhile, the simulator module automates multi-user preference simulation based on a well-designed two-phase attention mechanism (i.e., entity-phase attention and user-phase attention). Experimental results demonstrate that AutoSUM produces the state-of-the-art performance on two widely used datasets (i.e., DBpedia and LinkedMDB) in both F-measure and MAP.","publish_time":1587081600000,"author_summary":" Wei, Dongjun; Liu, Yaxin; Zhu, Fuqing; Zang,<br>Liangjun; Zhou, Wei; Lu, Yijun; Hu, Songlin","abstract_summary":" With the growth of knowledge graphs, entity<br>descriptions are becoming extremely lengthy. Entity<br>summarization task, aiming to generate diverse,<br>comprehensive and representative summaries for entities,<br>has received an increasing interest recently. In<br>most previous methods, features are usually<br>extracted by the hand-crafted templates. Then the<br>feature selection and multi-user preference<br>simulation take place, depending too much on human<br>expertise. In this paper, a novel integration method<br>called AutoSUM is proposed for automatic feature<br>extraction and multi-user preference simulation to<br>overcome the drawbacks of previous methods. There are<br>two modules in AutoSUM: extractor and simulator.<br>The extractor module operates automatic feature...","title_summary":" AutoSUM: Automating Feature Extraction and<br>Multi-user Preference Simulation for Entity<br>Summarization","x":-33.5619506836,"y":33.0493736267,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5619506836,"tsne_y":33.0493736267,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"81lfh99h","source_x":"PMC","title":"An Empirical Model for n-gram Frequency Distribution in Large Corpora","doi":"10.1007\/978-3-030-47436-2_63","abstract":"Statistical multiword extraction methods can benefit from the knowledge on the n-gram ([Formula: see text]) frequency distribution in natural language corpora, for indexing and time\/space optimization purposes. The appearance of increasingly large corpora raises new challenges on the investigation of the large scale behavior of the n-gram frequency distributions, not typically emerging on small scale corpora. We propose an empirical model, based on the assumption of finite n-gram language vocabularies, to estimate the number of distinct n-grams in large corpora, as well as the sizes of the equal-frequency n-gram groups, which occur in the lower frequencies starting from 1. The model was validated for n-grams with [Formula: see text], by a wide range of real corpora in English and French, from 60 million up to 8 billion words. These are full non-truncated corpora data, that is, their associated frequency data include the entire range of observed n-gram frequencies, from 1 up to the maximum. The model predicts the monotonic growth of the numbers of distinct n-grams until reaching asymptotic plateaux when the corpus size grows to infinity. It also predicts the non-monotonicity of the sizes of the equal-frequency n-gram groups as a function of the corpus size.","publish_time":1587081600000,"author_summary":" Silva, Joaquim F.; Cunha, Jose C.","abstract_summary":" Statistical multiword extraction methods can<br>benefit from the knowledge on the n-gram ([Formula: see<br>text]) frequency distribution in natural language<br>corpora, for indexing and time\/space optimization<br>purposes. The appearance of increasingly large corpora<br>raises new challenges on the investigation of the<br>large scale behavior of the n-gram frequency<br>distributions, not typically emerging on small scale corpora.<br>We propose an empirical model, based on the<br>assumption of finite n-gram language vocabularies, to<br>estimate the number of distinct n-grams in large<br>corpora, as well as the sizes of the equal-frequency<br>n-gram groups, which occur in the lower frequencies<br>starting from 1. The...","title_summary":" An Empirical Model for n-gram Frequency<br>Distribution in Large Corpora","x":-34.0031585693,"y":31.5735740662,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.0031585693,"tsne_y":31.5735740662,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"850rgjst","source_x":"PMC","title":"Semantics-Reconstructing Hashing for Cross-Modal Retrieval","doi":"10.1007\/978-3-030-47436-2_24","abstract":"Retrieval on Cross-modal data has attracted extensive attention as it enables fast searching across various data sources, such as texts, images and videos. As one of the typical techniques for cross-model searching, hashing methods project features with high dimension into short-length hash codes, thus effectively improving storage and retrieval efficiency. Recently, many efforts have been made to widely study supervised methods with promising performance. However, there still remain some problems. Conventionally, hash codes and projection functions are learnt by preserving the pairwise similarities between data items, which neglects the discriminative property of class associated with each data item. Most of the existing methods that utilise class labels also undertake the binary codes learning under a classification frame. The relations between binary codes and labels have not been well considered. To tackle these problems, we propose a shallow supervised hash learning method \u2013 Semantics-reconstructing Cross-modal Hashing (SCH), which reconstructs semantic representation and learns the hash codes for the entire dataset jointly. For the semantic reconstruction, the learned semantic representation is projected back into label space, extracting more semantic information. By leveraging reconstructed semantic representations, the hash codes are learnt by considering the underlying correlations between labels, hash codes and original features, resulting in a further performance improvement. Moreover, SCH learns the hash codes and functions without relaxing the binary constraints simultaneously, therefore, it further reduces the quantization errors. In addition, the linear computational complexity of its training makes it practicable to big data. Extensive experiments show that the proposed SCH can perform better than the state-of-the-art baselines.","publish_time":1587081600000,"author_summary":" Zhang, Peng-Fei; Huang, Zi; Zhang, Zheng","abstract_summary":" Retrieval on Cross-modal data has attracted<br>extensive attention as it enables fast searching across<br>various data sources, such as texts, images and videos.<br>As one of the typical techniques for cross-model<br>searching, hashing methods project features with high<br>dimension into short-length hash codes, thus<br>effectively improving storage and retrieval efficiency.<br>Recently, many efforts have been made to widely study<br>supervised methods with promising performance. However,<br>there still remain some problems. Conventionally,<br>hash codes and projection functions are learnt by<br>preserving the pairwise similarities between data items,<br>which neglects the discriminative property of class<br>associated with each data item. Most of...","title_summary":" Semantics-Reconstructing Hashing for<br>Cross-Modal Retrieval","x":-32.6786193848,"y":33.8551902771,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.6786193848,"tsne_y":33.8551902771,"subcluster":21,"subcluster_description":"Cross-Modal Retrievalgraph-Based Image Retrieval","shape":"p"},{"cord_uid":"dmqij3om","source_x":"PMC","title":"JPLink: On Linking Jobs to Vocational Interest Types","doi":"10.1007\/978-3-030-47436-2_17","abstract":"Linking job seekers with relevant jobs requires matching based on not only skills, but also personality types. Although the Holland Code also known as RIASEC has frequently been used to group people by their suitability for six different categories of occupations, the RIASEC category labels of individual jobs are often not found in job posts. This is attributed to significant manual efforts required for assigning job posts with RIASEC labels. To cope with assigning massive number of jobs with RIASEC labels, we propose JPLink, a machine learning approach using the text content in job titles and job descriptions. JPLink exploits domain knowledge available in an occupation-specific knowledge base known as O*NET to improve feature representation of job posts. To incorporate relative ranking of RIASEC labels of each job, JPLink proposes a listwise loss function inspired by learning to rank. Both our quantitative and qualitative evaluations show that JPLink outperforms conventional baselines. We conduct an error analysis on JPLink\u2019s predictions to show that it can uncover label errors in existing job posts.","publish_time":1587081600000,"author_summary":" Silva, Amila; Lo, Pei-Chi; Lim, Ee-Peng","abstract_summary":" Linking job seekers with relevant jobs<br>requires matching based on not only skills, but also<br>personality types. Although the Holland Code also known as<br>RIASEC has frequently been used to group people by<br>their suitability for six different categories of<br>occupations, the RIASEC category labels of individual jobs<br>are often not found in job posts. This is attributed<br>to significant manual efforts required for<br>assigning job posts with RIASEC labels. To cope with<br>assigning massive number of jobs with RIASEC labels, we<br>propose JPLink, a machine learning approach using the<br>text content in job titles and job descriptions.<br>JPLink exploits...","title_summary":" JPLink: On Linking Jobs to Vocational Interest<br>Types","x":-33.2355804443,"y":34.1881370544,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.2355804443,"tsne_y":34.1881370544,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"3wt8x74a","source_x":"PMC","title":"Attention-Based Graph Evolution","doi":"10.1007\/978-3-030-47426-3_34","abstract":"Based on the recent success of deep generative models on continuous data, various new methods are being developed to generate discrete data such as graphs. However, these approaches focus on unconditioned generation, which limits their control over the generating procedure to produce graphs in context, thus limiting the applicability to real-world settings. To address this gap, we introduce an attention-based graph evolution model (AGE). AGE is a conditional graph generator based on the neural attention mechanism that can not only model graph evolution in both space and time, but can also model the transformation between graphs from one state to another. We evaluate AGE on multiple conditional graph-generation tasks, and our results show that it can generate realistic graphs conditioned on source graphs, outperforming existing methods in terms of quality and generality.","publish_time":1587081600000,"author_summary":" Fan, Shuangfei; Huang, Bert","abstract_summary":" Based on the recent success of deep generative<br>models on continuous data, various new methods are<br>being developed to generate discrete data such as<br>graphs. However, these approaches focus on<br>unconditioned generation, which limits their control over<br>the generating procedure to produce graphs in<br>context, thus limiting the applicability to real-world<br>settings. To address this gap, we introduce an<br>attention-based graph evolution model (AGE). AGE is a<br>conditional graph generator based on the neural attention<br>mechanism that can not only model graph evolution in both<br>space and time, but can also model the transformation<br>between graphs from one state to...","title_summary":" Attention-Based Graph Evolution","x":-30.9701900482,"y":36.073261261,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.9701900482,"tsne_y":36.073261261,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"8jqkmqp9","source_x":"PMC","title":"What\u2019s in a Gist? Towards an Unsupervised Gist Representation for Few-Shot Large Document Classification","doi":"10.1007\/978-3-030-47426-3_21","abstract":"The gist can be viewed as an abstract concept that represents only the quintessential meaning derived from a single or multiple sources of information. We live in an age where vast quantities of information are widely available and easily accessible. Identifying the gist contextualises information which facilitates the fast disambiguation and prediction of related concepts bringing about a set of natural relationships defined between information sources. In this paper, we investigate and introduce a novel unsupervised gist extraction and quantification framework that represents a computational form of the gist based on notions from fuzzy trace theory. To evaluate our purposed framework, we apply the gist to the task of semantic similarity, specifically to few-shot large document classification where documents on average have a large number of words. The results show our proposed gist representation can effectively capture the essential information from a text document while dramatically reducing the features used.","publish_time":1587081600000,"author_summary":" Mar, Jaron; Liu, Jiamou","abstract_summary":" The gist can be viewed as an abstract concept<br>that represents only the quintessential meaning<br>derived from a single or multiple sources of<br>information. We live in an age where vast quantities of<br>information are widely available and easily accessible.<br>Identifying the gist contextualises information which<br>facilitates the fast disambiguation and prediction of<br>related concepts bringing about a set of natural<br>relationships defined between information sources. In this<br>paper, we investigate and introduce a novel<br>unsupervised gist extraction and quantification framework<br>that represents a computational form of the gist<br>based on notions from fuzzy trace theory. To evaluate<br>our purposed...","title_summary":" What\u2019s in a Gist? Towards an Unsupervised Gist<br>Representation for Few-Shot Large Document Classification","x":-33.974407196,"y":34.7105445862,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.974407196,"tsne_y":34.7105445862,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"3c7zm8s6","source_x":"PMC","title":"Retrofitting Embeddings for Unsupervised User Identity Linkage","doi":"10.1007\/978-3-030-47426-3_30","abstract":"User Identity Linkage (UIL) is the problem of matching user identities across multiple online social networks (OSNs) which belong to the same person. The solutions to UIL problem facilitate cross-platform research on OSN users and enable many useful applications such as user profiling and recommendation. As the UIL labeled data are often lacking and costly to obtain, learning user embeddings for matching user identities using an unsupervised approach is therefore highly desired. In this paper, we propose a novel unsupervised UIL framework for enhancing existing user embedding-based UIL methods. Our proposed framework incorporates two key ideas, user-discriminative features and retrofitting embedding. The user-discriminative features enable us to differentiate a specific user identity from other users in its OSN. From the user-discriminative features, we derive pairs of similar user identities across OSNs for retrofitting the base user embeddings of existing UIL methods. Through extensive experiments on three real-world OSN datasets, we show that our framework can leverage user-discriminative features to improve the accuracy of different user embedding-based UIL methods significantly. The quantum of improvement can also be surprisingly good even for existing UIL methods with very poor matching accuracy. ELECTRONIC SUPPLEMENTARY MATERIAL: The online version of this chapter (10.1007\/978-3-030-47426-3_30) contains supplementary material, which is available to authorized users.","publish_time":1587081600000,"author_summary":" Zhou, Tao; Lim, Ee-Peng; Lee, Roy Ka-Wei; Zhu,<br>Feida; Cao, Jiuxin","abstract_summary":" User Identity Linkage (UIL) is the problem of<br>matching user identities across multiple online social<br>networks (OSNs) which belong to the same person. The<br>solutions to UIL problem facilitate cross-platform<br>research on OSN users and enable many useful<br>applications such as user profiling and recommendation. As<br>the UIL labeled data are often lacking and costly to<br>obtain, learning user embeddings for matching user<br>identities using an unsupervised approach is therefore<br>highly desired. In this paper, we propose a novel<br>unsupervised UIL framework for enhancing existing user<br>embedding-based UIL methods. Our proposed framework<br>incorporates two key ideas, user-discriminative features<br>and retrofitting...","title_summary":" Retrofitting Embeddings for Unsupervised<br>User Identity Linkage","x":-31.1260967255,"y":34.5387115479,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.1260967255,"tsne_y":34.5387115479,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"gei9ktvy","source_x":"PMC","title":"Learning Discriminative Neural Sentiment Units for Semi-supervised Target-Level Sentiment Classification","doi":"10.1007\/978-3-030-47436-2_60","abstract":"Target-level sentiment classification aims at assigning sentiment polarities to opinion targets in a sentence, for which it is significantly more challenging to obtain large-scale labeled data than sentence\/document-level sentiment classification due to the intricate contexts and relations of the target words. To address this challenge, we propose a novel semi-supervised approach to learn sentiment-aware representations from easily accessible unlabeled data specifically for the fine-grained sentiment learning. This is very different from current popular semi-supervised solutions that use the unlabeled data via pretraining to generate generic representations for various types of downstream tasks. Particularly, we show for the first time that we can learn and detect some highly sentiment-discriminative neural units from the unsupervised pretrained model, termed neural sentiment units. Due to the discriminability, these sentiment units can be leveraged by downstream LSTM-based classifiers to generate sentiment-aware and context-dependent word representations to substantially improve their sentiment classification performance. Extensive empirical results on two benchmark datasets show that our approach (i) substantially outperforms state-of-the-art sentiment classifiers and (ii) achieves significantly better data efficiency.","publish_time":1587081600000,"author_summary":" Zhao, Jingjing; Yang, Yao; Pang, Guansong; Lv,<br>Lei; Shang, Hong; Sun, Zhongqian; Yang, Wei","abstract_summary":" Target-level sentiment classification aims<br>at assigning sentiment polarities to opinion<br>targets in a sentence, for which it is significantly<br>more challenging to obtain large-scale labeled<br>data than sentence\/document-level sentiment<br>classification due to the intricate contexts and relations of<br>the target words. To address this challenge, we<br>propose a novel semi-supervised approach to learn<br>sentiment-aware representations from easily accessible<br>unlabeled data specifically for the fine-grained<br>sentiment learning. This is very different from current<br>popular semi-supervised solutions that use the<br>unlabeled data via pretraining to generate generic<br>representations for various types of downstream tasks.<br>Particularly, we show for the first time that...","title_summary":" Learning Discriminative Neural Sentiment<br>Units for Semi-supervised Target-Level Sentiment<br>Classification","x":-34.0376434326,"y":30.7385101318,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.0376434326,"tsne_y":30.7385101318,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"knnvykc2","source_x":"PMC","title":"An Ontology for Collaborative Decision Making","doi":"10.1007\/978-3-030-48641-9_13","abstract":"This article focuses on an ontology construction for collaborative decision making. To do this, a state of the art on collaborative decision-making, on ontology engineering and on collaboration engineering has been done. An eight-step ontology development methodology was adopted and implemented to build the ontology. A corpus made up of more than seventy-seven (77) documents was the starting point for the extraction of terms from the ontology and the UML (Unified Modeling Language) language served as a description language of our ontology. This ontology is intended to be the starting point for a facilitation support system in a Collaborative Decision Making process. The aim of the work is to produce a new system according the \u201cFacilitator in the box\u201d paradigm.","publish_time":1587772800000,"author_summary":" Konat\u00e9, Jacqueline; Zarat\u00e9, Pascale; Gueye,<br>Aminata; Camilleri, Guy","abstract_summary":" This article focuses on an ontology<br>construction for collaborative decision making. To do this,<br>a state of the art on collaborative<br>decision-making, on ontology engineering and on collaboration<br>engineering has been done. An eight-step ontology<br>development methodology was adopted and implemented to<br>build the ontology. A corpus made up of more than<br>seventy-seven (77) documents was the starting point for the<br>extraction of terms from the ontology and the UML (Unified<br>Modeling Language) language served as a description<br>language of our ontology. This ontology is intended to be<br>the starting point for a facilitation support<br>system in a Collaborative Decision...","title_summary":" An Ontology for Collaborative Decision Making","x":-34.0259399414,"y":36.7889137268,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.0259399414,"tsne_y":36.7889137268,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"7ixe768e","source_x":"PMC","title":"Medical Dialogue Summarization for Automated Reporting in Healthcare","doi":"10.1007\/978-3-030-49165-9_7","abstract":"Healthcare providers generally spend excessive time on administrative tasks at the expense of direct patient care. The emergence of new artificial intelligence and natural language processing technologies gives rise to innovations that could relieve them of this burden. In this paper, we present a pipeline structure for building dialogue summarization systems. Our pipeline summarizes a consultation of a patient with a care provider and automatically generates a report compliant with medical formats. Four pipeline components are used to generate a report based on audio input. The outputs of each component are analyzed to determine the most important challenges and issues. The current proof-of-concept, which was applied to eight doctor-to-patient sessions concerning ear infection, shows that automatic dialogue summarization and reporting is achievable, but requires improvements to increase completeness.","publish_time":1588118400000,"author_summary":" Molenaar, Sabine; Maas, Lientje; Burriel,<br>Ver\u00f3nica; Dalpiaz, Fabiano; Brinkkemper, Sjaak","abstract_summary":" Healthcare providers generally spend<br>excessive time on administrative tasks at the expense of<br>direct patient care. The emergence of new artificial<br>intelligence and natural language processing technologies<br>gives rise to innovations that could relieve them of<br>this burden. In this paper, we present a pipeline<br>structure for building dialogue summarization systems.<br>Our pipeline summarizes a consultation of a<br>patient with a care provider and automatically<br>generates a report compliant with medical formats. Four<br>pipeline components are used to generate a report based<br>on audio input. The outputs of each component are<br>analyzed to determine the most important challenges and<br>issues....","title_summary":" Medical Dialogue Summarization for Automated<br>Reporting in Healthcare","x":-32.1292037964,"y":31.1948604584,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.1292037964,"tsne_y":31.1948604584,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"keqsdceh","source_x":"PMC","title":"Modular Graphical Ontology Engineering Evaluated","doi":"10.1007\/978-3-030-49461-2_2","abstract":"Ontology engineering is traditionally a complex and time-consuming process, requiring an intimate knowledge of description logic and predicting non-local effects of different ontological commitments. Pattern-based modular ontology engineering, coupled with a graphical modeling paradigm, can help make ontology engineering accessible to modellers with limited ontology expertise. We have developed CoModIDE, the Comprehensive Modular Ontology IDE, to develop and explore such a modeling approach. In this paper we present an evaluation of the CoModIDE tool, with a set of 21 subjects carrying out some typical modeling tasks. Our findings indicate that using CoModIDE improves task completion rate and reduces task completion time, compared to using standard Prot\u00e9g\u00e9. Further, our subjects report higher System Usability Scale (SUS) evaluation scores for CoModIDE, than for Prot\u00e9g\u00e9. The subjects also report certain room for improvements in the CoModIDE tool \u2013 notably, these comments all concern comparatively shallow UI bugs or issues, rather than limitations inherent in the proposed modeling method itself. We deduce that our modeling approach is viable, and propose some consequences for ontology engineering tool development.","publish_time":1588809600000,"author_summary":" Shimizu, Cogan; Hammar, Karl; Hitzler, Pascal","abstract_summary":" Ontology engineering is traditionally a<br>complex and time-consuming process, requiring an<br>intimate knowledge of description logic and predicting<br>non-local effects of different ontological<br>commitments. Pattern-based modular ontology engineering,<br>coupled with a graphical modeling paradigm, can help<br>make ontology engineering accessible to modellers<br>with limited ontology expertise. We have developed<br>CoModIDE, the Comprehensive Modular Ontology IDE, to<br>develop and explore such a modeling approach. In this<br>paper we present an evaluation of the CoModIDE tool,<br>with a set of 21 subjects carrying out some typical<br>modeling tasks. Our findings indicate that using<br>CoModIDE improves task completion rate and reduces task<br>completion...","title_summary":" Modular Graphical Ontology Engineering<br>Evaluated","x":-33.756855011,"y":36.5913848877,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.756855011,"tsne_y":36.5913848877,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"3ptwbbpd","source_x":"PMC","title":"SASOBUS: Semi-automatic Sentiment Domain Ontology Building Using Synsets","doi":"10.1007\/978-3-030-49461-2_7","abstract":"In this paper, a semi-automatic approach for building a sentiment domain ontology is proposed. Differently than other methods, this research makes use of synsets in term extraction, concept formation, and concept subsumption. Using several state-of-the-art hybrid aspect-based sentiment analysis methods like Ont + CABASC and Ont + LCR-Rot-hop on a standard dataset, the accuracies obtained using the semi-automatically built ontology as compared to the manually built one, are slightly lower (from approximately 87% to 84%). However, the user time needed for building the ontology is reduced by more than half (from 7 h to 3 h), thus showing the usefulness of this work. This is particularly useful for domains for which sentiment ontologies are not yet available.","publish_time":1588809600000,"author_summary":" Dera, Ewelina; Frasincar, Flavius; Schouten,<br>Kim; Zhuang, Lisa","abstract_summary":" In this paper, a semi-automatic approach for<br>building a sentiment domain ontology is proposed.<br>Differently than other methods, this research makes use of<br>synsets in term extraction, concept formation, and<br>concept subsumption. Using several state-of-the-art<br>hybrid aspect-based sentiment analysis methods like<br>Ont + CABASC and Ont + LCR-Rot-hop on a standard<br>dataset, the accuracies obtained using the<br>semi-automatically built ontology as compared to the manually<br>built one, are slightly lower (from approximately<br>87% to 84%). However, the user time needed for<br>building the ontology is reduced by more than half (from 7<br>h to 3 h), thus showing the usefulness...","title_summary":" SASOBUS: Semi-automatic Sentiment Domain<br>Ontology Building Using Synsets","x":-33.3782501221,"y":32.1887664795,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3782501221,"tsne_y":32.1887664795,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"5fax1u3w","source_x":"PMC","title":"A Knowledge Graph for Industry 4.0","doi":"10.1007\/978-3-030-49461-2_27","abstract":"One of the most crucial tasks for today\u2019s knowledge workers is to get and retain a thorough overview on the latest state of the art. Especially in dynamic and evolving domains, the amount of relevant sources is constantly increasing, updating and overruling previous methods and approaches. For instance, the digital transformation of manufacturing systems, called Industry 4.0, currently faces an overwhelming amount of standardization efforts and reference initiatives, resulting in a sophisticated information environment. We propose a structured dataset in the form of a semantically annotated knowledge graph for Industry 4.0 related standards, norms and reference frameworks. The graph provides a Linked Data-conform collection of annotated, classified reference guidelines supporting newcomers and experts alike in understanding how to implement Industry 4.0 systems. We illustrate the suitability of the graph for various use cases, its already existing applications, present the maintenance process and evaluate its quality.","publish_time":1588809600000,"author_summary":" Bader, Sebastian R.; Grangel-Gonzalez,<br>Irlan; Nanjappa, Priyanka; Vidal, Maria-Esther;<br>Maleshkova, Maria","abstract_summary":" One of the most crucial tasks for today\u2019s<br>knowledge workers is to get and retain a thorough overview<br>on the latest state of the art. Especially in<br>dynamic and evolving domains, the amount of relevant<br>sources is constantly increasing, updating and<br>overruling previous methods and approaches. For<br>instance, the digital transformation of manufacturing<br>systems, called Industry 4.0, currently faces an<br>overwhelming amount of standardization efforts and<br>reference initiatives, resulting in a sophisticated<br>information environment. We propose a structured dataset<br>in the form of a semantically annotated knowledge<br>graph for Industry 4.0 related standards, norms and<br>reference frameworks. The graph provides...","title_summary":" A Knowledge Graph for Industry 4.0","x":-33.0989112854,"y":35.8151512146,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.0989112854,"tsne_y":35.8151512146,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"kf65lxp5","source_x":"PMC","title":"Embedding-Based Recommendations on Scholarly Knowledge Graphs","doi":"10.1007\/978-3-030-49461-2_15","abstract":"The increasing availability of scholarly metadata in the form of Knowledge Graphs (KG) offers opportunities for studying the structure of scholarly communication and evolution of science. Such KGs build the foundation for knowledge-driven tasks e.g., link discovery, prediction and entity classification which allow to provide recommendation services. Knowledge graph embedding (KGE) models have been investigated for such knowledge-driven tasks in different application domains. One of the applications of KGE models is to provide link predictions, which can also be viewed as a foundation for recommendation service, e.g. high confidence \u201cco-author\u201d links in a scholarly knowledge graph can be seen as suggested collaborations. In this paper, KGEs are reconciled with a specific loss function (Soft Margin) and examined with respect to their performance for co-authorship link prediction task on scholarly KGs. The results show a significant improvement in the accuracy of the experimented KGE models on the considered scholarly KGs using this specific loss. TransE with Soft Margin (TransE-SM) obtains a score of 79.5% Hits@10 for co-authorship link prediction task while the original TransE obtains 77.2%, on the same task. In terms of accuracy and Hits@10, TransE-SM also outperforms other state-of-the-art embedding models such as ComplEx, ConvE and RotatE in this setting. The predicted co-authorship links have been validated by evaluating profile of scholars.","publish_time":1588809600000,"author_summary":" Nayyeri, Mojtaba; Vahdati, Sahar; Zhou,<br>Xiaotian; Shariat Yazdi, Hamed; Lehmann, Jens","abstract_summary":" The increasing availability of scholarly<br>metadata in the form of Knowledge Graphs (KG) offers<br>opportunities for studying the structure of scholarly<br>communication and evolution of science. Such KGs build the<br>foundation for knowledge-driven tasks e.g., link<br>discovery, prediction and entity classification which<br>allow to provide recommendation services.<br>Knowledge graph embedding (KGE) models have been<br>investigated for such knowledge-driven tasks in different<br>application domains. One of the applications of KGE models<br>is to provide link predictions, which can also be<br>viewed as a foundation for recommendation service,<br>e.g. high confidence \u201cco-author\u201d links in a<br>scholarly knowledge graph can be seen as...","title_summary":" Embedding-Based Recommendations on<br>Scholarly Knowledge Graphs","x":-32.6907806396,"y":35.4587364197,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.6907806396,"tsne_y":35.4587364197,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"g039jdp6","source_x":"PMC","title":"Keyword Search over RDF Using Document-Centric Information Retrieval Systems","doi":"10.1007\/978-3-030-49461-2_8","abstract":"For ordinary users, the task of accessing knowledge graphs through structured query languages like SPARQL is rather demanding. As a result, various approaches exploit the simpler and widely used keyword-based search paradigm, either by translating keyword queries to structured queries, or by adopting classical information retrieval (IR) techniques. In this paper, we study and adapt Elasticsearch, an out-of-the-box document-centric IR system, for supporting keyword search over RDF datasets. Contrary to other works that mainly retrieve entities, we opt for retrieving triples, due to their expressiveness and informativeness. We specify the set of functional requirements and study the emerging questions related to the selection and weighting of the triple data to index, and the structuring and ranking of the retrieved results. Finally, we perform an extensive evaluation of the different factors that affect the IR performance for four different query types. The reported results are promising and offer useful insights on how different Elasticsearch configurations affect the retrieval effectiveness and efficiency.","publish_time":1588809600000,"author_summary":" Kadilierakis, Giorgos; Fafalios, Pavlos;<br>Papadakos, Panagiotis; Tzitzikas, Yannis","abstract_summary":" For ordinary users, the task of accessing<br>knowledge graphs through structured query languages<br>like SPARQL is rather demanding. As a result,<br>various approaches exploit the simpler and widely used<br>keyword-based search paradigm, either by translating<br>keyword queries to structured queries, or by adopting<br>classical information retrieval (IR) techniques. In<br>this paper, we study and adapt Elasticsearch, an<br>out-of-the-box document-centric IR system, for supporting<br>keyword search over RDF datasets. Contrary to other<br>works that mainly retrieve entities, we opt for<br>retrieving triples, due to their expressiveness and<br>informativeness. We specify the set of functional requirements<br>and study the emerging questions related...","title_summary":" Keyword Search over RDF Using<br>Document-Centric Information Retrieval Systems","x":-33.6661453247,"y":35.1136322021,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6661453247,"tsne_y":35.1136322021,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"lonn6u82","source_x":"PMC","title":"ESBM: An Entity Summarization BenchMark","doi":"10.1007\/978-3-030-49461-2_32","abstract":"Entity summarization is the problem of computing an optimal compact summary for an entity by selecting a size-constrained subset of triples from RDF data. Entity summarization supports a multiplicity of applications and has led to fruitful research. However, there is a lack of evaluation efforts that cover the broad spectrum of existing systems. One reason is a lack of benchmarks for evaluation. Some benchmarks are no longer available, while others are small and have limitations. In this paper, we create an Entity Summarization BenchMark (ESBM) which overcomes the limitations of existing benchmarks and meets standard desiderata for a benchmark. Using this largest available benchmark for evaluating general-purpose entity summarizers, we perform the most extensive experiment to date where 9 existing systems are compared. Considering that all of these systems are unsupervised, we also implement and evaluate a supervised learning based system for reference.","publish_time":1588809600000,"author_summary":" Liu, Qingxia; Cheng, Gong; Gunaratna, Kalpa;<br>Qu, Yuzhong","abstract_summary":" Entity summarization is the problem of<br>computing an optimal compact summary for an entity by<br>selecting a size-constrained subset of triples from RDF<br>data. Entity summarization supports a multiplicity<br>of applications and has led to fruitful research.<br>However, there is a lack of evaluation efforts that cover<br>the broad spectrum of existing systems. One reason<br>is a lack of benchmarks for evaluation. Some<br>benchmarks are no longer available, while others are small<br>and have limitations. In this paper, we create an<br>Entity Summarization BenchMark (ESBM) which<br>overcomes the limitations of existing benchmarks and<br>meets standard desiderata for a benchmark. Using...","title_summary":" ESBM: An Entity Summarization BenchMark","x":-33.5651016235,"y":34.5464286804,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5651016235,"tsne_y":34.5464286804,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"dxxd62lr","source_x":"PMC","title":"SAShA: Semantic-Aware Shilling Attacks on Recommender Systems Exploiting Knowledge Graphs","doi":"10.1007\/978-3-030-49461-2_18","abstract":"Recommender systems (RS) play a focal position in modern user-centric online services. Among them, collaborative filtering (CF) approaches have shown leading accuracy performance compared to content-based filtering (CBF) methods. Their success is due to an effective exploitation of similarities\/correlations encoded in user interaction patterns, which is computed by considering common items users rated in the past. However, their strength is also their weakness. Indeed, a malicious agent can alter recommendations by adding fake user profiles into the platform thereby altering the actual similarity values in an engineered way. The spread of well-curated information available in knowledge graphs ([Formula: see text]) has opened the door to several new possibilities in compromising the security of a recommender system. In fact, [Formula: see text] are a wealthy source of information that can dramatically increase the attacker\u2019s (and the defender\u2019s) knowledge of the underlying system. In this paper, we introduce SAShA, a new attack strategy that leverages semantic features extracted from a knowledge graph in order to strengthen the efficacy of the attack to standard CF models. We performed an extensive experimental evaluation in order to investigate whether SAShA is more effective than baseline attacks against CF models by taking into account the impact of various semantic features. Experimental results on two real-world datasets show the usefulness of our strategy in favor of attacker\u2019s capacity in attacking CF models.","publish_time":1588809600000,"author_summary":" Anelli, Vito Walter; Deldjoo, Yashar; Di Noia,<br>Tommaso; Di Sciascio, Eugenio; Merra, Felice Antonio","abstract_summary":" Recommender systems (RS) play a focal position<br>in modern user-centric online services. Among<br>them, collaborative filtering (CF) approaches have<br>shown leading accuracy performance compared to<br>content-based filtering (CBF) methods. Their success is due<br>to an effective exploitation of<br>similarities\/correlations encoded in user interaction patterns, which is<br>computed by considering common items users rated in the<br>past. However, their strength is also their<br>weakness. Indeed, a malicious agent can alter<br>recommendations by adding fake user profiles into the platform<br>thereby altering the actual similarity values in an<br>engineered way. The spread of well-curated information<br>available in knowledge graphs ([Formula: see text])...","title_summary":" SAShA: Semantic-Aware Shilling Attacks on<br>Recommender Systems Exploiting Knowledge Graphs","x":-30.8525981903,"y":34.1607093811,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8525981903,"tsne_y":34.1607093811,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"eb502cin","source_x":"PMC","title":"Entity Summarization with User Feedback","doi":"10.1007\/978-3-030-49461-2_22","abstract":"Semantic Web applications have benefited from entity summarization techniques which compute a compact summary for an entity by selecting a set of key triples from underlying data. A wide variety of entity summarizers have been developed. However, the quality of summaries they generate are still not satisfying, and we lack mechanisms for improving computed summaries. To address this challenge, in this paper we present the first study of entity summarization with user feedback. We consider a cooperative environment where a user reads the current entity summary and provides feedback to help an entity summarizer compute an improved summary. Our approach represents this iterative process as a Markov decision process where the entity summarizer is modeled as a reinforcement learning agent. To exploit user feedback, we represent the interdependence of triples in the current summary and the user feedback by a novel deep neural network which is incorporated into the policy of the agent. Our approach outperforms five baseline methods in extensive experiments with both real users and simulated users.","publish_time":1588809600000,"author_summary":" Liu, Qingxia; Chen, Yue; Cheng, Gong;<br>Kharlamov, Evgeny; Li, Junyou; Qu, Yuzhong","abstract_summary":" Semantic Web applications have benefited from<br>entity summarization techniques which compute a<br>compact summary for an entity by selecting a set of key<br>triples from underlying data. A wide variety of entity<br>summarizers have been developed. However, the quality of<br>summaries they generate are still not satisfying, and we<br>lack mechanisms for improving computed summaries.<br>To address this challenge, in this paper we<br>present the first study of entity summarization with<br>user feedback. We consider a cooperative<br>environment where a user reads the current entity summary<br>and provides feedback to help an entity summarizer<br>compute an improved summary. Our approach...","title_summary":" Entity Summarization with User Feedback","x":-33.5302124023,"y":33.8178749084,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5302124023,"tsne_y":33.8178749084,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"vm5jl6il","source_x":"PMC","title":"YAGO 4: A Reason-able Knowledge Base","doi":"10.1007\/978-3-030-49461-2_34","abstract":"YAGO is one of the large knowledge bases in the Linked Open Data cloud. In this resource paper, we present its latest version, YAGO 4, which reconciles the rigorous typing and constraints of schema.org with the rich instance data of Wikidata. The resulting resource contains 2 billion type-consistent triples for 64 Million entities, and has a consistent ontology that allows semantic reasoning with OWL 2 description logics.","publish_time":1588809600000,"author_summary":" Pellissier Tanon, Thomas; Weikum, Gerhard;<br>Suchanek, Fabian","abstract_summary":" YAGO is one of the large knowledge bases in the<br>Linked Open Data cloud. In this resource paper, we<br>present its latest version, YAGO 4, which reconciles<br>the rigorous typing and constraints of schema.org<br>with the rich instance data of Wikidata. The<br>resulting resource contains 2 billion type-consistent<br>triples for 64 Million entities, and has a consistent<br>ontology that allows semantic reasoning with OWL 2<br>description logics.","title_summary":" YAGO 4: A Reason-able Knowledge Base","x":-33.9153823853,"y":36.5438232422,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9153823853,"tsne_y":36.5438232422,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"c3na8gze","source_x":"PMC","title":"VQuAnDa: Verbalization QUestion ANswering DAtaset","doi":"10.1007\/978-3-030-49461-2_31","abstract":"Question Answering (QA) systems over Knowledge Graphs (KGs) aim to provide a concise answer to a given natural language question. Despite the significant evolution of QA methods over the past years, there are still some core lines of work, which are lagging behind. This is especially true for methods and datasets that support the verbalization of answers in natural language. Specifically, to the best of our knowledge, none of the existing Question Answering datasets provide any verbalization data for the question-query pairs. Hence, we aim to fill this gap by providing the first QA dataset VQuAnDa that includes the verbalization of each answer. We base VQuAnDa on a commonly used large-scale QA dataset \u2013 LC-QuAD, in order to support compatibility and continuity of previous work. We complement the dataset with baseline scores for measuring future training and evaluation work, by using a set of standard sequence to sequence models and sharing the results of the experiments. This resource empowers researchers to train and evaluate a variety of models to generate answer verbalizations.","publish_time":1588809600000,"author_summary":" Kacupaj, Endri; Zafar, Hamid; Lehmann, Jens;<br>Maleshkova, Maria","abstract_summary":" Question Answering (QA) systems over<br>Knowledge Graphs (KGs) aim to provide a concise answer to a<br>given natural language question. Despite the<br>significant evolution of QA methods over the past years,<br>there are still some core lines of work, which are<br>lagging behind. This is especially true for methods and<br>datasets that support the verbalization of answers in<br>natural language. Specifically, to the best of our<br>knowledge, none of the existing Question Answering<br>datasets provide any verbalization data for the<br>question-query pairs. Hence, we aim to fill this gap by<br>providing the first QA dataset VQuAnDa that includes the<br>verbalization...","title_summary":" VQuAnDa: Verbalization QUestion ANswering<br>DAtaset","x":-34.9300003052,"y":32.930393219,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.9300003052,"tsne_y":32.930393219,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"gp7ttnar","source_x":"PMC","title":"Applying Knowledge Graphs as Integrated Semantic Information Model for the Computerized Engineering of Building Automation Systems","doi":"10.1007\/978-3-030-49461-2_36","abstract":"During the life cycle of a smart building, an extensive amount of heterogeneous information is required to plan, construct, operate and maintain the building and its technical systems. Traditionally, there is an information gap between the different phases and stakeholders, leading to information being exchanged, processed and stored in a variety of mostly human-readable documents. This paper shows how a knowledge graph can be established as integrated information model that can provide the required information for all phases in a machine-interpretable way. The knowledge graph describes and connects all relevant information, which allows combining and applying it in a holistic way. This makes the knowledge graph a key enabler for a variety of advanced, computerized engineering tasks, ranging from the planning and design phases over the commissioning and the operation of a building. The computerized engineering of building automation systems (BAS) with an advanced software tool chain is presented as such a use case in more detail. The knowledge graph is based on standard semantic web technologies and builds on existing ontologies, such as the Brick and QUDT ontologies, with various novel extensions presented in this paper. Special attention is given to the rich semantic definition of the entities, such as the equipment and the typically thousands of datapoints in a BAS, which can be achieved as a combination of contextual modeling and semantic tagging.","publish_time":1588809600000,"author_summary":" Dibowski, Henrik; Massa Gray, Francesco","abstract_summary":" During the life cycle of a smart building, an<br>extensive amount of heterogeneous information is<br>required to plan, construct, operate and maintain the<br>building and its technical systems. Traditionally,<br>there is an information gap between the different<br>phases and stakeholders, leading to information<br>being exchanged, processed and stored in a variety of<br>mostly human-readable documents. This paper shows<br>how a knowledge graph can be established as<br>integrated information model that can provide the<br>required information for all phases in a<br>machine-interpretable way. The knowledge graph describes and<br>connects all relevant information, which allows<br>combining and applying it in a holistic...","title_summary":" Applying Knowledge Graphs as Integrated<br>Semantic Information Model for the Computerized<br>Engineering of Building Automation Systems","x":-33.7730712891,"y":36.7301063538,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7730712891,"tsne_y":36.7301063538,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"vh147p7e","source_x":"PMC","title":"Unsupervised Bootstrapping of Active Learning for Entity Resolution","doi":"10.1007\/978-3-030-49461-2_13","abstract":"Entity resolution is one of the central challenges when integrating data from large numbers of data sources. Active learning for entity resolution aims to learn high-quality matching models while minimizing the human labeling effort by selecting only the most informative record pairs for labeling. Most active learning methods proposed so far, start with an empty set of labeled record pairs and iteratively improve the prediction quality of a classification model by asking for new labels. The absence of adequate labeled data in the early active learning iterations leads to unstable models of low quality which is known as the cold start problem. In our work we solve the cold start problem using an unsupervised matching method to bootstrap active learning. We implement a thresholding heuristic that considers pre-calculated similarity scores and assigns matching labels with some degree of noise at no manual labeling cost. The noisy labels are used for initializing the active learning process and throughout the whole active learning cycle for model learning and query selection. We evaluate our pipeline with six datasets from three different entity resolution settings using active learning with a committee-based query strategy and show it successfully deals with the cold start problem. Comparing our method against two active learning baselines without bootstrapping, we show that it can additionally lead to overall improved learned models in terms of [Formula: see text] score and stability.","publish_time":1588809600000,"author_summary":" Primpeli, Anna; Bizer, Christian; Keuper,<br>Margret","abstract_summary":" Entity resolution is one of the central<br>challenges when integrating data from large numbers of<br>data sources. Active learning for entity<br>resolution aims to learn high-quality matching models<br>while minimizing the human labeling effort by<br>selecting only the most informative record pairs for<br>labeling. Most active learning methods proposed so far,<br>start with an empty set of labeled record pairs and<br>iteratively improve the prediction quality of a<br>classification model by asking for new labels. The absence of<br>adequate labeled data in the early active learning<br>iterations leads to unstable models of low quality which is<br>known as the cold...","title_summary":" Unsupervised Bootstrapping of Active<br>Learning for Entity Resolution","x":-33.1499900818,"y":33.844493866,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.1499900818,"tsne_y":33.844493866,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"9n5692ul","source_x":"PMC","title":"Hyperbolic Knowledge Graph Embeddings for Knowledge Base Completion","doi":"10.1007\/978-3-030-49461-2_12","abstract":"Learning embeddings of entities and relations existing in knowledge bases allows the discovery of hidden patterns in them. In this work, we examine the contribution of geometrical space to the task of knowledge base completion. We focus on the family of translational models, whose performance has been lagging. We extend these models to the hyperbolic space so as to better reflect the topological properties of knowledge bases. We investigate the type of regularities that our model, dubbed HyperKG, can capture and show that it is a prominent candidate for effectively representing a subset of Datalog rules. We empirically show, using a variety of link prediction datasets, that hyperbolic space allows to narrow down significantly the performance gap between translational and bilinear models and effectively represent certain types of rules. ELECTRONIC SUPPLEMENTARY MATERIAL: The online version of this chapter (10.1007\/978-3-030-49461-2_12) contains supplementary material, which is available to authorized users.","publish_time":1588809600000,"author_summary":" Kolyvakis, Prodromos; Kalousis, Alexandros;<br>Kiritsis, Dimitris","abstract_summary":" Learning embeddings of entities and relations<br>existing in knowledge bases allows the discovery of<br>hidden patterns in them. In this work, we examine the<br>contribution of geometrical space to the task of knowledge<br>base completion. We focus on the family of<br>translational models, whose performance has been lagging. We<br>extend these models to the hyperbolic space so as to<br>better reflect the topological properties of<br>knowledge bases. We investigate the type of regularities<br>that our model, dubbed HyperKG, can capture and show<br>that it is a prominent candidate for effectively<br>representing a subset of Datalog rules. We empirically show,<br>using...","title_summary":" Hyperbolic Knowledge Graph Embeddings for<br>Knowledge Base Completion","x":-31.6459121704,"y":35.6345329285,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.6459121704,"tsne_y":35.6345329285,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"sekpmynq","source_x":"PMC","title":"Hybrid Reasoning Over Large Knowledge Bases Using On-The-Fly Knowledge Extraction","doi":"10.1007\/978-3-030-49461-2_5","abstract":"The success of logic-based methods for comparing entities heavily depends on the axioms that have been described for them in the Knowledge Base (KB). Due to the incompleteness of even large and well engineered KBs, such methods suffer from low recall when applied in real-world use cases. To address this, we designed a reasoning framework that combines logic-based subsumption with statistical methods for on-the-fly knowledge extraction. Statistical methods extract additional (missing) axioms for the compared entities with the goal of tackling the incompleteness of KBs and thus improving recall. Although this can be beneficial, it can also introduce noise (false positives or false negatives). Hence, our framework uses heuristics to assess whether knowledge extraction is likely to be advantageous and only activates the statistical components if this is the case. We instantiate our framework by combining lightweight logic-based reasoning implemented on top of existing triple-stores with an axiom extraction method that is based on the labels of concepts. Our work was motivated by industrial use cases over which we evaluate our instantiated framework, showing that it outperforms approaches that are only based on textual information. Besides the best combination of precision and recall, our implementation is also scalable and is currently used in an industrial production environment.","publish_time":1588809600000,"author_summary":" Stoilos, Giorgos; Juric, Damir; Wartak,<br>Szymon; Schulz, Claudia; Khodadadi, Mohammad","abstract_summary":" The success of logic-based methods for<br>comparing entities heavily depends on the axioms that<br>have been described for them in the Knowledge Base<br>(KB). Due to the incompleteness of even large and well<br>engineered KBs, such methods suffer from low recall when<br>applied in real-world use cases. To address this, we<br>designed a reasoning framework that combines<br>logic-based subsumption with statistical methods for<br>on-the-fly knowledge extraction. Statistical methods<br>extract additional (missing) axioms for the compared<br>entities with the goal of tackling the incompleteness of<br>KBs and thus improving recall. Although this can be<br>beneficial, it can also introduce noise (false...","title_summary":" Hybrid Reasoning Over Large Knowledge Bases<br>Using On-The-Fly Knowledge Extraction","x":-33.4930648804,"y":36.5425300598,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4930648804,"tsne_y":36.5425300598,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"948cxajh","source_x":"PMC","title":"The Knowledge Graph Track at OAEI: Gold Standards, Baselines, and the Golden Hammer Bias","doi":"10.1007\/978-3-030-49461-2_20","abstract":"The Ontology Alignment Evaluation Initiative (OAEI) is an annual evaluation of ontology matching tools. In 2018, we have started the Knowledge Graph track, whose goal is to evaluate the simultaneous matching of entities and schemas of large-scale knowledge graphs. In this paper, we discuss the design of the track and two different strategies of gold standard creation. We analyze results and experiences obtained in first editions of the track, and, by revealing a hidden task, we show that all tools submitted to the track (and probably also to other tracks) suffer from a bias which we name the golden hammer bias.","publish_time":1588809600000,"author_summary":" Hertling, Sven; Paulheim, Heiko","abstract_summary":" The Ontology Alignment Evaluation Initiative<br>(OAEI) is an annual evaluation of ontology matching<br>tools. In 2018, we have started the Knowledge Graph<br>track, whose goal is to evaluate the simultaneous<br>matching of entities and schemas of large-scale<br>knowledge graphs. In this paper, we discuss the design of<br>the track and two different strategies of gold<br>standard creation. We analyze results and experiences<br>obtained in first editions of the track, and, by<br>revealing a hidden task, we show that all tools submitted<br>to the track (and probably also to other tracks)<br>suffer from a bias which we name the golden hammer...","title_summary":" The Knowledge Graph Track at OAEI: Gold<br>Standards, Baselines, and the Golden Hammer Bias","x":-33.3782196045,"y":35.7975921631,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3782196045,"tsne_y":35.7975921631,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"m7800or1","source_x":"PMC","title":"MetaLink: A Travel Guide to the LOD Cloud","doi":"10.1007\/978-3-030-49461-2_28","abstract":"Graph-based traversal is an important navigation paradigm for the Semantic Web, where datasets are interlinked to provide context. While following links may result in the discovery of complementary data sources and enriched query results, it is widely recognized that traversing the LOD Cloud indiscriminately results in low quality answers. Over the years, approaches have been published that help to determine whether links are trustworthy or not, based on certain criteria. While such approaches are often useful for specific datasets and\/or in specific applications, they are not yet widely used in practice or at the scale of the entire LOD Cloud. This paper introduces a new resource called MetaLink. MetaLink is a dataset that contains metadata for a very large set of owl:sameAs links that are crawled from the LOD Cloud. MetaLink encodes a previously published error metric for each of these links. MetaLink is published in combination with LOD-a-lot, a dataset that is based on a large crawl of a subset of the LOD Cloud. By combining MetaLink and LOD-a-lot, applications are able to make informed decisions about whether or not to follow specific links on the LOD Cloud. This paper describes our approach for creating the MetaLink dataset. It describes the vocabulary that it uses and provides an overview of multiple real-world use cases in which the MetaLink dataset can solve non-trivial research and application challenges that were not addressed before.","publish_time":1588809600000,"author_summary":" Beek, Wouter; Raad, Joe; Acar, Erman; van<br>Harmelen, Frank","abstract_summary":" Graph-based traversal is an important<br>navigation paradigm for the Semantic Web, where datasets<br>are interlinked to provide context. While<br>following links may result in the discovery of<br>complementary data sources and enriched query results, it is<br>widely recognized that traversing the LOD Cloud<br>indiscriminately results in low quality answers. Over the years,<br>approaches have been published that help to determine<br>whether links are trustworthy or not, based on certain<br>criteria. While such approaches are often useful for<br>specific datasets and\/or in specific applications,<br>they are not yet widely used in practice or at the<br>scale of the entire LOD Cloud....","title_summary":" MetaLink: A Travel Guide to the LOD Cloud","x":-33.3290672302,"y":35.7286987305,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3290672302,"tsne_y":35.7286987305,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"2pxms5k6","source_x":"PMC","title":"Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach","doi":"10.1007\/978-3-030-49461-2_16","abstract":"Knowledge about the software used in scientific investigations is necessary for different reasons, including provenance of the results, measuring software impact to attribute developers, and bibliometric software citation analysis in general. Additionally, providing information about whether and how the software and the source code are available allows an assessment about the state and role of open source software in science in general. While such analyses can be done manually, large scale analyses require the application of automated methods of information extraction and linking. In this paper, we present SoftwareKG\u2014a knowledge graph that contains information about software mentions from more than 51,000 scientific articles from the social sciences. A silver standard corpus, created by a distant and weak supervision approach, and a gold standard corpus, created by manual annotation, were used to train an LSTM based neural network to identify software mentions in scientific articles. The model achieves a recognition rate of .82 F-score in exact matches. As a result, we identified more than 133,000 software mentions. For entity disambiguation, we used the public domain knowledge base DBpedia. Furthermore, we linked the entities of the knowledge graph to other knowledge bases such as the Microsoft Academic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we illustrate, how SoftwareKG can be used to assess the role of software in the social sciences.","publish_time":1588809600000,"author_summary":" Schindler, David; Zapilko, Benjamin; Kr\u00fcger,<br>Frank","abstract_summary":" Knowledge about the software used in<br>scientific investigations is necessary for different<br>reasons, including provenance of the results,<br>measuring software impact to attribute developers, and<br>bibliometric software citation analysis in general.<br>Additionally, providing information about whether and how<br>the software and the source code are available<br>allows an assessment about the state and role of open<br>source software in science in general. While such<br>analyses can be done manually, large scale analyses<br>require the application of automated methods of<br>information extraction and linking. In this paper, we<br>present SoftwareKG\u2014a knowledge graph that contains<br>information about software mentions from more than...","title_summary":" Investigating Software Usage in the Social<br>Sciences: A Knowledge Graph Approach","x":-33.3873519897,"y":35.0364074707,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3873519897,"tsne_y":35.0364074707,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"sl9kredk","source_x":"PMC","title":"SemTab 2019: Resources to Benchmark Tabular Data to Knowledge Graph Matching Systems","doi":"10.1007\/978-3-030-49461-2_30","abstract":"Tabular data to Knowledge Graph matching is the process of assigning semantic tags from knowledge graphs (e.g., Wikidata or DBpedia) to the elements of a table. This task is a challenging problem for various reasons, including the lack of metadata (e.g., table and column names), the noisiness, heterogeneity, incompleteness and ambiguity in the data. The results of this task provide significant insights about potentially highly valuable tabular data, as recent works have shown, enabling a new family of data analytics and data science applications. Despite significant amount of work on various flavors of this problem, there is a lack of a common framework to conduct a systematic evaluation of state-of-the-art systems. The creation of the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab) aims at filling this gap. In this paper, we report about the datasets, infrastructure and lessons learned from the first edition of the SemTab challenge.","publish_time":1588809600000,"author_summary":" Jim\u00e9nez-Ruiz, Ernesto; Hassanzadeh, Oktie;<br>Efthymiou, Vasilis; Chen, Jiaoyan; Srinivas, Kavitha","abstract_summary":" Tabular data to Knowledge Graph matching is the<br>process of assigning semantic tags from knowledge<br>graphs (e.g., Wikidata or DBpedia) to the elements of a<br>table. This task is a challenging problem for various<br>reasons, including the lack of metadata (e.g., table and<br>column names), the noisiness, heterogeneity,<br>incompleteness and ambiguity in the data. The results of this<br>task provide significant insights about<br>potentially highly valuable tabular data, as recent works<br>have shown, enabling a new family of data analytics<br>and data science applications. Despite<br>significant amount of work on various flavors of this<br>problem, there is a lack of...","title_summary":" SemTab 2019: Resources to Benchmark Tabular<br>Data to Knowledge Graph Matching Systems","x":-33.1799888611,"y":35.6706047058,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.1799888611,"tsne_y":35.6706047058,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"ddwzgqta","source_x":"PMC","title":"GEval: A Modular and Extensible Evaluation Framework for Graph Embedding Techniques","doi":"10.1007\/978-3-030-49461-2_33","abstract":"While RDF data are graph shaped by nature, most traditional Machine Learning (ML) algorithms expect data in a vector form. To transform graph elements to vectors, several graph embedding approaches have been proposed. Comparing these approaches is interesting for 1) developers of new embedding techniques to verify in which cases their proposal outperforms the state-of-art and 2) consumers of these techniques in choosing the best approach according to the task(s) the vectors will be used for. The comparison could be delayed (and made difficult) by the choice of tasks, the design of the evaluation, the selection of models, parameters, and needed datasets. We propose GEval, an evaluation framework to simplify the evaluation and the comparison of graph embedding techniques. The covered tasks range from ML tasks (Classification, Regression, Clustering), semantic tasks (entity relatedness, document similarity) to semantic analogies. However, GEval is designed to be (easily) extensible. In this article, we will describe the design and development of the proposed framework by detailing its overall structure, the already implemented tasks, and how to extend it. In conclusion, to demonstrate its operating approach, we consider the parameter tuning of the KGloVe algorithm as a use case.","publish_time":1588809600000,"author_summary":" Pellegrino, Maria Angela; Altabba,<br>Abdulrahman; Garofalo, Martina; Ristoski, Petar; Cochez,<br>Michael","abstract_summary":" While RDF data are graph shaped by nature, most<br>traditional Machine Learning (ML) algorithms expect data<br>in a vector form. To transform graph elements to<br>vectors, several graph embedding approaches have been<br>proposed. Comparing these approaches is interesting for<br>1) developers of new embedding techniques to<br>verify in which cases their proposal outperforms the<br>state-of-art and 2) consumers of these techniques in<br>choosing the best approach according to the task(s) the<br>vectors will be used for. The comparison could be<br>delayed (and made difficult) by the choice of tasks, the<br>design of the evaluation, the selection of models,<br>parameters, and...","title_summary":" GEval: A Modular and Extensible Evaluation<br>Framework for Graph Embedding Techniques","x":-32.5480575562,"y":35.4321136475,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.5480575562,"tsne_y":35.4321136475,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"omfkd5dp","source_x":"PMC","title":"On Modeling the Physical World as a Collection of Things: The W3C Thing Description Ontology","doi":"10.1007\/978-3-030-49461-2_35","abstract":"This document presents the Thing Description ontology, an axiomatization of the W3C Thing Description model. It also introduces an alignment with the Semantic Sensor Network ontology and evaluates how this alignment contributes to semantic interoperability in the Web of Things.","publish_time":1588809600000,"author_summary":" Charpenay, Victor; K\u00e4bisch, Sebastian","abstract_summary":" This document presents the Thing Description<br>ontology, an axiomatization of the W3C Thing Description<br>model. It also introduces an alignment with the<br>Semantic Sensor Network ontology and evaluates how this<br>alignment contributes to semantic interoperability in<br>the Web of Things.","title_summary":" On Modeling the Physical World as a Collection<br>of Things: The W3C Thing Description Ontology","x":-34.118106842,"y":37.0000572205,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.118106842,"tsne_y":37.0000572205,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"qfpfllay","source_x":"PMC","title":"Fostering Scientific Meta-analyses with Knowledge Graphs: A Case-Study","doi":"10.1007\/978-3-030-49461-2_17","abstract":"A meta-analysis is a Science of Science method widely used in the medical and social sciences to review, aggregate and quantitatively synthesise a body of studies that address the same research question. With the volume of research growing exponentially every year, conducting meta-analyses can be costly and inefficient, as a significant amount of time and human efforts needs to be spent in finding studies meeting research criteria, annotating them, and properly performing the statistical analyses to summarise the findings. In this work, we show these issues can be tackled with semantic representations and technologies, using a social science scenario as case-study. We show how the domain-specific content of research outputs can be represented and used to facilitate their search, analysis and synthesis. We present the very first representation of the domain of human cooperation, and the application we built on top of this to help experts in performing meta-analyses semi-automatically. Using few application scenarios, we show how our approach supports the various phases meta-analyses, and more in general contributes towards research replication and automated hypotheses generation.","publish_time":1588809600000,"author_summary":" Tiddi, Ilaria; Balliet, Daniel; ten Teije,<br>Annette","abstract_summary":" A meta-analysis is a Science of Science method<br>widely used in the medical and social sciences to<br>review, aggregate and quantitatively synthesise a<br>body of studies that address the same research<br>question. With the volume of research growing<br>exponentially every year, conducting meta-analyses can be<br>costly and inefficient, as a significant amount of<br>time and human efforts needs to be spent in finding<br>studies meeting research criteria, annotating them,<br>and properly performing the statistical analyses<br>to summarise the findings. In this work, we show<br>these issues can be tackled with semantic<br>representations and technologies, using a social science<br>scenario as...","title_summary":" Fostering Scientific Meta-analyses with<br>Knowledge Graphs: A Case-Study","x":-33.2875862122,"y":35.312461853,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.2875862122,"tsne_y":35.312461853,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"ak19r2w8","source_x":"PMC","title":"Incremental Multi-source Entity Resolution for Knowledge Graph Completion","doi":"10.1007\/978-3-030-49461-2_23","abstract":"We present and evaluate new methods for incremental entity resolution as needed for the completion of knowledge graphs integrating data from multiple sources. Compared to previous approaches we aim at reducing the dependency on the order in which new sources and entities are added. For this purpose, we consider sets of new entities for an optimized assignment of them to entity clusters. We also propose the use of a light-weight approach to repair entity clusters in order to correct wrong clusters. The new approaches are integrated within the FAMER framework for parallel and scalable entity clustering. A detailed evaluation of the new approaches for real-world workloads shows their high effectiveness. In particular, the repair approach outperforms other incremental approaches and achieves the same quality than with batch-like entity resolution showing that its results are independent from the order in which new entities are added.","publish_time":1588809600000,"author_summary":" Saeedi, Alieh; Peukert, Eric; Rahm, Erhard","abstract_summary":" We present and evaluate new methods for<br>incremental entity resolution as needed for the completion<br>of knowledge graphs integrating data from<br>multiple sources. Compared to previous approaches we<br>aim at reducing the dependency on the order in which<br>new sources and entities are added. For this<br>purpose, we consider sets of new entities for an<br>optimized assignment of them to entity clusters. We also<br>propose the use of a light-weight approach to repair<br>entity clusters in order to correct wrong clusters.<br>The new approaches are integrated within the FAMER<br>framework for parallel and scalable entity clustering. A<br>detailed evaluation of...","title_summary":" Incremental Multi-source Entity Resolution<br>for Knowledge Graph Completion","x":-32.8472061157,"y":35.2347068787,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.8472061157,"tsne_y":35.2347068787,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"472hlrau","source_x":"PMC","title":"Astrea: Automatic Generation of SHACL Shapes from Ontologies","doi":"10.1007\/978-3-030-49461-2_29","abstract":"Knowledge Graphs (KGs) that publish RDF data modelled using ontologies in a wide range of domains have populated the Web. The SHACL language is a W3C recommendation that has been endowed to encode a set of either value or model data restrictions that aim at validating KG data, ensuring data quality. Developing shapes is a complex and time consuming task that is not feasible to achieve manually. This article presents two resources that aim at generating automatically SHACL shapes for a set of ontologies: (1) Astrea-KG, a KG that publishes a set of mappings that encode the equivalent conceptual restrictions among ontology constraint patterns and SHACL constraint patterns, and (2) Astrea, a tool that automatically generates SHACL shapes from a set of ontologies by executing the mappings from the Astrea-KG. These two resources are openly available at Zenodo, GitHub, and a web application. In contrast to other proposals, these resources cover a large number of SHACL restrictions producing both value and model data restrictions, whereas other proposals consider only a limited number of restrictions or focus only on value or model restrictions.","publish_time":1588809600000,"author_summary":" Cimmino, Andrea; Fern\u00e1ndez-Izquierdo, Alba;<br>Garc\u00eda-Castro, Ra\u00fal","abstract_summary":" Knowledge Graphs (KGs) that publish RDF data<br>modelled using ontologies in a wide range of domains have<br>populated the Web. The SHACL language is a W3C<br>recommendation that has been endowed to encode a set of either<br>value or model data restrictions that aim at<br>validating KG data, ensuring data quality. Developing<br>shapes is a complex and time consuming task that is not<br>feasible to achieve manually. This article presents two<br>resources that aim at generating automatically SHACL<br>shapes for a set of ontologies: (1) Astrea-KG, a KG that<br>publishes a set of mappings that encode the equivalent<br>conceptual restrictions...","title_summary":" Astrea: Automatic Generation of SHACL Shapes<br>from Ontologies","x":-33.6753234863,"y":36.232624054,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6753234863,"tsne_y":36.232624054,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"p21fpv29","source_x":"PMC","title":"Entity Extraction from Wikipedia List Pages","doi":"10.1007\/978-3-030-49461-2_19","abstract":"When it comes to factual knowledge about a wide range of domains, Wikipedia is often the prime source of information on the web. DBpedia and YAGO, as large cross-domain knowledge graphs, encode a subset of that knowledge by creating an entity for each page in Wikipedia, and connecting them through edges. It is well known, however, that Wikipedia-based knowledge graphs are far from complete. Especially, as Wikipedia\u2019s policies permit pages about subjects only if they have a certain popularity, such graphs tend to lack information about less well-known entities. Information about these entities is oftentimes available in the encyclopedia, but not represented as an individual page. In this paper, we present a two-phased approach for the extraction of entities from Wikipedia\u2019s list pages, which have proven to serve as a valuable source of information. In the first phase, we build a large taxonomy from categories and list pages with DBpedia as a backbone. With distant supervision, we extract training data for the identification of new entities in list pages that we use in the second phase to train a classification model. With this approach we extract over 700k new entities and extend DBpedia with 7.5M new type statements and 3.8M new facts of high precision.","publish_time":1588809600000,"author_summary":" Heist, Nicolas; Paulheim, Heiko","abstract_summary":" When it comes to factual knowledge about a wide<br>range of domains, Wikipedia is often the prime source<br>of information on the web. DBpedia and YAGO, as<br>large cross-domain knowledge graphs, encode a<br>subset of that knowledge by creating an entity for each<br>page in Wikipedia, and connecting them through<br>edges. It is well known, however, that<br>Wikipedia-based knowledge graphs are far from complete.<br>Especially, as Wikipedia\u2019s policies permit pages about<br>subjects only if they have a certain popularity, such<br>graphs tend to lack information about less well-known<br>entities. Information about these entities is<br>oftentimes available in the encyclopedia, but...","title_summary":" Entity Extraction from Wikipedia List Pages","x":-33.4573898315,"y":34.858505249,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4573898315,"tsne_y":34.858505249,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"g3k1lpvd","source_x":"PMC","title":"Estimating Characteristic Sets for RDF Dataset Profiles Based on Sampling","doi":"10.1007\/978-3-030-49461-2_10","abstract":"RDF dataset profiles provide a formal representation of a dataset\u2019s characteristics (features). These profiles may cover various aspects of the data represented in the dataset as well as statistical descriptors of the data distribution. In this work, we focus on the characteristic sets profile feature summarizing the characteristic sets contained in an RDF graph. As this type of feature provides detailed information on both the structure and semantics of RDF graphs, they can be very beneficial in query optimization. However, in decentralized query processing, computing them is challenging as it is difficult and\/or costly to access and process all datasets. To overcome this shortcoming, we propose the concept of a profile feature estimation. We present sampling methods and projection functions to generate estimations which aim to be as similar as possible to the original characteristic sets profile feature. In our evaluation, we investigate the feasibility of the proposed methods on four RDF graphs. Our results show that samples containing [Formula: see text] of the entities in the graph allow for good estimations and may be used by downstream tasks such as query plan optimization in decentralized querying.","publish_time":1588809600000,"author_summary":" Heling, Lars; Acosta, Maribel","abstract_summary":" RDF dataset profiles provide a formal<br>representation of a dataset\u2019s characteristics (features).<br>These profiles may cover various aspects of the data<br>represented in the dataset as well as statistical<br>descriptors of the data distribution. In this work, we focus<br>on the characteristic sets profile feature<br>summarizing the characteristic sets contained in an RDF<br>graph. As this type of feature provides detailed<br>information on both the structure and semantics of RDF<br>graphs, they can be very beneficial in query<br>optimization. However, in decentralized query processing,<br>computing them is challenging as it is difficult and\/or<br>costly to access and process all datasets....","title_summary":" Estimating Characteristic Sets for RDF<br>Dataset Profiles Based on Sampling","x":-33.1708450317,"y":35.6350364685,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.1708450317,"tsne_y":35.6350364685,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"xnumh93g","source_x":"PMC","title":"Entity Linking and Lexico-Semantic Patterns for Ontology Learning","doi":"10.1007\/978-3-030-49461-2_9","abstract":"Ontology learning from a text written in natural language is a well-studied domain. However, the applicability of techniques for ontology learning from natural language texts is strongly dependent on the characteristics of the text corpus and the language used. In this paper, we present our work so far in entity linking and enhancing the ontology with extracted relations between concepts. We discuss the benefits of adequately designed lexico-semantic patterns in ontology learning. We propose a preliminary set of lexico-semantic patterns designed for the Czech language to learn new relations between concepts in the related domain ontology in a semi-supervised approach. We utilize data from the urban planning and development domain to evaluate the introduced technique. As a partial prototypical implementation of the stack, we present Annotace, a text annotation service that provides links between the ontology model and the textual documents in Czech.","publish_time":1588809600000,"author_summary":" Saeeda, Lama; Med, Michal; Ledvinka, Martin;<br>Bla\u0161ko, Miroslav; K\u0159emen, Petr","abstract_summary":" Ontology learning from a text written in<br>natural language is a well-studied domain. However,<br>the applicability of techniques for ontology<br>learning from natural language texts is strongly<br>dependent on the characteristics of the text corpus and<br>the language used. In this paper, we present our<br>work so far in entity linking and enhancing the<br>ontology with extracted relations between concepts. We<br>discuss the benefits of adequately designed<br>lexico-semantic patterns in ontology learning. We propose a<br>preliminary set of lexico-semantic patterns designed for<br>the Czech language to learn new relations between<br>concepts in the related domain ontology in a<br>semi-supervised approach....","title_summary":" Entity Linking and Lexico-Semantic Patterns<br>for Ontology Learning","x":-33.897190094,"y":35.7370567322,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.897190094,"tsne_y":35.7370567322,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"g6hz5ix9","source_x":"PMC","title":"Detecting Synonymous Properties by Shared Data-Driven Definitions","doi":"10.1007\/978-3-030-49461-2_21","abstract":"Knowledge graphs have become an essential source of entity-centric information for modern applications. Today\u2019s KGs have reached a size of billions of RDF triples extracted from a variety of sources, including structured sources and text. While this definitely improves completeness, the inherent variety of sources leads to severe heterogeneity, negatively affecting data quality by introducing duplicate information. We present a novel technique for detecting synonymous properties in large knowledge graphs by mining interpretable definitions of properties using association rule mining. Relying on such shared definitions, our technique is able to mine even synonym rules that have only little support in the data. In particular, our extensive experiments on DBpedia and Wikidata show that our rule-based approach can outperform state-of-the-art knowledge graph embedding techniques, while offering good interpretability through shared logical rules.","publish_time":1588809600000,"author_summary":" Kalo, Jan-Christoph; Mennicke, Stephan;<br>Ehler, Philipp; Balke, Wolf-Tilo","abstract_summary":" Knowledge graphs have become an essential<br>source of entity-centric information for modern<br>applications. Today\u2019s KGs have reached a size of billions of<br>RDF triples extracted from a variety of sources,<br>including structured sources and text. While this<br>definitely improves completeness, the inherent variety<br>of sources leads to severe heterogeneity,<br>negatively affecting data quality by introducing<br>duplicate information. We present a novel technique for<br>detecting synonymous properties in large knowledge<br>graphs by mining interpretable definitions of<br>properties using association rule mining. Relying on such<br>shared definitions, our technique is able to mine even<br>synonym rules that have only little support in...","title_summary":" Detecting Synonymous Properties by Shared<br>Data-Driven Definitions","x":-33.2930717468,"y":35.5178604126,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.2930717468,"tsne_y":35.5178604126,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"bo0e54vq","source_x":"PMC","title":"Supporting Complex Decision Making by Semantic Technologies","doi":"10.1007\/978-3-030-49461-2_37","abstract":"Complex decisions require stakeholders to identify potential decision options and collaboratively select the optimal option. Identifying potential decision options and communicating them to stakeholders is a challenging task as it requires the translation of the decision option\u2019s technical dimension to a stakeholder-compliant language which describes the impact of the decision (e.g., financial, political). Existing knowledge-driven decision support methods generate decision options by automatically processing available data and knowledge. Ontology-based methods emerged as a sub-field in the medical domain and provide concrete instructions for given medical problems. However, the research field lacks an evaluated practical approach to support the full cycle from data and knowledge assessment to the actual decision making. This work advances the field by: (i) a problem-driven ontology engineering method which (a) supports creating the necessary ontology model for the given problem domain and (b) harmonizes relevant data and knowledge sources for automatically identifying decision options by reasoners, and (ii) an approach which translates technical decision options into a language that is understood by relevant stakeholders. Expert evaluations and real-world deployments in three different domains demonstrate the added value of this method.","publish_time":1588809600000,"author_summary":" Fenz, Stefan","abstract_summary":" Complex decisions require stakeholders to<br>identify potential decision options and<br>collaboratively select the optimal option. Identifying<br>potential decision options and communicating them to<br>stakeholders is a challenging task as it requires the<br>translation of the decision option\u2019s technical dimension<br>to a stakeholder-compliant language which<br>describes the impact of the decision (e.g., financial,<br>political). Existing knowledge-driven decision support<br>methods generate decision options by automatically<br>processing available data and knowledge. Ontology-based<br>methods emerged as a sub-field in the medical domain and<br>provide concrete instructions for given medical<br>problems. However, the research field lacks an evaluated<br>practical approach to support the full cycle...","title_summary":" Supporting Complex Decision Making by<br>Semantic Technologies","x":-33.9663734436,"y":36.8389625549,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9663734436,"tsne_y":36.8389625549,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"ncsrxp6p","source_x":"PMC","title":"QAnswer KG: Designing a Portable Question Answering System over RDF Data","doi":"10.1007\/978-3-030-49461-2_25","abstract":"While RDF was designed to make data easily readable by machines, it does not make data easily usable by end-users. Question Answering (QA) over Knowledge Graphs (KGs) is seen as the technology which is able to bridge this gap. It aims to build systems which are capable of extracting the answer to a user\u2019s natural language question from an RDF dataset. In recent years, many approaches were proposed which tackle the problem of QA over KGs. Despite such efforts, it is hard and cumbersome to create a Question Answering system on top of a new RDF dataset. The main open challenge remains portability, i.e., the possibility to apply a QA algorithm easily on new and previously untested RDF datasets. In this publication, we address the problem of portability by presenting an architecture for a portable QA system. We present a novel approach called QAnswer KG, which allows the construction of on-demand QA systems over new RDF datasets. Hence, our approach addresses non-expert users in QA domain. In this paper, we provide the details of QA system generation process. We show that it is possible to build a QA system over any RDF dataset while requiring minimal investments in terms of training. We run experiments using 3 different datasets. To the best of our knowledge, we are the first to design a process for non-expert users. We enable such users to efficiently create an on-demand, scalable, multilingual, QA system on top of any RDF dataset.","publish_time":1588809600000,"author_summary":" Diefenbach, Dennis; Gim\u00e9nez-Garc\u00eda, Jos\u00e9;<br>Both, Andreas; Singh, Kamal; Maret, Pierre","abstract_summary":" While RDF was designed to make data easily<br>readable by machines, it does not make data easily usable<br>by end-users. Question Answering (QA) over<br>Knowledge Graphs (KGs) is seen as the technology which is<br>able to bridge this gap. It aims to build systems<br>which are capable of extracting the answer to a user\u2019s<br>natural language question from an RDF dataset. In<br>recent years, many approaches were proposed which<br>tackle the problem of QA over KGs. Despite such<br>efforts, it is hard and cumbersome to create a Question<br>Answering system on top of a new RDF dataset. The main open...","title_summary":" QAnswer KG: Designing a Portable Question<br>Answering System over RDF Data","x":-34.1947822571,"y":35.7026519775,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1947822571,"tsne_y":35.7026519775,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"qmuoxr1z","source_x":"PMC","title":"SchemaTree: Maximum-Likelihood Property Recommendation for Wikidata","doi":"10.1007\/978-3-030-49461-2_11","abstract":"Wikidata is a free and open knowledge base which can be read and edited by both humans and machines. It acts as a central storage for the structured data of several Wikimedia projects. To improve the process of manually inserting new facts, the Wikidata platform features an association rule-based tool to recommend additional suitable properties. In this work, we introduce a novel approach to provide such recommendations based on frequentist inference. We introduce a trie-based method that can efficiently learn and represent property set probabilities in RDF graphs. We extend the method by adding type information to improve recommendation precision and introduce backoff strategies which further increase the performance of the initial approach for entities with rare property combinations. We investigate how the captured structure can be employed for property recommendation, analogously to the Wikidata PropertySuggester. We evaluate our approach on the full Wikidata dataset and compare its performance to the state-of-the-art Wikidata PropertySuggester, outperforming it in all evaluated metrics. Notably we could reduce the average rank of the first relevant recommendation by 71%.","publish_time":1588809600000,"author_summary":" Gleim, Lars C.; Schimassek, Rafael; H\u00fcser,<br>Dominik; Peters, Maximilian; Kr\u00e4mer, Christoph;<br>Cochez, Michael; Decker, Stefan","abstract_summary":" Wikidata is a free and open knowledge base which<br>can be read and edited by both humans and machines.<br>It acts as a central storage for the structured<br>data of several Wikimedia projects. To improve the<br>process of manually inserting new facts, the Wikidata<br>platform features an association rule-based tool to<br>recommend additional suitable properties. In this work,<br>we introduce a novel approach to provide such<br>recommendations based on frequentist inference. We introduce a<br>trie-based method that can efficiently learn and<br>represent property set probabilities in RDF graphs. We<br>extend the method by adding type information to<br>improve recommendation precision...","title_summary":" SchemaTree: Maximum-Likelihood Property<br>Recommendation for Wikidata","x":-33.0209693909,"y":35.3407440186,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.0209693909,"tsne_y":35.3407440186,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"33icoa9o","source_x":"PMC","title":"A Simple Method for Inducing Class Taxonomies in Knowledge Graphs","doi":"10.1007\/978-3-030-49461-2_4","abstract":"The rise of knowledge graphs as a medium for storing and organizing large amounts of data has spurred research interest in automated methods for reasoning with and extracting information from this representation of data. One area which seems to receive less attention is that of inducing a class taxonomy from such graphs. Ontologies, which provide the axiomatic foundation on which knowledge graphs are built, are often governed by a set of class subsumption axioms. These class subsumptions form a class taxonomy which hierarchically organizes the type classes present in the knowledge graph. Manually creating and curating these class taxonomies oftentimes requires expert knowledge and is time costly, especially in large-scale knowledge graphs. Thus, methods capable of inducing the class taxonomy from the knowledge graph data automatically are an appealing solution to the problem. In this paper, we propose a simple method for inducing class taxonomies from knowledge graphs that is scalable to large datasets. Our method borrows ideas from tag hierarchy induction methods, relying on class frequencies and co-occurrences, such that it requires no information outside the knowledge graph\u2019s triple representation. We demonstrate the use of our method on three real-world datasets and compare our results with existing tag hierarchy induction methods. We show that our proposed method outperforms existing tag hierarchy induction methods, although both perform well when applied to knowledge graphs.","publish_time":1588809600000,"author_summary":" Pietrasik, Marcin; Reformat, Marek","abstract_summary":" The rise of knowledge graphs as a medium for<br>storing and organizing large amounts of data has<br>spurred research interest in automated methods for<br>reasoning with and extracting information from this<br>representation of data. One area which seems to receive less<br>attention is that of inducing a class taxonomy from such<br>graphs. Ontologies, which provide the axiomatic<br>foundation on which knowledge graphs are built, are often<br>governed by a set of class subsumption axioms. These<br>class subsumptions form a class taxonomy which<br>hierarchically organizes the type classes present in the<br>knowledge graph. Manually creating and curating these<br>class taxonomies oftentimes...","title_summary":" A Simple Method for Inducing Class Taxonomies<br>in Knowledge Graphs","x":-33.425907135,"y":35.4918212891,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.425907135,"tsne_y":35.4918212891,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"xvr27wqd","source_x":"PMC","title":"A Journey to BSO: Evaluating Earlier and More Recent Ideas of Mario Bunge as a Foundation for Information Systems and Software Development","doi":"10.1007\/978-3-030-49418-6_24","abstract":"A prominent theoretical foundation for IT analysis, design and development is general ontology - a branch of philosophy which studies what exists in reality. A widely used general ontology is BWW (Bunge-Wand-Weber) \u2013 based on ideas of the philosopher and physicist Mario Bunge, synthesized by Wand and Weber. It is regarded as a major contribution to conceptual modeling, database design, data collection design and information quality, as well as theory of IT. At the same time, the ontology was founded on an early subset of Bunge\u2019s philosophy and Bunge\u2019s ideas have evolved since then. An important question, therefore, is: do the more recent ideas expressed by Bunge call for a new ontology? In this paper we conduct an analysis of research by Bunge aiming at addressing this question. We compare the constructs of BWW with what we call Bunge\u2019s Systemist Ontology (BSO) \u2013 a new ontology based on broader and more recent ideas developed by Bunge. Informed by this comparison we offer suggestions for ontology studies as well as future applications of Bunge in conceptual modeling and other areas of IT.","publish_time":1588636800000,"author_summary":" Lukyanenko, Roman","abstract_summary":" A prominent theoretical foundation for IT<br>analysis, design and development is general ontology - a<br>branch of philosophy which studies what exists in<br>reality. A widely used general ontology is BWW<br>(Bunge-Wand-Weber) \u2013 based on ideas of the philosopher and<br>physicist Mario Bunge, synthesized by Wand and Weber. It<br>is regarded as a major contribution to conceptual<br>modeling, database design, data collection design and<br>information quality, as well as theory of IT. At the same<br>time, the ontology was founded on an early subset of<br>Bunge\u2019s philosophy and Bunge\u2019s ideas have evolved<br>since then. An important question, therefore, is: do...","title_summary":" A Journey to BSO: Evaluating Earlier and More<br>Recent Ideas of Mario Bunge as a Foundation for<br>Information Systems and Software Development","x":-33.9670181274,"y":36.7479705811,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9670181274,"tsne_y":36.7479705811,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"cw10rd07","source_x":"PMC","title":"Towards Automating the Synthesis of Chatbots for Conversational Model Query","doi":"10.1007\/978-3-030-49418-6_17","abstract":"Conversational interfaces (also called chatbots) are being increasingly adopted in various domains such as e-commerce or customer service, as a direct communication channel between companies and end-users. Their advantage is that they can be embedded within social networks, and provide a natural language (NL) interface that enables their use by non-technical users. While there are many emerging platforms for building chatbots, their construction remains a highly technical, challenging task. In this paper, we propose the use of chatbots to facilitate querying domain-specific models. This way, instead of relying on technical query languages (e.g., OCL), models are queried using NL as this can be more suitable for non-technical users. To avoid manual programming, our solution is based on the automatic synthesis of the model query chatbots from a domain meta-model. These chatbots communicate with an EMF-based modelling backend using the Xatkit framework.","publish_time":1588636800000,"author_summary":" P\u00e9rez-Soler, Sara; Daniel, Gwendal; Cabot,<br>Jordi; Guerra, Esther; de Lara, Juan","abstract_summary":" Conversational interfaces (also called<br>chatbots) are being increasingly adopted in various<br>domains such as e-commerce or customer service, as a<br>direct communication channel between companies and<br>end-users. Their advantage is that they can be embedded<br>within social networks, and provide a natural<br>language (NL) interface that enables their use by<br>non-technical users. While there are many emerging platforms<br>for building chatbots, their construction<br>remains a highly technical, challenging task. In this<br>paper, we propose the use of chatbots to facilitate<br>querying domain-specific models. This way, instead of<br>relying on technical query languages (e.g., OCL),<br>models are queried using NL as...","title_summary":" Towards Automating the Synthesis of Chatbots<br>for Conversational Model Query","x":-36.5966758728,"y":31.0452079773,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.5966758728,"tsne_y":31.0452079773,"subcluster":0,"subcluster_description":"Blocks World Dialogue System","shape":"p"},{"cord_uid":"3lxfrsqb","source_x":"PMC","title":"RDF Reasoning on Large Ontologies: A Study on Cultural Heritage and Wikidata","doi":"10.1007\/978-3-030-49161-1_32","abstract":"Large ontologies are available as linked data, and they are used across many domains, but to process them considerable resources are required. RDF provides automation possibilities for semantic interpretation, which can lower the effort. We address the usage of RDF reasoning in large ontologies, and we test approaches for solving reasoning problems, having in mind use cases of low availability of computational resources. In our experiment, we designed and evaluated a method based on a reasoning problem of inferring Schema.org statements from cultural objects described in Wikidata. The method defines two intermediate tasks that reduce the volume of data used during the execution of the RDF reasoner, resulting in an efficient execution taking on average 10.3 \u00b1 7.6 ms per RDF resource. The inferences obtained in the Wikidata test were analysed and found to be correct, and the computational resource requirements for reasoning were significantly reduced. Schema.org inference resulted in at least one rdf:type statement for each cultural resource, but the inference of Schema.org predicates was below expectations. Our experiment on cultural data has shown that Wikidata contains alignment statements to other ontologies used in the cultural domain, which with the application of RDF and OWL reasoning can be used to infer views of Wikidata expressed in cultural domain\u2019s data models.","publish_time":1588723200000,"author_summary":" Freire, Nuno; Proen\u00e7a, Diogo","abstract_summary":" Large ontologies are available as linked data,<br>and they are used across many domains, but to<br>process them considerable resources are required. RDF<br>provides automation possibilities for semantic<br>interpretation, which can lower the effort. We address the usage<br>of RDF reasoning in large ontologies, and we test<br>approaches for solving reasoning problems, having in mind<br>use cases of low availability of computational<br>resources. In our experiment, we designed and evaluated a<br>method based on a reasoning problem of inferring<br>Schema.org statements from cultural objects described in<br>Wikidata. The method defines two intermediate tasks that<br>reduce the volume of data used...","title_summary":" RDF Reasoning on Large Ontologies: A Study on<br>Cultural Heritage and Wikidata","x":-33.7732048035,"y":36.3771820068,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7732048035,"tsne_y":36.3771820068,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"vmetwotq","source_x":"PMC","title":"Improving Movie Recommendation Systems Filtering by Exploiting User-Based Reviews and Movie Synopses","doi":"10.1007\/978-3-030-49190-1_17","abstract":"This paper addresses the subject of Movie Recommendation Systems, focusing on two of the most well-known filtering techniques, Collaborative Filtering and Content-based Filtering. The first approach proposes a supervised probabilistic Bayesian model that forms recommendations based on the previous evaluations of other movies the user has watched. The second approach composes an unsupervised learning technique that forms clusters of users, using the K-Means algorithm, based on their preference of different movie genres, as it is expressed through their ratings. Both of the above approaches are compared to each other as well as to a basic method known as Weighted Sum, which makes predictions based on the cosine similarity and the euclidean distance between users and movies. In addition, Content-based Filtering is implemented through K-Means clustering techniques that focus on identifying the resemblance between movie plots. The first approach clusters movies according to the Tf\/Idf weighting scheme, applying weights to the terms of movie plots. The latter identifies the likeness between movie plots, utilizing the BM25 algorithm. The efficiency of the above methods is calculated through the Accuracy metric.","publish_time":1588550400000,"author_summary":" Iliopoulou, Konstantina; Kanavos, Andreas;<br>Ilias, Aristidis; Makris, Christos; Vonitsanos,<br>Gerasimos","abstract_summary":" This paper addresses the subject of Movie<br>Recommendation Systems, focusing on two of the most well-known<br>filtering techniques, Collaborative Filtering and<br>Content-based Filtering. The first approach proposes a<br>supervised probabilistic Bayesian model that forms<br>recommendations based on the previous evaluations of other<br>movies the user has watched. The second approach<br>composes an unsupervised learning technique that forms<br>clusters of users, using the K-Means algorithm, based on<br>their preference of different movie genres, as it is<br>expressed through their ratings. Both of the above<br>approaches are compared to each other as well as to a basic<br>method known as Weighted Sum,...","title_summary":" Improving Movie Recommendation Systems<br>Filtering by Exploiting User-Based Reviews and Movie<br>Synopses","x":-31.0170402527,"y":34.044128418,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0170402527,"tsne_y":34.044128418,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"snl075km","source_x":"PMC","title":"Opinion Mining of Consumer Reviews Using Deep Neural Networks with Word-Sentiment Associations","doi":"10.1007\/978-3-030-49161-1_35","abstract":"Automated opinion mining of consumer reviews is becoming increasingly important due to the rising influence of reviews on online retail shopping. Existing approaches to automated opinion classification rely either on sentiment lexicons or supervised machine learning. Deep neural networks perform this classification task particularly well by utilizing dense document representation in terms of word embeddings. However, this representation model does not consider the sentiment polarity or sentiment intensity of the words. To overcome this problem, we propose a novel model of deep neural network with word-sentiment associations. This model produces richer document representation that incorporates both word context and word sentiment. Specifically, our model utilizes pre-trained word embeddings and lexicon-based sentiment indicators to provide inputs to a deep feed-forward neural network. To verify the effectiveness of the proposed model, a benchmark dataset of Amazon reviews is used. Our results strongly support integrated document representation, which shows that the proposed model outperforms other existing machine learning approaches to opinion mining of consumer reviews.","publish_time":1588723200000,"author_summary":" Hajek, Petr; Barushka, Aliaksandr; Munk,<br>Michal","abstract_summary":" Automated opinion mining of consumer reviews<br>is becoming increasingly important due to the<br>rising influence of reviews on online retail<br>shopping. Existing approaches to automated opinion<br>classification rely either on sentiment lexicons or<br>supervised machine learning. Deep neural networks<br>perform this classification task particularly well by<br>utilizing dense document representation in terms of word<br>embeddings. However, this representation model does not<br>consider the sentiment polarity or sentiment intensity<br>of the words. To overcome this problem, we propose<br>a novel model of deep neural network with<br>word-sentiment associations. This model produces richer<br>document representation that incorporates both word<br>context and word...","title_summary":" Opinion Mining of Consumer Reviews Using Deep<br>Neural Networks with Word-Sentiment Associations","x":-33.8978614807,"y":30.5382518768,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8978614807,"tsne_y":30.5382518768,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"qiwq0pe5","source_x":"PMC","title":"Sentiment Analysis on Movie Scripts and Reviews: Utilizing Sentiment Scores in Rating Prediction","doi":"10.1007\/978-3-030-49161-1_36","abstract":"In recent years, many models for predicting movie ratings have been proposed, focusing on utilizing movie reviews combined with sentiment analysis tools. In this study, we offer a different approach based on the emotionally analyzed concatenation of movie script and their respective reviews. The rationale behind this model is that if the emotional experience described by the reviewer corresponds with or diverges from the emotions expressed in the movie script, then this correlation will be reflected in the particular rating of the movie. We collected a dataset consisting of 747 movie scripts and 78.000 reviews and recreated many conventional approaches for movie rating prediction, including Vector Semantics and Sentiment Analysis techniques ran with a variety of Machine Learning algorithms, in order to more accurately evaluate the performance of our model and the validity of our hypothesis. The results indicate that our proposed combination of features achieves a notable performance, similar to conventional approaches.","publish_time":1588723200000,"author_summary":" Frangidis, Paschalis; Georgiou,<br>Konstantinos; Papadopoulos, Stefanos","abstract_summary":" In recent years, many models for predicting<br>movie ratings have been proposed, focusing on<br>utilizing movie reviews combined with sentiment<br>analysis tools. In this study, we offer a different<br>approach based on the emotionally analyzed<br>concatenation of movie script and their respective reviews.<br>The rationale behind this model is that if the<br>emotional experience described by the reviewer<br>corresponds with or diverges from the emotions expressed in<br>the movie script, then this correlation will be<br>reflected in the particular rating of the movie. We<br>collected a dataset consisting of 747 movie scripts and<br>78.000 reviews and recreated many conventional<br>approaches...","title_summary":" Sentiment Analysis on Movie Scripts and<br>Reviews: Utilizing Sentiment Scores in Rating<br>Prediction","x":-33.9433403015,"y":30.3343925476,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9433403015,"tsne_y":30.3343925476,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"hp2j9386","source_x":"PMC","title":"Promoting Diversity in Content Based Recommendation Using Feature Weighting and LSH","doi":"10.1007\/978-3-030-49161-1_38","abstract":"This work proposes an efficient Content-Based (CB) product recommendation methodology that promotes diversity. A heuristic CB approach incorporating feature weighting and Locality-Sensitive Hashing (LSH) is used, along with the TF-IDF method and functionality of tuning the importance of product features to adjust its logic to the needs of various e-commerce sites. The problem of efficiently producing recommendations, without compromising similarity, is addressed by approximating product similarities via the LSH technique. The methodology is evaluated on two sets with real e-commerce data. The evaluation of the proposed methodology shows that the produced recommendations can help customers to continue browsing a site by providing them with the necessary \u201cnext step\u201d. Finally, it is demonstrated that the methodology incorporates recommendation diversity which can be adjusted by tuning the appropriate feature weights.","publish_time":1588723200000,"author_summary":" Beleveslis, Dimosthenis; Tjortjis, Christos","abstract_summary":" This work proposes an efficient Content-Based<br>(CB) product recommendation methodology that<br>promotes diversity. A heuristic CB approach<br>incorporating feature weighting and Locality-Sensitive<br>Hashing (LSH) is used, along with the TF-IDF method and<br>functionality of tuning the importance of product features to<br>adjust its logic to the needs of various e-commerce<br>sites. The problem of efficiently producing<br>recommendations, without compromising similarity, is<br>addressed by approximating product similarities via the<br>LSH technique. The methodology is evaluated on two<br>sets with real e-commerce data. The evaluation of<br>the proposed methodology shows that the produced<br>recommendations can help customers to continue browsing a site...","title_summary":" Promoting Diversity in Content Based<br>Recommendation Using Feature Weighting and LSH","x":-31.0440502167,"y":34.1275062561,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0440502167,"tsne_y":34.1275062561,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"rb4sr8r4","source_x":"PMC","title":"Automated MeSH Indexing of Biomedical Literature Using Contextualized Word Representations","doi":"10.1007\/978-3-030-49161-1_29","abstract":"Appropriate indexing of resources is necessary for their efficient search, discovery and utilization. Relying solely on manual effort is time-consuming, costly and error prone. On the other hand, the special nature, volume and broadness of biomedical literature pose barriers for automated methods. We argue that current word embedding algorithms can be efficiently used to support the task of biomedical text classification. Both deep- and shallow network approaches are implemented and evaluated. Large datasets of biomedical citations and full texts are harvested for their metadata and used for training and testing. The ontology representation of Medical Subject Headings provides machine-readable labels and specifies the dimensionality of the problem space. These automated approaches are still far from entirely substituting human experts, yet they can be useful as a mechanism for validation and recommendation. Dataset balancing, distributed processing and training parallelization in GPUs, all play an important part regarding the effectiveness and performance of proposed methods.","publish_time":1588723200000,"author_summary":" Koutsomitropoulos, Dimitrios A.;<br>Andriopoulos, Andreas D.","abstract_summary":" Appropriate indexing of resources is<br>necessary for their efficient search, discovery and<br>utilization. Relying solely on manual effort is<br>time-consuming, costly and error prone. On the other hand, the<br>special nature, volume and broadness of biomedical<br>literature pose barriers for automated methods. We argue<br>that current word embedding algorithms can be<br>efficiently used to support the task of biomedical text<br>classification. Both deep- and shallow network approaches are<br>implemented and evaluated. Large datasets of biomedical<br>citations and full texts are harvested for their metadata<br>and used for training and testing. The ontology<br>representation of Medical Subject Headings provides<br>machine-readable labels...","title_summary":" Automated MeSH Indexing of Biomedical<br>Literature Using Contextualized Word Representations","x":-32.5192604065,"y":32.4194717407,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.5192604065,"tsne_y":32.4194717407,"subcluster":6,"subcluster_description":"Pubmed Knowledge Graphbuilding","shape":"p"},{"cord_uid":"e7phn1y9","source_x":"PMC","title":"An Innovative Graph-Based Approach to Advance Feature Selection from Multiple Textual Documents","doi":"10.1007\/978-3-030-49161-1_9","abstract":"This paper introduces a novel graph-based approach to select features from multiple textual documents. The proposed solution enables the investigation of the importance of a term into a whole corpus of documents by utilizing contemporary graph theory methods, such as community detection algorithms and node centrality measures. Compared to well-tried existing solutions, evaluation results show that the proposed approach increases the accuracy of most text classifiers employed and decreases the number of features required to achieve \u2018state-of-the-art\u2019 accuracy. Well-known datasets used for the experimentations reported in this paper include 20Newsgroups, LingSpam, Amazon Reviews and Reuters.","publish_time":1588723200000,"author_summary":" Giarelis, Nikolaos; Kanakaris, Nikos;<br>Karacapilidis, Nikos","abstract_summary":" This paper introduces a novel graph-based<br>approach to select features from multiple textual<br>documents. The proposed solution enables the<br>investigation of the importance of a term into a whole corpus of<br>documents by utilizing contemporary graph theory<br>methods, such as community detection algorithms and<br>node centrality measures. Compared to well-tried<br>existing solutions, evaluation results show that the<br>proposed approach increases the accuracy of most text<br>classifiers employed and decreases the number of features<br>required to achieve \u2018state-of-the-art\u2019 accuracy.<br>Well-known datasets used for the experimentations<br>reported in this paper include 20Newsgroups, LingSpam,<br>Amazon Reviews and Reuters.","title_summary":" An Innovative Graph-Based Approach to Advance<br>Feature Selection from Multiple Textual Documents","x":-32.7556648254,"y":34.7476730347,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.7556648254,"tsne_y":34.7476730347,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"6ttps1nx","source_x":"PMC","title":"Cross-Domain Authorship Attribution Using Pre-trained Language Models","doi":"10.1007\/978-3-030-49161-1_22","abstract":"Authorship attribution attempts to identify the authors behind texts and has important applications mainly in cyber-security, digital humanities and social media analytics. An especially challenging but very realistic scenario is cross-domain attribution where texts of known authorship (training set) differ from texts of disputed authorship (test set) in topic or genre. In this paper, we modify a successful authorship verification approach based on a multi-headed neural network language model and combine it with pre-trained language models. Based on experiments on a controlled corpus covering several text genres where topic and genre is specifically controlled, we demonstrate that the proposed approach achieves very promising results. We also demonstrate the crucial effect of the normalization corpus in cross-domain attribution.","publish_time":1588723200000,"author_summary":" Barlas, Georgios; Stamatatos, Efstathios","abstract_summary":" Authorship attribution attempts to identify<br>the authors behind texts and has important<br>applications mainly in cyber-security, digital humanities<br>and social media analytics. An especially<br>challenging but very realistic scenario is cross-domain<br>attribution where texts of known authorship (training set)<br>differ from texts of disputed authorship (test set) in<br>topic or genre. In this paper, we modify a successful<br>authorship verification approach based on a multi-headed<br>neural network language model and combine it with<br>pre-trained language models. Based on experiments on a<br>controlled corpus covering several text genres where<br>topic and genre is specifically controlled, we<br>demonstrate that the proposed approach...","title_summary":" Cross-Domain Authorship Attribution Using<br>Pre-trained Language Models","x":-33.3060035706,"y":29.9017963409,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3060035706,"tsne_y":29.9017963409,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"7rrq5olm","source_x":"PMC","title":"On the Reusability of Sentiment Analysis Datasets in Applications with Dissimilar Contexts","doi":"10.1007\/978-3-030-49161-1_34","abstract":"The main goal of this paper is to evaluate the usability of several algorithms on various sentiment-labeled datasets. The process of creating good semantic vector representations for textual data is considered a very demanding task for the research community. The first and most important step of a Natural Language Processing (NLP) system, is text preprocessing, which greatly affects the overall accuracy of the classification algorithms. In this work, two vector space models are created, and a study consisting of a variety of algorithms, is performed on them. The work is based on the IMDb dataset which contains movie reviews along with their associated labels (positive or negative). The goal is to obtain the model with the highest accuracy and the best generalization. To measure how well these models generalize in other domains, several datasets, which are further analyzed later, are used.","publish_time":1588723200000,"author_summary":" Sarlis, S.; Maglogiannis, I.","abstract_summary":" The main goal of this paper is to evaluate the<br>usability of several algorithms on various<br>sentiment-labeled datasets. The process of creating good<br>semantic vector representations for textual data is<br>considered a very demanding task for the research<br>community. The first and most important step of a Natural<br>Language Processing (NLP) system, is text<br>preprocessing, which greatly affects the overall accuracy of<br>the classification algorithms. In this work, two<br>vector space models are created, and a study<br>consisting of a variety of algorithms, is performed on<br>them. The work is based on the IMDb dataset which<br>contains movie reviews along...","title_summary":" On the Reusability of Sentiment Analysis<br>Datasets in Applications with Dissimilar Contexts","x":-33.702671051,"y":30.9953460693,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.702671051,"tsne_y":30.9953460693,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"e6v8wm7y","source_x":"PMC","title":"A Deep Learning Approach to Aspect-Based Sentiment Prediction","doi":"10.1007\/978-3-030-49161-1_33","abstract":"Sentiment analysis is a vigorous research area, with many application domains. In this work, aspect-based sentiment prediction is examined as a component of a larger architecture that crawls, indexes and stores documents from a wide variety of online sources, including the most popular social networks. The textual part of the collected information is processed by a hybrid bi-directional long short-term memory architecture, coupled with convolutional layers along with an attention mechanism. The extracted textual features are then combined with other characteristics, such as the number of repetitions, the type and frequency of emoji ideograms in a fully-connected, feed-forward artificial neural network that performs the final prediction task. The obtained results, especially for the negative sentiment class, which is of particular importance in certain cases, are encouraging, underlying the robustness of the proposed approach.","publish_time":1588723200000,"author_summary":" Alexandridis, Georgios; Michalakis,<br>Konstantinos; Aliprantis, John; Polydoras, Pavlos;<br>Tsantilas, Panagiotis; Caridakis, George","abstract_summary":" Sentiment analysis is a vigorous research<br>area, with many application domains. In this work,<br>aspect-based sentiment prediction is examined as a<br>component of a larger architecture that crawls, indexes<br>and stores documents from a wide variety of online<br>sources, including the most popular social networks.<br>The textual part of the collected information is<br>processed by a hybrid bi-directional long short-term<br>memory architecture, coupled with convolutional<br>layers along with an attention mechanism. The<br>extracted textual features are then combined with other<br>characteristics, such as the number of repetitions, the type and<br>frequency of emoji ideograms in a fully-connected,<br>feed-forward artificial neural...","title_summary":" A Deep Learning Approach to Aspect-Based<br>Sentiment Prediction","x":-33.8936691284,"y":30.7465000153,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8936691284,"tsne_y":30.7465000153,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"2pgm3b4t","source_x":"PMC","title":"Manifold Learning for Innovation Funding: Identification of Potential Funding Recipients","doi":"10.1007\/978-3-030-49161-1_11","abstract":"finElink is a recommendation system that provides guidance to French innovative companies with regard to their financing strategy through public funding mechanisms. Analysis of financial data from former funding recipients partially feeds the recommendation system. Financial company data from a representative French population are reduced and projected onto a two-dimensional space with Uniform Manifold Approximation and Projection, a manifold learning algorithm. Former French funding recipients\u2019 data are projected onto the two-dimensional space. Their distribution is non-uniform, with data concentrating in one region of the projection space. This region is identified using Density-based Spatial Clustering of Applications with Noise. Applicant companies which are projected within this region are labeled potential funding recipients and will be suggested the most competitive funding mechanisms.","publish_time":1588723200000,"author_summary":" Grollemund, Vincent; Chat, Ga\u00e9tan Le;<br>Pradat-Peyre, Jean-Fran\u00e7ois; Delbot, Fran\u00e7ois","abstract_summary":" finElink is a recommendation system that<br>provides guidance to French innovative companies with<br>regard to their financing strategy through public<br>funding mechanisms. Analysis of financial data from<br>former funding recipients partially feeds the<br>recommendation system. Financial company data from a<br>representative French population are reduced and projected<br>onto a two-dimensional space with Uniform Manifold<br>Approximation and Projection, a manifold learning<br>algorithm. Former French funding recipients\u2019 data are<br>projected onto the two-dimensional space. Their<br>distribution is non-uniform, with data concentrating in one<br>region of the projection space. This region is<br>identified using Density-based Spatial Clustering of<br>Applications with Noise. Applicant companies...","title_summary":" Manifold Learning for Innovation Funding:<br>Identification of Potential Funding Recipients","x":-30.2702503204,"y":35.0755004883,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.2702503204,"tsne_y":35.0755004883,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"vbmfv1o8","source_x":"PMC","title":"Knowledge-Based Management and Reasoning on Cultural and Natural Touristic Routes","doi":"10.1007\/978-3-030-49161-1_30","abstract":"There is great potential in interdisciplinary traveling platforms mingling knowledge about cultural heritage aspects, such as places with schedules providing visits or even containing augmented reality features also, along with environmental concerns to enhance personalized tourist experience and tripping avocation. For an ontological framework to support and nominate trip detours of targeted interests according to end-users, it should incorporate and unify as much heterogeneous information, deriving either from web sources or wherever there are ubiquitously available such as sensors or open databases. A plethora of qualitatively diverse data along with adequate quantities of them escalate the contingent results in terms of conferring a plurality of relevant options which can be utterly manifested through involving axioms with rule-based reasoning functionalities upon properties considered to be irrelevant to each other at first glance. Thus, managing to import predefined concepts from other ontologies, such as temporality or spatiality, and combine them with new defined concepts to tourist assets, such as points of interest, results in novel meaningful relationships never established before. Apart from the utilization of pre-existent resources and logic towards automatic detouring suggestions, a wide-spectrum modeling enables a suitable problem statement relevant to the e-Tracer framework and comprehension of the issues, providing the opportunity of statistical analysis of knowledge when adequate amounts amassed.","publish_time":1588723200000,"author_summary":" Stathopoulos, Evangelos A.; Kokkalas,<br>Alexandros; Mitsopoulou, Eirini E.; Patenidis,<br>Athanasios T.; Meditskos, Georgios; Diplaris, Sotiris;<br>Paliokas, Ioannis; Vrochidis, Stefanos; Votis,<br>Konstantinos; Tzovaras, Dimitrios; Kompatsiaris, Ioannis","abstract_summary":" There is great potential in interdisciplinary<br>traveling platforms mingling knowledge about cultural<br>heritage aspects, such as places with schedules<br>providing visits or even containing augmented reality<br>features also, along with environmental concerns to<br>enhance personalized tourist experience and tripping<br>avocation. For an ontological framework to support and<br>nominate trip detours of targeted interests according<br>to end-users, it should incorporate and unify as<br>much heterogeneous information, deriving either<br>from web sources or wherever there are ubiquitously<br>available such as sensors or open databases. A plethora of<br>qualitatively diverse data along with adequate quantities of<br>them escalate the contingent results in terms...","title_summary":" Knowledge-Based Management and Reasoning on<br>Cultural and Natural Touristic Routes","x":-33.811668396,"y":36.3201713562,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.811668396,"tsne_y":36.3201713562,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"nsl05lqb","source_x":"PMC","title":"A Sentiment-Based Hotel Review Summarization Using Machine Learning Techniques","doi":"10.1007\/978-3-030-49190-1_14","abstract":"With the advent of social media, there is a data abundance so that analytics can be reliably designed for ultimately providing valuable information towards a given product or service. In this paper, we examine the problem of classifying hotel critiques using views expressed in users\u2019 reviews. There is a massive development of opinions and reviews on the web, which invariably include assessments of products and services, and beliefs about events and persons. In this study, we aim to face the problem of the forever increasing amount of opinionated data that is published in a variety of data sources. The intuition is the extraction of meaningful services despite the lack of sufficient existing architectures. Another important aspect that needs to be taken into consideration when dealing with brand monitoring, relates to the rapid heterogeneous data processing, which is vital to be implemented in real-time in order for the business to react in a more immediate way.","publish_time":1588550400000,"author_summary":" Bompotas, Agorakis; Ilias, Aristidis;<br>Kanavos, Andreas; Makris, Christos; Rompolas,<br>Gerasimos; Savvopoulos, Alkiviadis","abstract_summary":" With the advent of social media, there is a data<br>abundance so that analytics can be reliably designed for<br>ultimately providing valuable information towards a<br>given product or service. In this paper, we examine<br>the problem of classifying hotel critiques using<br>views expressed in users\u2019 reviews. There is a massive<br>development of opinions and reviews on the web, which<br>invariably include assessments of products and services,<br>and beliefs about events and persons. In this<br>study, we aim to face the problem of the forever<br>increasing amount of opinionated data that is published in<br>a variety of data sources. The intuition...","title_summary":" A Sentiment-Based Hotel Review Summarization<br>Using Machine Learning Techniques","x":-33.6348457336,"y":30.810333252,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6348457336,"tsne_y":30.810333252,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"gptqp18l","source_x":"PMC","title":"A Graph-Based Extension for the Set-Based Model Implementing Algorithms Based on Important Nodes","doi":"10.1007\/978-3-030-49190-1_13","abstract":"The purpose of this paper is the expansion of the set-based model, namely an information retrieval model, with the use of graphs. The indexing process implements a graphical representation, while the querying and document representation are based on the classical set-based model. The root of the set-based model corresponds to the use of term sets to complete the querying process based on the terms of the query. However, in the weighting process, this paper presents a wholly different approach elaborating on algorithms that may clearly benefit the process based on the k-core decomposition of each single graph. The main focus will finally be the estimation and presentation of the most important nodes belonging to each graph. These intend to be regarded as keywords presenting the evaluation of their major influence.","publish_time":1588550400000,"author_summary":" Kalogeropoulos, Nikitas-Rigas; Doukas,<br>Ioannis; Makris, Christos; Kanavos, Andreas","abstract_summary":" The purpose of this paper is the expansion of the<br>set-based model, namely an information retrieval model,<br>with the use of graphs. The indexing process<br>implements a graphical representation, while the<br>querying and document representation are based on the<br>classical set-based model. The root of the set-based<br>model corresponds to the use of term sets to complete<br>the querying process based on the terms of the<br>query. However, in the weighting process, this paper<br>presents a wholly different approach elaborating on<br>algorithms that may clearly benefit the process based on<br>the k-core decomposition of each single graph. The<br>main focus...","title_summary":" A Graph-Based Extension for the Set-Based<br>Model Implementing Algorithms Based on Important<br>Nodes","x":-33.3801193237,"y":35.3884506226,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3801193237,"tsne_y":35.3884506226,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"e7umr7yh","source_x":"PMC","title":"On the Learnability of Concepts: With Applications to Comparing Word Embedding Algorithms","doi":"10.1007\/978-3-030-49186-4_35","abstract":"Word Embeddings are used widely in multiple Natural Language Processing (NLP) applications. They are coordinates associated with each word in a dictionary, inferred from statistical properties of these words in a large corpus. In this paper we introduce the notion of \u201cconcept\u201d as a list of words that have shared semantic content. We use this notion to analyse the learnability of certain concepts, defined as the capability of a classifier to recognise unseen members of a concept after training on a random subset of it. We first use this method to measure the learnability of concepts on pretrained word embeddings. We then develop a statistical analysis of concept learnability, based on hypothesis testing and ROC curves, in order to compare the relative merits of various embedding algorithms using a fixed corpora and hyper parameters. We find that all embedding methods capture the semantic content of those word lists, but fastText performs better than the others.","publish_time":1588723200000,"author_summary":" Sutton, Adam; Cristianini, Nello","abstract_summary":" Word Embeddings are used widely in multiple<br>Natural Language Processing (NLP) applications. They<br>are coordinates associated with each word in a<br>dictionary, inferred from statistical properties of these<br>words in a large corpus. In this paper we introduce the<br>notion of \u201cconcept\u201d as a list of words that have shared<br>semantic content. We use this notion to analyse the<br>learnability of certain concepts, defined as the capability<br>of a classifier to recognise unseen members of a<br>concept after training on a random subset of it. We first<br>use this method to measure the learnability of<br>concepts on pretrained word embeddings....","title_summary":" On the Learnability of Concepts: With<br>Applications to Comparing Word Embedding Algorithms","x":-34.2084197998,"y":32.4818077087,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2084197998,"tsne_y":32.4818077087,"subcluster":19,"subcluster_description":"Select Important Context Words","shape":"p"},{"cord_uid":"85zfwdxt","source_x":"PMC","title":"Applying an Intelligent Personal Agent on a Smart Home Using a Novel Dialogue Generator","doi":"10.1007\/978-3-030-49186-4_32","abstract":"Nowadays, Intelligent Personal Agents include Natural Language Understanding (NLU) modules, that utilize Machine Learning (ML), which can be included in different kind of applications in order to enable the translation of users\u2019 input into different kinds of actions, as well as ML modules that handle dialogue. This translation is attained by the matching of a user\u2019s sentence with an intent contained in an Agent. This paper introduces the first generation of the CERTH Intelligent Personal Agent (CIPA) which is based on the RASA (https:\/\/rasa.com\/) framework and utilizes two machine learning models for NLU and dialogue flow classification. Besides the architecture of CIPA\u2014Generation A, a novel dialogue-story generator that is based on the idea of adjacency pairs is introduced. By utilizing on this novel-generator, the agent is able to create all the possible dialog trees in order to handle conversations without training on existing data in contrast with the majority of the current alternative solutions. CIPA supports multiple intents and it is capable of classifying complex sentences consisting of two user\u2019s intents into two automatic operations from the part of the agent. The introduced CIPA\u2014Generation A has been deployed and tested in a real-world scenario at Centre\u2019s of Research & Technology Hellas (CERTH) nZEB Smart Home (https:\/\/smarthome.iti.gr\/) in two different domains, energy and health domain.","publish_time":1588723200000,"author_summary":" Alexiadis, Anastasios; Nizamis, Alexandros;<br>Koskinas, Ioannis; Ioannidis, Dimosthenis; Votis,<br>Konstantinos; Tzovaras, Dimitrios","abstract_summary":" Nowadays, Intelligent Personal Agents<br>include Natural Language Understanding (NLU)<br>modules, that utilize Machine Learning (ML), which can<br>be included in different kind of applications in<br>order to enable the translation of users\u2019 input into<br>different kinds of actions, as well as ML modules that<br>handle dialogue. This translation is attained by the<br>matching of a user\u2019s sentence with an intent contained in<br>an Agent. This paper introduces the first<br>generation of the CERTH Intelligent Personal Agent (CIPA)<br>which is based on the RASA (https:\/\/rasa.com\/)<br>framework and utilizes two machine learning models for<br>NLU and dialogue flow classification. Besides the<br>architecture...","title_summary":" Applying an Intelligent Personal Agent on a<br>Smart Home Using a Novel Dialogue Generator","x":-36.6674156189,"y":31.0949211121,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.6674156189,"tsne_y":31.0949211121,"subcluster":0,"subcluster_description":"Blocks World Dialogue System","shape":"p"},{"cord_uid":"126gukag","source_x":"PMC","title":"Greek Lyrics Generation","doi":"10.1007\/978-3-030-49186-4_37","abstract":"This paper documents the efforts in implementing lyric generation machine learning models in the Greek language for the genre of \u00c9ntekhno music. To accomplish this, we used three different Long Short-Term Memory Recurrent Neural Network approaches. The first method utilizes word-level bi-directional network models, the second method expands on the first by learning the word embeddings on the initial layer of the network, while the last method is based on a char-level network model. Our experimental procedure, which utilized a high sample of human judges, shows that texts of lyrics generated by our models are of high quality and are not that easily distinguishable from actual lyrics.","publish_time":1588723200000,"author_summary":" Lampridis, Orestis; Kefalas, Athanasios;<br>Tzallas, Petros","abstract_summary":" This paper documents the efforts in<br>implementing lyric generation machine learning models in<br>the Greek language for the genre of \u00c9ntekhno music.<br>To accomplish this, we used three different Long<br>Short-Term Memory Recurrent Neural Network approaches.<br>The first method utilizes word-level<br>bi-directional network models, the second method expands on<br>the first by learning the word embeddings on the<br>initial layer of the network, while the last method is<br>based on a char-level network model. Our<br>experimental procedure, which utilized a high sample of<br>human judges, shows that texts of lyrics generated by<br>our models are of high quality and are...","title_summary":" Greek Lyrics Generation","x":-35.1644477844,"y":31.2954006195,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.1644477844,"tsne_y":31.2954006195,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"9rkjafl1","source_x":"PMC","title":"Innovative Deep Neural Network Fusion for Pairwise Translation Evaluation","doi":"10.1007\/978-3-030-49186-4_7","abstract":"A language independent deep learning (DL) architecture for machine translation (MT) evaluation is presented. This DL architecture aims at the best choice between two MT (S1, S2) outputs, based on the reference translation (Sr) and the annotation score. The outputs were generated from a statistical machine translation (SMT) system and a neural machine translation (NMT) system. The model applied in two language pairs: English - Greek (EN-EL) and English - Italian (EN-IT). In this paper, a variety of experiments with different parameter configurations is presented. Moreover, linguistic features, embeddings representation and natural language processing (NLP) metrics (BLEU, METEOR, TER, WER) were tested. The best score was achieved when the proposed model used source segments (SSE) information and the NLP metrics set. Classification accuracy has increased up to 5% (compared to previous related work) and reached quite satisfactory results for the Kendall \u03c4 score.","publish_time":1588723200000,"author_summary":" Mouratidis, Despoina; Kermanidis, Katia<br>Lida; Sosoni, Vilelmini","abstract_summary":" A language independent deep learning (DL)<br>architecture for machine translation (MT) evaluation is<br>presented. This DL architecture aims at the best choice<br>between two MT (S1, S2) outputs, based on the reference<br>translation (Sr) and the annotation score. The outputs were<br>generated from a statistical machine translation (SMT)<br>system and a neural machine translation (NMT) system.<br>The model applied in two language pairs: English -<br>Greek (EN-EL) and English - Italian (EN-IT). In this<br>paper, a variety of experiments with different<br>parameter configurations is presented. Moreover,<br>linguistic features, embeddings representation and<br>natural language processing (NLP) metrics (BLEU,<br>METEOR, TER, WER)...","title_summary":" Innovative Deep Neural Network Fusion for<br>Pairwise Translation Evaluation","x":-34.9219589233,"y":32.1243286133,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.9219589233,"tsne_y":32.1243286133,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"1gja56jb","source_x":"PMC","title":"Exploring NLP and Information Extraction to Jointly Address Question Generation and Answering","doi":"10.1007\/978-3-030-49186-4_33","abstract":"Question Answering (QA) and Question Generation (QG) have been subjects of an intensive study in recent years and much progress has been made in both areas. However, works on combining these two topics mainly focus on how QG can be used to improve QA results. Through existing Natural Language Processing (NLP) techniques, we have implemented a tool that addresses these two topics separately. We further use them jointly in a pipeline. Thus, our goal is to understand how these modules can help each other. For QG, our methodology employs a detailed analysis of the relevant content of a sentence through Part-of-speech (POS) tagging and Named Entity Recognition (NER). Ensuring loose coupling with the QA task, in the latter we use Information Retrieval to rank sentences that might contain relevant information regarding a certain question, together with Open Information Retrieval to analyse the sentences. In its current version, the QG tool takes a sentence to formulate a simple question. By connecting QG with the QA component, we provide a means to effortlessly generate a test set for QA. While our current QA approach shows promising results, when enhancing the QG component we will, in the future, provide questions for which a more elaborated QA will be needed. The generated QA datasets contribute to QA evaluation, while QA proves to be an important technique for assessing the ambiguity of the questions.","publish_time":1588723200000,"author_summary":" Azevedo, Pedro; Leite, Bernardo; Cardoso,<br>Henrique Lopes; Silva, Daniel Castro; Reis, Lu\u00eds Paulo","abstract_summary":" Question Answering (QA) and Question<br>Generation (QG) have been subjects of an intensive study in<br>recent years and much progress has been made in both<br>areas. However, works on combining these two topics<br>mainly focus on how QG can be used to improve QA results.<br>Through existing Natural Language Processing (NLP)<br>techniques, we have implemented a tool that addresses these<br>two topics separately. We further use them jointly<br>in a pipeline. Thus, our goal is to understand how<br>these modules can help each other. For QG, our<br>methodology employs a detailed analysis of the relevant<br>content of a sentence through...","title_summary":" Exploring NLP and Information Extraction to<br>Jointly Address Question Generation and Answering","x":-34.6728858948,"y":32.8823890686,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6728858948,"tsne_y":32.8823890686,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"7vouj8pp","source_x":"PMC","title":"Backward-Forward Sequence Generative Network for Multiple Lexical Constraints","doi":"10.1007\/978-3-030-49186-4_4","abstract":"Advancements in Long Short Term Memory (LSTM) Networks have shown remarkable success in various Natural Language Generation (NLG) tasks. However, generating sequence from pre-specified lexical constraints is a new, challenging and less researched area in NLG. Lexical constraints take the form of words in the language model\u2019s output to create fluent and meaningful sequences. Furthermore, most of the previous approaches cater this problem by allowing the inclusion of pre-specified lexical constraints during the decoding process, which increases the decoding complexity exponentially or linearly with the number of constraints. Moreover, some of the previous approaches can only deal with single constraint. Additionally, most of the previous approaches only deal with single constraints. In this paper, we propose a novel neural probabilistic architecture based on backward-forward language model and word embedding substitution method that can cater multiple lexical constraints for generating quality sequences. Experiments shows that our proposed architecture outperforms previous methods in terms of intrinsic evaluation.","publish_time":1588723200000,"author_summary":" Latif, Seemab; Bashir, Sarmad; Agha, Mir<br>Muntasar Ali; Latif, Rabia","abstract_summary":" Advancements in Long Short Term Memory (LSTM)<br>Networks have shown remarkable success in various<br>Natural Language Generation (NLG) tasks. However,<br>generating sequence from pre-specified lexical<br>constraints is a new, challenging and less researched area<br>in NLG. Lexical constraints take the form of words<br>in the language model\u2019s output to create fluent<br>and meaningful sequences. Furthermore, most of<br>the previous approaches cater this problem by<br>allowing the inclusion of pre-specified lexical<br>constraints during the decoding process, which increases<br>the decoding complexity exponentially or<br>linearly with the number of constraints. Moreover, some<br>of the previous approaches can only deal with<br>single constraint....","title_summary":" Backward-Forward Sequence Generative<br>Network for Multiple Lexical Constraints","x":-34.7000770569,"y":32.3615646362,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.7000770569,"tsne_y":32.3615646362,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"207f964o","source_x":"PMC","title":"Aspect Term Extraction Using Deep Learning Model with Minimal Feature Engineering","doi":"10.1007\/978-3-030-49435-3_12","abstract":"With the explosive growth of social media on the Web, opinion mining has been extensively investigated and consists of the automatic identification and extraction of opinions, emotions, and sentiments from text and multimedia data. One of the tasks involved in opinion mining is Aspect Term Extraction (ATE) which aims at identifying aspects (attributes or characteristics) that have been explicitly evaluated in a sentence or a document. For example, in the sentence \u201cThe picture quality of this camera is amazing\u201d, the aspect term is \u201cpicture quality\u201d. This work proposes POS-AttWD-BLSTM-CRF, a neural network architecture using a deep learning model, and minimal feature engineering, to solve the problem of ATE in opinionated documents. The proposed architecture consists of a BLSTM-CRF classifier that uses the part-of-speech tag (POS tags) as an additional feature, along with a BLSTM encoder with an attention mechanism to allow the incorporation of another relevant feature: the grammatical relations between words. The experiments show that the proposed architecture achieves promising results with minimal feature engineering comparing to the state-of-the-art solutions.","publish_time":1588982400000,"author_summary":" Zschornack Rodrigues Saraiva, Felipe;<br>Linhares Coelho da Silva, Ticiana; Fernandes de Mac\u00eado,<br>Jos\u00e9 Ant\u00f4nio","abstract_summary":" With the explosive growth of social media on the<br>Web, opinion mining has been extensively<br>investigated and consists of the automatic identification<br>and extraction of opinions, emotions, and<br>sentiments from text and multimedia data. One of the tasks<br>involved in opinion mining is Aspect Term Extraction<br>(ATE) which aims at identifying aspects (attributes<br>or characteristics) that have been explicitly<br>evaluated in a sentence or a document. For example, in the<br>sentence \u201cThe picture quality of this camera is<br>amazing\u201d, the aspect term is \u201cpicture quality\u201d. This work<br>proposes POS-AttWD-BLSTM-CRF, a neural network<br>architecture using a deep learning model, and minimal...","title_summary":" Aspect Term Extraction Using Deep Learning<br>Model with Minimal Feature Engineering","x":-33.8809394836,"y":31.8853359222,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8809394836,"tsne_y":31.8853359222,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"zuc6tmv7","source_x":"PMC","title":"Query-Based Metrics for Evaluating and Comparing Document Schemas","doi":"10.1007\/978-3-030-49435-3_33","abstract":"Document stores are frequently used as representation format in many applications. It is often necessary to transform a set of data stored in a relational database (RDB) into a document store. There are several approaches that execute such translation. However, it is difficult to evaluate which target document structure is the most appropriate. In this article, we present a set of query-based metrics for evaluating and comparing documents schemas against a set of existing queries, that represent the application access pattern. We represent the target document schema and the queries as DAGs (Directed Acyclic Graphs), which are used to calculate the metrics. The metrics allow to evaluate if a given target document schema is adequate to answer the queries. We performed a set of experiments to calculate the metrics over a set of documents produced by existing transformation solutions. The metric results are related with smaller coding effort, showing that the metrics are effective to guide the choice of a target NoSQL document structure.","publish_time":1588982400000,"author_summary":" Kuszera, Evandro Miguel; Peres, Let\u00edcia M.;<br>Didonet Del Fabro, Marcos","abstract_summary":" Document stores are frequently used as<br>representation format in many applications. It is often<br>necessary to transform a set of data stored in a relational<br>database (RDB) into a document store. There are several<br>approaches that execute such translation. However, it is<br>difficult to evaluate which target document structure is<br>the most appropriate. In this article, we present a<br>set of query-based metrics for evaluating and<br>comparing documents schemas against a set of existing<br>queries, that represent the application access<br>pattern. We represent the target document schema and the<br>queries as DAGs (Directed Acyclic Graphs), which are<br>used to calculate...","title_summary":" Query-Based Metrics for Evaluating and<br>Comparing Document Schemas","x":-33.8076972961,"y":35.7482299805,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8076972961,"tsne_y":35.7482299805,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"x0ox0fp1","source_x":"PMC","title":"State Machine Based Human-Bot Conversation Model and Services","doi":"10.1007\/978-3-030-49435-3_13","abstract":"Task-oriented virtual assistants (or simply chatbots) are in very high demand these days. They employ third-party APIs to serve end-users via natural language interactions. Chatbots are famed for their easy-to-use interface and gentle learning curve (it only requires one of humans\u2019 most innate ability, the use of natural language). Studies on human conversation patterns show, however, that day-to-day dialogues are of multi-turn and multi-intent nature, which pushes the need for chatbots that are more resilient and flexible to this style of conversations. In this paper, we propose the idea of leveraging Conversational State Machine to make it a core part of chatbots\u2019 conversation engine by formulating conversations as a sequence of states. Here, each state covers an intent and contains a nested state machine to help manage tasks associated to the conversation intent. Such enhanced conversation engine, together with a novel technique to spot implicit information from dialogues (by exploiting Dialog Acts), allows chatbots to manage tangled conversation situations where most existing chatbot technologies fail.","publish_time":1588982400000,"author_summary":" Zamanirad, Shayan; Benatallah, Boualem;<br>Rodriguez, Carlos; Yaghoubzadehfard, Mohammadali;<br>Bouguelia, Sara; Brabra, Hayet","abstract_summary":" Task-oriented virtual assistants (or simply<br>chatbots) are in very high demand these days. They employ<br>third-party APIs to serve end-users via natural language<br>interactions. Chatbots are famed for their easy-to-use<br>interface and gentle learning curve (it only requires one<br>of humans\u2019 most innate ability, the use of natural<br>language). Studies on human conversation patterns show,<br>however, that day-to-day dialogues are of multi-turn<br>and multi-intent nature, which pushes the need for<br>chatbots that are more resilient and flexible to this<br>style of conversations. In this paper, we propose the<br>idea of leveraging Conversational State Machine to<br>make it a core part...","title_summary":" State Machine Based Human-Bot Conversation<br>Model and Services","x":-36.5175170898,"y":31.1750526428,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.5175170898,"tsne_y":31.1750526428,"subcluster":0,"subcluster_description":"Blocks World Dialogue System","shape":"p"},{"cord_uid":"6s3b83av","source_x":"PMC","title":"A Combined Method for Usage of NLP Libraries Towards Analyzing Software Documents","doi":"10.1007\/978-3-030-49435-3_32","abstract":"Natural Language Processing (NLP) library is widely used while analyzing software documents. The numerous toolkits result in a problem on NLP library selection. The selection of NLP library in current work commonly misses some objective reasons, which may pose threats to validity. And it is also not clear that whether the existing guideline on selection still works for the latest versions. In this work, we propose a solution for NLP library selection when the effectiveness is unknown. We use the NLP libraries together in a combined method. Our combined method can utilize the strengths of different NLP libraries to obtain accurate results. The combination is conducted through two steps, i.e., document-level selection of NLP library and sentence-level overwriting. In document-level selection of primary library, the results are obtained from the library that has the highest overall accuracy. Through sentence-level overwriting, the possible fine-gained improvements from other libraries are extracted to overwrite the outputs of primary library. We evaluate the combined method with 4 widely used NLP libraries and 200 documents from 3 different sources. The results show that the combined method can generally outperform all the studied NLP libraries in terms of accuracy. The finding means that our combined method can be used instead of individual NLP library for more effective results.","publish_time":1588982400000,"author_summary":" Cheng, Xinyun; Kong, Xianglong; Liao, Li; Li,<br>Bixin","abstract_summary":" Natural Language Processing (NLP) library is<br>widely used while analyzing software documents. The<br>numerous toolkits result in a problem on NLP library<br>selection. The selection of NLP library in current work<br>commonly misses some objective reasons, which may pose<br>threats to validity. And it is also not clear that<br>whether the existing guideline on selection still<br>works for the latest versions. In this work, we<br>propose a solution for NLP library selection when the<br>effectiveness is unknown. We use the NLP libraries together in<br>a combined method. Our combined method can<br>utilize the strengths of different NLP libraries to<br>obtain...","title_summary":" A Combined Method for Usage of NLP Libraries<br>Towards Analyzing Software Documents","x":-34.1805648804,"y":32.9256706238,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.1805648804,"tsne_y":32.9256706238,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ui2k5d8t","source_x":"PMC","title":"Recommendations for Evolving Relational Databases","doi":"10.1007\/978-3-030-49435-3_31","abstract":"Relational databases play a central role in many information systems. Their schemas contain structural and behavioral entity descriptions. Databases must continuously be adapted to new requirements of a world in constant change while: (1) relational database management systems (RDBMS) do not allow inconsistencies in the schema; (2) stored procedure bodies are not meta-described in RDBMS such as PostgreSQL that consider their bodies as plain text. As a consequence, evaluating the impact of an evolution of the database schema is cumbersome, being essentially manual. We present a semi-automatic approach based on recommendations that can be compiled into a SQL patch fulfilling RDBMS constraints. To support recommendations, we designed a meta-model for relational databases easing computation of change impact. We performed an experiment to validate the approach by reproducing a real evolution on a database. The results of our experiment show that our approach can set the database in the same state as the one produced by the manual evolution in 75% less time.","publish_time":1588982400000,"author_summary":" Delplanque, Julien; Etien, Anne; Anquetil,<br>Nicolas; Ducasse, St\u00e9phane","abstract_summary":" Relational databases play a central role in<br>many information systems. Their schemas contain<br>structural and behavioral entity descriptions.<br>Databases must continuously be adapted to new<br>requirements of a world in constant change while: (1)<br>relational database management systems (RDBMS) do not<br>allow inconsistencies in the schema; (2) stored<br>procedure bodies are not meta-described in RDBMS such as<br>PostgreSQL that consider their bodies as plain text. As a<br>consequence, evaluating the impact of an evolution of the<br>database schema is cumbersome, being essentially<br>manual. We present a semi-automatic approach based on<br>recommendations that can be compiled into a SQL patch fulfilling...","title_summary":" Recommendations for Evolving Relational<br>Databases","x":-33.863899231,"y":36.0875968933,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.863899231,"tsne_y":36.0875968933,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"qf1z1phw","source_x":"PMC","title":"Information Extraction and Graph Representation for the Design of Formulated Products","doi":"10.1007\/978-3-030-49435-3_27","abstract":"Formulated products like cosmetics, personal and household care, and pharmaceutical products are ubiquitous in everyday life. The multi-billion-dollar formulated products industry depends primarily on experiential knowledge for the design of new products. Vast knowledge of formulation ingredients and recipes exists in offline and online resources. Experts often use rudimentary searches over this data to find ingredients and construct recipes. This state of the art leads to considerable time to market and cost. We present an approach for formulated product design that enables extraction, storage, and non-trivial search of details required for product variant generation. Our contributions are threefold. First, we show how various information extraction techniques can be used to extract ingredients and recipe actions from textual sources. Second, we describe how to store this highly connected information as a graph database with an extensible domain model. And third, we demonstrate an aid to experts in putting together a new product based on non-trivial search. In an ongoing proof of concept, we use 410 formulations of various cosmetic creams to demonstrate these capabilities with promising results.","publish_time":1588982400000,"author_summary":" Sunkle, Sagar; Saxena, Krati; Patil, Ashwini;<br>Kulkarni, Vinay; Jain, Deepak; Chacko, Rinu; Rai, Beena","abstract_summary":" Formulated products like cosmetics, personal<br>and household care, and pharmaceutical products<br>are ubiquitous in everyday life. The<br>multi-billion-dollar formulated products industry depends<br>primarily on experiential knowledge for the design of new<br>products. Vast knowledge of formulation ingredients and<br>recipes exists in offline and online resources.<br>Experts often use rudimentary searches over this data<br>to find ingredients and construct recipes. This<br>state of the art leads to considerable time to market<br>and cost. We present an approach for formulated<br>product design that enables extraction, storage, and<br>non-trivial search of details required for product variant<br>generation. Our contributions are threefold. First, we...","title_summary":" Information Extraction and Graph<br>Representation for the Design of Formulated Products","x":-33.1095161438,"y":35.5765762329,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.1095161438,"tsne_y":35.5765762329,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"ymiy7pin","source_x":"PMC","title":"Mining User Opinions to Support Requirement Engineering: An Empirical Study","doi":"10.1007\/978-3-030-49435-3_25","abstract":"App reviews provide a rich source of user opinions that can support requirement engineering activities. Analysing them manually to find these opinions, however, is challenging due to their large quantity and noisy nature. To overcome the problem, automated approaches have been proposed for so-called opinion mining. These approaches facilitate the analysis by extracting features discussed in app reviews and identifying their associated sentiments. The effectiveness of these approaches has been evaluated using different methods and datasets. Unfortunately, replicating these studies to confirm their results and to provide benchmarks of different approaches is a challenging problem. We address the problem by extending previous evaluations and performing a comparison of these approaches. In this paper, we present an empirical study in which, we evaluated feature extraction and sentiment analysis approaches on the same dataset. The results show these approaches achieve lower effectiveness than reported originally, and raise an important question about their practical use.","publish_time":1588982400000,"author_summary":" D\u0105browski, Jacek; Letier, Emmanuel; Perini,<br>Anna; Susi, Angelo","abstract_summary":" App reviews provide a rich source of user<br>opinions that can support requirement engineering<br>activities. Analysing them manually to find these<br>opinions, however, is challenging due to their large<br>quantity and noisy nature. To overcome the problem,<br>automated approaches have been proposed for so-called<br>opinion mining. These approaches facilitate the<br>analysis by extracting features discussed in app<br>reviews and identifying their associated sentiments.<br>The effectiveness of these approaches has been<br>evaluated using different methods and datasets.<br>Unfortunately, replicating these studies to confirm their<br>results and to provide benchmarks of different<br>approaches is a challenging problem. We address the<br>problem by...","title_summary":" Mining User Opinions to Support Requirement<br>Engineering: An Empirical Study","x":-33.5855331421,"y":30.8040142059,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5855331421,"tsne_y":30.8040142059,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"28sn49ls","source_x":"PMC","title":"An Approach for Process Model Extraction by Multi-grained Text Classification","doi":"10.1007\/978-3-030-49435-3_17","abstract":"Process model extraction (PME) is a recently emerged interdiscipline between natural language processing (NLP) and business process management (BPM), which aims to extract process models from textual descriptions. Previous process extractors heavily depend on manual features and ignore the potential relations between clues of different text granularities. In this paper, we formalize the PME task into the multi-grained text classification problem, and propose a hierarchical neural network to effectively model and extract multi-grained information without manually-defined procedural features. Under this structure, we accordingly propose the coarse-to-fine (grained) learning mechanism, training multi-grained tasks in coarse-to-fine grained order to share the high-level knowledge for the low-level tasks. To evaluate our approach, we construct two multi-grained datasets from two different domains and conduct extensive experiments from different dimensions. The experimental results demonstrate that our approach outperforms the state-of-the-art methods with statistical significance and further investigations demonstrate its effectiveness.","publish_time":1588982400000,"author_summary":" Qian, Chen; Wen, Lijie; Kumar, Akhil; Lin,<br>Leilei; Lin, Li; Zong, Zan; Li, Shu\u2019ang; Wang, Jianmin","abstract_summary":" Process model extraction (PME) is a recently<br>emerged interdiscipline between natural language<br>processing (NLP) and business process management (BPM),<br>which aims to extract process models from textual<br>descriptions. Previous process extractors heavily depend on<br>manual features and ignore the potential relations<br>between clues of different text granularities. In this<br>paper, we formalize the PME task into the<br>multi-grained text classification problem, and propose a<br>hierarchical neural network to effectively model and<br>extract multi-grained information without<br>manually-defined procedural features. Under this structure, we<br>accordingly propose the coarse-to-fine (grained)<br>learning mechanism, training multi-grained tasks in<br>coarse-to-fine grained order to share the high-level...","title_summary":" An Approach for Process Model Extraction by<br>Multi-grained Text Classification","x":-33.6318397522,"y":32.4179000854,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6318397522,"tsne_y":32.4179000854,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"i243f6kr","source_x":"PMC","title":"From Eliza to Siri and Beyond","doi":"10.1007\/978-3-030-50146-4_3","abstract":"Since Eliza, the first chatbot ever, developed in the 60s, researchers try to make machines understand (or mimic the understanding) of Natural Language input. Some conversational agents target small talk, while others are more task-oriented. However, from the earliest rule-based systems to the recent data-driven approaches, although many paths were explored with more or less success, we are not there yet. Rule-based systems require much manual work; data-driven systems require a lot of data. Domain adaptation is (again) a current hot-topic. The possibility to add emotions to the conversational agents\u2019 responses, or to make their answers capture their \u201cpersona\u201d, are some popular research topics. This paper explains why the task of Natural Language Understanding is so complicated, detailing the linguistic phenomena that lead to the main challenges. Then, the long walk in this field is surveyed, from the earlier systems to the current trends.","publish_time":1589760000000,"author_summary":" Coheur, Lu\u00edsa","abstract_summary":" Since Eliza, the first chatbot ever, developed<br>in the 60s, researchers try to make machines<br>understand (or mimic the understanding) of Natural<br>Language input. Some conversational agents target<br>small talk, while others are more task-oriented.<br>However, from the earliest rule-based systems to the<br>recent data-driven approaches, although many paths<br>were explored with more or less success, we are not<br>there yet. Rule-based systems require much manual<br>work; data-driven systems require a lot of data.<br>Domain adaptation is (again) a current hot-topic. The<br>possibility to add emotions to the conversational agents\u2019<br>responses, or to make their answers capture their<br>\u201cpersona\u201d, are...","title_summary":" From Eliza to Siri and Beyond","x":-36.3456039429,"y":31.2012691498,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.3456039429,"tsne_y":31.2012691498,"subcluster":0,"subcluster_description":"Blocks World Dialogue System","shape":"p"},{"cord_uid":"9773qanf","source_x":"PMC","title":"Image-Based World-perceiving Knowledge Graph (WpKG) with Imprecision","doi":"10.1007\/978-3-030-50146-4_31","abstract":"Knowledge graphs are a data format that enables the representation of semantics. Most of the available graphs focus on the representation of facts, their features, and relations between them. However, from the point of view of possible applications of semantically rich data formats in intelligent, real-world scenarios, there is a need for knowledge graphs that describe contextual information regarding realistic and casual relations between items in the real world. In this paper, we present a methodology of generating knowledge graphs addressing such a need. We call them World-perceiving Knowledge Graphs \u2013 WpKG. The process of their construction is based on analyzing images. We apply deep learning image processing methods to extract scene graphs. We combine these graphs, and process the obtained graph to determine importance of relations between items detected on the images. The generated WpKG is used as a basis for constructing possibility graphs. We illustrate the process and show some snippets of the generated knowledge and possibility graphs.","publish_time":1589760000000,"author_summary":" Rezaei, Navid; Reformat, Marek Z.; Yager,<br>Ronald R.","abstract_summary":" Knowledge graphs are a data format that enables<br>the representation of semantics. Most of the<br>available graphs focus on the representation of facts,<br>their features, and relations between them.<br>However, from the point of view of possible applications<br>of semantically rich data formats in<br>intelligent, real-world scenarios, there is a need for<br>knowledge graphs that describe contextual information<br>regarding realistic and casual relations between items<br>in the real world. In this paper, we present a<br>methodology of generating knowledge graphs addressing<br>such a need. We call them World-perceiving<br>Knowledge Graphs \u2013 WpKG. The process of their<br>construction is based on...","title_summary":" Image-Based World-perceiving Knowledge<br>Graph (WpKG) with Imprecision","x":-32.141204834,"y":35.4273338318,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.141204834,"tsne_y":35.4273338318,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"ewpm7y7t","source_x":"PMC","title":"Creating Classification Models from Textual Descriptions of Companies Using Crunchbase","doi":"10.1007\/978-3-030-50146-4_51","abstract":"This paper compares different models for multilabel text classification, using information collected from Crunchbase, a large database that holds information about more than 600000 companies. Each company is labeled with one or more categories, from a subset of 46 possible categories, and the proposed models predict the categories based solely on the company textual description. A number of natural language processing strategies have been tested for feature extraction, including stemming, lemmatization, and part-of-speech tags. This is a highly unbalanced dataset, where the frequency of each category ranges from 0.7% to 28%. Our findings reveal that the description text of each company contain features that allow to predict its area of activity, expressed by its corresponding categories, with about 70% precision, and 42% recall. In a second set of experiments, a multiclass problem that attempts to find the most probable category, we obtained about 67% accuracy using SVM and Fuzzy Fingerprints. The resulting models may constitute an important asset for automatic classification of texts, not only consisting of company descriptions, but also other texts, such as web pages, text blogs, news pages, etc.","publish_time":1589760000000,"author_summary":" Felgueiras, Marco; Batista, Fernando;<br>Carvalho, Joao Paulo","abstract_summary":" This paper compares different models for<br>multilabel text classification, using information<br>collected from Crunchbase, a large database that holds<br>information about more than 600000 companies. Each company<br>is labeled with one or more categories, from a<br>subset of 46 possible categories, and the proposed<br>models predict the categories based solely on the<br>company textual description. A number of natural<br>language processing strategies have been tested for<br>feature extraction, including stemming,<br>lemmatization, and part-of-speech tags. This is a highly<br>unbalanced dataset, where the frequency of each category<br>ranges from 0.7% to 28%. Our findings reveal that the<br>description text of each...","title_summary":" Creating Classification Models from Textual<br>Descriptions of Companies Using Crunchbase","x":-33.4832496643,"y":31.7004356384,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4832496643,"tsne_y":31.7004356384,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"jarlpg7v","source_x":"PMC","title":"To BERT or Not to BERT Dealing with Possible BERT Failures in an Entailment Task","doi":"10.1007\/978-3-030-50146-4_54","abstract":"In this paper we focus on an Natural Language Inference task. Being given two sentences, we classify their relation as NEUTRAL, ENTAILMENT or CONTRADICTION. Considering the achievements of BERT (Bidirectional Encoder Representations from Transformers) in many Natural Language Processing tasks, we use BERT features to create our base model for this task. However, several questions arise: can other features improve the performance obtained with BERT? If we are able to predict the situations in which BERT will fail, can we improve the performance by providing alternative models for these situations? We test several strategies and models, as alternatives to the standalone BERT model in the possible failure situations, and we take advantage of semantic features extracted from Discourse Representation Structures.","publish_time":1589760000000,"author_summary":" Fialho, Pedro; Coheur, Lu\u00edsa; Quaresma, Paulo","abstract_summary":" In this paper we focus on an Natural Language<br>Inference task. Being given two sentences, we classify<br>their relation as NEUTRAL, ENTAILMENT or<br>CONTRADICTION. Considering the achievements of BERT<br>(Bidirectional Encoder Representations from Transformers)<br>in many Natural Language Processing tasks, we use<br>BERT features to create our base model for this task.<br>However, several questions arise: can other features<br>improve the performance obtained with BERT? If we are<br>able to predict the situations in which BERT will<br>fail, can we improve the performance by providing<br>alternative models for these situations? We test several<br>strategies and models, as alternatives to the...","title_summary":" To BERT or Not to BERT Dealing with Possible BERT<br>Failures in an Entailment Task","x":-35.4108009338,"y":32.9841384888,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.4108009338,"tsne_y":32.9841384888,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"79y1qlgg","source_x":"PMC","title":"Feature Extraction with TF-IDF and Game-Theoretic Shadowed Sets","doi":"10.1007\/978-3-030-50146-4_53","abstract":"TF-IDF is one of the most commonly used weighting metrics for measuring the relationship of words to documents. It is widely used for word feature extraction. In many research and applications, the thresholds of TF-IDF for selecting relevant words are only based on trial or experiences. Some cut-off strategies have been proposed in which the thresholds are selected based on Zipf\u2019s law or feedbacks from model performances. However, the existing approaches are restricted in specific domains or tasks, and they ignore the imbalance of the number of representative words in different categories of documents. To address these issues, we apply game-theoretic shadowed set model to select the word features given TF-IDF information. Game-theoretic shadowed sets determine the thresholds of TF-IDF using game theory and repetition learning mechanism. Experimental results on real world news category dataset show that our model not only outperforms all baseline cut-off approaches, but also speeds up the classification algorithms.","publish_time":1589760000000,"author_summary":" Zhang, Yan; Zhou, Yue; Yao, JingTao","abstract_summary":" TF-IDF is one of the most commonly used<br>weighting metrics for measuring the relationship of<br>words to documents. It is widely used for word feature<br>extraction. In many research and applications, the<br>thresholds of TF-IDF for selecting relevant words are only<br>based on trial or experiences. Some cut-off<br>strategies have been proposed in which the thresholds are<br>selected based on Zipf\u2019s law or feedbacks from model<br>performances. However, the existing approaches are<br>restricted in specific domains or tasks, and they ignore<br>the imbalance of the number of representative<br>words in different categories of documents. To<br>address these issues, we apply...","title_summary":" Feature Extraction with TF-IDF and<br>Game-Theoretic Shadowed Sets","x":-33.4557113647,"y":32.6818008423,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4557113647,"tsne_y":32.6818008423,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"52upvoiy","source_x":"PMC","title":"Using Topic Information to Improve Non-exact Keyword-Based Search for Mobile Applications","doi":"10.1007\/978-3-030-50146-4_28","abstract":"Considering the wide offer of mobile applications available nowadays, effective search engines are imperative for an user to find applications that provide a specific desired functionality. Retrieval approaches that leverage topic similarity between queries and applications have shown promising results in previous studies. However, the search engines used by most app stores are based on keyword-matching and boosting. In this paper, we explore means to include topic information in such approaches, in order to improve their ability to retrieve relevant applications for non-exact queries, without impairing their computational performance. More specifically, we create topic models specialized on application descriptions and explore how the most relevant terms for each topic covered by an application can be used to complement the information provided by its description. Our experiments show that, although these topic keywords are not able to provide all the information of the topic model, they provide a sufficiently informative summary of the topics covered by the descriptions, leading to improved performance.","publish_time":1589760000000,"author_summary":" Ribeiro, Eug\u00e9nio; Ribeiro, Ricardo; Batista,<br>Fernando; Oliveira, Jo\u00e3o","abstract_summary":" Considering the wide offer of mobile<br>applications available nowadays, effective search engines<br>are imperative for an user to find applications<br>that provide a specific desired functionality.<br>Retrieval approaches that leverage topic similarity<br>between queries and applications have shown promising<br>results in previous studies. However, the search<br>engines used by most app stores are based on<br>keyword-matching and boosting. In this paper, we explore means to<br>include topic information in such approaches, in order<br>to improve their ability to retrieve relevant<br>applications for non-exact queries, without impairing<br>their computational performance. More<br>specifically, we create topic models specialized on<br>application descriptions and...","title_summary":" Using Topic Information to Improve Non-exact<br>Keyword-Based Search for Mobile Applications","x":-33.6535682678,"y":34.2060966492,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6535682678,"tsne_y":34.2060966492,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"mzyxcxh2","source_x":"PMC","title":"Automatic Truecasing of Video Subtitles Using BERT: A Multilingual Adaptable Approach","doi":"10.1007\/978-3-030-50146-4_52","abstract":"This paper describes an approach for automatic capitalization of text without case information, such as spoken transcripts of video subtitles, produced by automatic speech recognition systems. Our approach is based on pre-trained contextualized word embeddings, requires only a small portion of data for training when compared with traditional approaches, and is able to achieve state-of-the-art results. The paper reports experiments both on general written data from the European Parliament, and on video subtitles, revealing that the proposed approach is suitable for performing capitalization, not only in each one of the domains, but also in a cross-domain scenario. We have also created a versatile multilingual model, and the conducted experiments show that good results can be achieved both for monolingual and multilingual data. Finally, we applied domain adaptation by finetuning models, initially trained on general written data, on video subtitles, revealing gains over other approaches not only in performance but also in terms of computational cost.","publish_time":1589760000000,"author_summary":" Rei, Ricardo; Guerreiro, Nuno Miguel;<br>Batista, Fernando","abstract_summary":" This paper describes an approach for automatic<br>capitalization of text without case information, such as<br>spoken transcripts of video subtitles, produced by<br>automatic speech recognition systems. Our approach is<br>based on pre-trained contextualized word<br>embeddings, requires only a small portion of data for<br>training when compared with traditional approaches,<br>and is able to achieve state-of-the-art results.<br>The paper reports experiments both on general<br>written data from the European Parliament, and on video<br>subtitles, revealing that the proposed approach is<br>suitable for performing capitalization, not only in<br>each one of the domains, but also in a cross-domain<br>scenario. We have also...","title_summary":" Automatic Truecasing of Video Subtitles Using<br>BERT: A Multilingual Adaptable Approach","x":-34.972442627,"y":31.9864139557,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.972442627,"tsne_y":31.9864139557,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"038elr0x","source_x":"PMC","title":"Random Steinhaus Distances for Robust Syntax-Based Classification of Partially Inconsistent Linguistic Data","doi":"10.1007\/978-3-030-50153-2_2","abstract":"We use the Steinhaus transform of metric distances to deal with inconsistency in linguistic classification. We focus on data due to G. Longobardi\u2019s school: languages are represented through yes-no strings of length 53, each string position corresponding to a syntactic feature which can be present or absent. However, due to a complex network of logical implications which constrain features, some positions might be undefined (logically inconsistent). To take into account linguistic inconsistency, the distances we use are Steinhaus metric distances generalizing the normalized Hamming distance. To validate the robustness of classifications based on Longobardi\u2019s data we resort to randomized transforms. Experimental results are provided and commented upon.","publish_time":1589587200000,"author_summary":" Franzoi, Laura; Sgarro, Andrea; Dinu, Anca;<br>Dinu, Liviu P.","abstract_summary":" We use the Steinhaus transform of metric<br>distances to deal with inconsistency in linguistic<br>classification. We focus on data due to G. Longobardi\u2019s school:<br>languages are represented through yes-no strings of<br>length 53, each string position corresponding to a<br>syntactic feature which can be present or absent.<br>However, due to a complex network of logical<br>implications which constrain features, some positions<br>might be undefined (logically inconsistent). To<br>take into account linguistic inconsistency, the<br>distances we use are Steinhaus metric distances<br>generalizing the normalized Hamming distance. To validate<br>the robustness of classifications based on<br>Longobardi\u2019s data we resort to randomized transforms....","title_summary":" Random Steinhaus Distances for Robust<br>Syntax-Based Classification of Partially Inconsistent<br>Linguistic Data","x":-35.1957550049,"y":34.0149345398,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.1957550049,"tsne_y":34.0149345398,"subcluster":5,"subcluster_description":"Variantsrandom Steinhaus Distances","shape":"p"},{"cord_uid":"sqhj7bfg","source_x":"PMC","title":"Information Fusion-2-Text: Explainable Aggregation via Linguistic Protoforms","doi":"10.1007\/978-3-030-50153-2_9","abstract":"Recent advancements and applications in artificial intelligence (AI) and machine learning (ML) have highlighted the need for explainable, interpretable, and actionable AI-ML. Most work is focused on explaining deep artificial neural networks, e.g., visual and image captioning. In recent work, we established a set of indices and processes for explainable AI (XAI) relative to information fusion. While informative, the result is information overload and domain expertise is required to understand the results. Herein, we explore the extraction of a reduced set of higher-level linguistic summaries to inform and improve communication with non-fusion experts. Our contribution is a proposed structure of a fusion summary and method to extract this information from a given set of indices. In order to demonstrate the usefulness of the proposed methodology, we provide a case study for using the fuzzy integral to combine a heterogeneous set of deep learners in remote sensing for object detection and land cover classification. This case study shows the potential of our approach to inform users about important trends and anomalies in the models, data and fusion results. This information is critical with respect to transparency, trustworthiness, and identifying limitations of fusion techniques, which may motivate future research and innovation.","publish_time":1589587200000,"author_summary":" Murray, Bryce J.; Anderson, Derek T.; Havens,<br>Timothy C.; Wilkin, Tim; Wilbik, Anna","abstract_summary":" Recent advancements and applications in<br>artificial intelligence (AI) and machine learning (ML)<br>have highlighted the need for explainable,<br>interpretable, and actionable AI-ML. Most work is focused on<br>explaining deep artificial neural networks, e.g., visual<br>and image captioning. In recent work, we<br>established a set of indices and processes for explainable<br>AI (XAI) relative to information fusion. While<br>informative, the result is information overload and domain<br>expertise is required to understand the results. Herein,<br>we explore the extraction of a reduced set of<br>higher-level linguistic summaries to inform and improve<br>communication with non-fusion experts. Our contribution is a<br>proposed structure of...","title_summary":" Information Fusion-2-Text: Explainable<br>Aggregation via Linguistic Protoforms","x":-33.9318351746,"y":31.9932975769,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9318351746,"tsne_y":31.9932975769,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"7w1f5lwy","source_x":"PMC","title":"A Method to Generate Soft Reference Data for Topic Identification","doi":"10.1007\/978-3-030-50153-2_5","abstract":"Text mining and topic identification models are becoming increasingly relevant to extract value from the huge amount of unstructured textual information that companies obtain from their users and clients nowadays. Soft approaches to these problems are also gaining relevance, as in some contexts it may be unrealistic to assume that any document has to be associated to a single topic without any further consideration of the involved uncertainties. However, there is an almost total lack of reference documents allowing a proper assessment of the performance of soft classifiers in such soft topic identification tasks. To address this lack, in this paper a method is proposed that generates topic identification reference documents with a soft but objective nature, and which proceeds by combining, in random but known proportions, phrases of existing documents dealing with different topics. We also provide a computational study illustrating the application of the proposed method on a well-known benchmark for topic identification, as well as showing the possibility of carrying out an informative evaluation of soft classifiers in the context of soft topic identification.","publish_time":1589587200000,"author_summary":" V\u00e9lez, Daniel; Villarino, Guillermo;<br>Rodr\u00edguez, J. Tinguaro; G\u00f3mez, Daniel","abstract_summary":" Text mining and topic identification models<br>are becoming increasingly relevant to extract<br>value from the huge amount of unstructured textual<br>information that companies obtain from their users and<br>clients nowadays. Soft approaches to these problems<br>are also gaining relevance, as in some contexts it<br>may be unrealistic to assume that any document has<br>to be associated to a single topic without any<br>further consideration of the involved uncertainties.<br>However, there is an almost total lack of reference<br>documents allowing a proper assessment of the<br>performance of soft classifiers in such soft topic<br>identification tasks. To address this lack, in this...","title_summary":" A Method to Generate Soft Reference Data for<br>Topic Identification","x":-33.4760322571,"y":31.70561409,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4760322571,"tsne_y":31.70561409,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"ypv2v4un","source_x":"PMC","title":"A semantic network approach to measuring sentiment","doi":"10.1007\/s11135-020-01000-x","abstract":"Sentiment research is dominated by studies that assign texts to positive and negative categories. This classification is often based on a bag-of-words approach that counts the frequencies of sentiment terms from a predefined vocabulary, ignoring the contexts for these words. We test an aspect-based network analysis model that computes sentiment about an entity from the shortest paths between the sentiment words and the target word across a corpus. Two ground-truth datasets in which human annotators judged whether tweets were positive or negative enabled testing the internal and external validity of the automated network-based method, evaluating the extent to which this approach\u2019s scoring corresponds to the annotations. We found that tweets annotated as negative had an automated negativity score that was nearly twice as strong than positivity, while positively annotated tweets were six times stronger in positivity than negativity. To assess the predictive validity of the approach, we analyzed sentiment associated with coronavirus coverage in television news from January 1 to March 25, 2020. Support was found for the four hypotheses tested, demonstrating the utility of the approach. H1: broadcast news expresses less sentiment about coronavirus, panic, and social distancing than non-broadcast news outlets. H2: there is a negative bias in the news across channels. H3: sentiment increases are associated with an increased volume of news stories. H4: sentiment is associated with uncertainty in news coverage of coronavirus over time. We also found that as the type of channel moved from broadcast network news to 24-h business, general, and foreign news sentiment increased for coronavirus, panic, and social distancing.","publish_time":1591920000000,"author_summary":" Danowski, James A.; Yan, Bei; Riopelle, Ken","abstract_summary":" Sentiment research is dominated by studies<br>that assign texts to positive and negative<br>categories. This classification is often based on a<br>bag-of-words approach that counts the frequencies of<br>sentiment terms from a predefined vocabulary, ignoring<br>the contexts for these words. We test an<br>aspect-based network analysis model that computes<br>sentiment about an entity from the shortest paths between<br>the sentiment words and the target word across a<br>corpus. Two ground-truth datasets in which human<br>annotators judged whether tweets were positive or<br>negative enabled testing the internal and external<br>validity of the automated network-based method,<br>evaluating the extent to which this...","title_summary":" A semantic network approach to measuring<br>sentiment","x":-33.7589378357,"y":30.3837070465,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7589378357,"tsne_y":30.3837070465,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"mtw7vxyd","source_x":"PMC","title":"Not All Swear Words Are Used Equal: Attention over Word n-grams for Abusive Language Identification","doi":"10.1007\/978-3-030-49076-8_27","abstract":"The increasing propagation of abusive language in social media is a major concern for supplier companies and governments because of its negative social impact. A large number of methods have been developed for its automatic identification, ranging from dictionary-based methods to sophisticated deep learning approaches. A common problem in all these methods is to distinguish the offensive use of swear words from their everyday and humorous usage. To tackle this particular issue we propose an attention-based neural network architecture that captures the word n-grams importance according to their context. The obtained results in four standard collections from Twitter and Facebook are encouraging, they outperform the [Formula: see text] scores from state-of-the-art methods and allow identifying a set of inherently offensive swear words, and others in which its interpretation depends on its context.","publish_time":1588118400000,"author_summary":" Jarqu\u00edn-V\u00e1squez, Horacio Jes\u00fas;<br>Montes-y-G\u00f3mez, Manuel; Villase\u00f1or-Pineda, Luis","abstract_summary":" The increasing propagation of abusive<br>language in social media is a major concern for supplier<br>companies and governments because of its negative social<br>impact. A large number of methods have been developed<br>for its automatic identification, ranging from<br>dictionary-based methods to sophisticated deep learning<br>approaches. A common problem in all these methods is to<br>distinguish the offensive use of swear words from their<br>everyday and humorous usage. To tackle this particular<br>issue we propose an attention-based neural network<br>architecture that captures the word n-grams importance<br>according to their context. The obtained results in four<br>standard collections from Twitter and Facebook...","title_summary":" Not All Swear Words Are Used Equal: Attention<br>over Word n-grams for Abusive Language<br>Identification","x":-33.5488319397,"y":30.0177783966,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5488319397,"tsne_y":30.0177783966,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"74q3ttjg","source_x":"PMC","title":"Investigating Query Expansion and Coreference Resolution in Question Answering on BERT","doi":"10.1007\/978-3-030-51310-8_5","abstract":"The Bidirectional Encoder Representations from Transformers (BERT) model produces state-of-the-art results in many question answering (QA) datasets, including the Stanford Question Answering Dataset (SQuAD). This paper presents a query expansion (QE) method that identifies good terms from input questions, extracts synonyms for the good terms using a widely-used language resource, WordNet, and selects the most relevant synonyms from the list of extracted synonyms. The paper also introduces a novel QE method that produces many alternative sequences for a given input question using same-language machine translation (MT). Furthermore, we use a coreference resolution (CR) technique to identify anaphors or cataphors in paragraphs and substitute them with the original referents. We found that the QA system with this simple CR technique significantly outperforms the BERT baseline in a QA task. We also found that our best-performing QA system is the one that applies these three preprocessing methods (two QE and CR methods) together to BERT, which produces an excellent [Formula: see text] score (89.8 [Formula: see text] points) in a QA task. Further, we present a comparative analysis on the performances of the BERT QA models taking a variety of criteria into account, and demonstrate our findings in the answer span prediction task.","publish_time":1590451200000,"author_summary":" Bhattacharjee, Santanu; Haque, Rejwanul; de<br>Buy Wenniger, Gideon Maillette; Way, Andy","abstract_summary":" The Bidirectional Encoder Representations<br>from Transformers (BERT) model produces<br>state-of-the-art results in many question answering (QA)<br>datasets, including the Stanford Question Answering<br>Dataset (SQuAD). This paper presents a query expansion<br>(QE) method that identifies good terms from input<br>questions, extracts synonyms for the good terms using a<br>widely-used language resource, WordNet, and selects the<br>most relevant synonyms from the list of extracted<br>synonyms. The paper also introduces a novel QE method that<br>produces many alternative sequences for a given input<br>question using same-language machine translation<br>(MT). Furthermore, we use a coreference resolution<br>(CR) technique to identify anaphors or cataphors in...","title_summary":" Investigating Query Expansion and<br>Coreference Resolution in Question Answering on BERT","x":-34.8535957336,"y":33.0452308655,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.8535957336,"tsne_y":33.0452308655,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"fxlqu8uf","source_x":"PMC","title":"Pattern Learning for Detecting Defect Reports and Improvement Requests in App Reviews","doi":"10.1007\/978-3-030-51310-8_12","abstract":"Online reviews are an important source of feedback for understanding customers. In this study, we follow novel approaches that target the absence of actionable insights by classifying reviews as defect reports and requests for improvement. Unlike traditional classification methods based on expert rules, we reduce the manual labour by employing a supervised system that is capable of learning lexico-semantic patterns through genetic programming. Additionally, we experiment with a distantly-supervised SVM that makes use of the noisy labels generated by patterns. Using a real-world dataset of app reviews, we show that the automatically learned patterns outperform the manually created ones. Also the distantly-supervised SVM models are not far behind the pattern-based solutions, showing the usefulness of this approach when the amount of annotated data is limited.","publish_time":1590451200000,"author_summary":" Mangnoesing, Gino V. H.; Tru\u015fc\u01ce, Maria<br>Mihaela; Frasincar, Flavius","abstract_summary":" Online reviews are an important source of<br>feedback for understanding customers. In this study, we<br>follow novel approaches that target the absence of<br>actionable insights by classifying reviews as defect<br>reports and requests for improvement. Unlike<br>traditional classification methods based on expert rules,<br>we reduce the manual labour by employing a<br>supervised system that is capable of learning<br>lexico-semantic patterns through genetic programming.<br>Additionally, we experiment with a distantly-supervised SVM<br>that makes use of the noisy labels generated by<br>patterns. Using a real-world dataset of app reviews, we<br>show that the automatically learned patterns<br>outperform the manually created ones. Also...","title_summary":" Pattern Learning for Detecting Defect Reports<br>and Improvement Requests in App Reviews","x":-33.3866882324,"y":30.8795394897,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3866882324,"tsne_y":30.8795394897,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"o7ckdng4","source_x":"PMC","title":"Movies Emotional Analysis Using Textual Contents","doi":"10.1007\/978-3-030-51310-8_19","abstract":"In this paper, we use movies and series subtitles and applied text mining and Natural Language Processing methods to evaluate emotions in videos. Three different word lexicons were used and one of the outcomes of this research is the generation of a secondary dataset with more than 3658 records which can be used for other data analysis and data mining research. We used our secondary dataset to find and display correlations between different emotions on the videos and the correlation between emotions on the movies and users\u2019 scores on IMDb using the Pearson correlation method and found some statistically significant correlations.","publish_time":1590451200000,"author_summary":" Kayhani, Amir Kazem; Meziane, Farid; Chiky,<br>Raja","abstract_summary":" In this paper, we use movies and series<br>subtitles and applied text mining and Natural Language<br>Processing methods to evaluate emotions in videos. Three<br>different word lexicons were used and one of the outcomes<br>of this research is the generation of a secondary<br>dataset with more than 3658 records which can be used for<br>other data analysis and data mining research. We used<br>our secondary dataset to find and display<br>correlations between different emotions on the videos and<br>the correlation between emotions on the movies and<br>users\u2019 scores on IMDb using the Pearson correlation<br>method and found some statistically significant...","title_summary":" Movies Emotional Analysis Using Textual<br>Contents","x":-34.4177970886,"y":29.8363761902,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.4177970886,"tsne_y":29.8363761902,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"cspryo9e","source_x":"PMC","title":"A Sentiwordnet Strategy for Curriculum Learning in Sentiment Analysis","doi":"10.1007\/978-3-030-51310-8_16","abstract":"Curriculum Learning (CL) is the idea that learning on a training set sequenced or ordered in a manner where samples range from easy to difficult, results in an increment in performance over otherwise random ordering. The idea parallels cognitive science\u2019s theory of how human brains learn, and that learning a difficult task can be made easier by phrasing it as a sequence of easy to difficult tasks. This idea has gained a lot of traction in machine learning and image processing for a while and recently in Natural Language Processing (NLP). In this paper, we apply the ideas of curriculum learning, driven by SentiWordNet in a sentiment analysis setting. In this setting, given a text segment, our aim is to extract its sentiment or polarity. SentiWordNet is a lexical resource with sentiment polarity annotations. By comparing performance with other curriculum strategies and with no curriculum, the effectiveness of the proposed strategy is presented. Convolutional, Recurrence and Attention based architectures are employed to assess this improvement. The models are evaluated on standard sentiment dataset, Stanford Sentiment Treebank.","publish_time":1590451200000,"author_summary":" Rao, Vijjini Anvesh; Anuranjana, Kaveri;<br>Mamidi, Radhika","abstract_summary":" Curriculum Learning (CL) is the idea that<br>learning on a training set sequenced or ordered in a<br>manner where samples range from easy to difficult,<br>results in an increment in performance over otherwise<br>random ordering. The idea parallels cognitive<br>science\u2019s theory of how human brains learn, and that<br>learning a difficult task can be made easier by phrasing<br>it as a sequence of easy to difficult tasks. This<br>idea has gained a lot of traction in machine learning<br>and image processing for a while and recently in<br>Natural Language Processing (NLP). In this paper, we<br>apply the ideas of curriculum...","title_summary":" A Sentiwordnet Strategy for Curriculum<br>Learning in Sentiment Analysis","x":-34.3721542358,"y":31.1737194061,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.3721542358,"tsne_y":31.1737194061,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"n8iuia82","source_x":"PMC","title":"CONQUEST: A Framework for Building Template-Based IQA Chatbots for Enterprise Knowledge Graphs","doi":"10.1007\/978-3-030-51310-8_6","abstract":"The popularization of Enterprise Knowledge Graphs (EKGs) brings an opportunity to use Question Answering Systems to consult these sources using natural language. We present CONQUEST, a framework that automates much of the process of building chatbots for the Template-Based Interactive Question Answering task on EKGs. The framework automatically handles the processes of construction of the Natural Language Processing engine, construction of the question classification mechanism, definition of the system interaction flow, construction of the EKG query mechanism, and finally, the construction of the user interaction interface. CONQUEST uses a machine learning-based mechanism to classify input questions to known templates extracted from EKGs, utilizing the clarification dialog to resolve inconclusive classifications and request mandatory missing parameters. CONQUEST also evolves with question clarification: these cases define question patterns used as new examples for training.","publish_time":1590451200000,"author_summary":" Avila, Caio Viktor S.; Franco, Wellington;<br>Maia, Jos\u00e9 Gilvan R.; Vidal, Vania M. P.","abstract_summary":" The popularization of Enterprise Knowledge<br>Graphs (EKGs) brings an opportunity to use Question<br>Answering Systems to consult these sources using natural<br>language. We present CONQUEST, a framework that<br>automates much of the process of building chatbots for the<br>Template-Based Interactive Question Answering task on EKGs.<br>The framework automatically handles the<br>processes of construction of the Natural Language<br>Processing engine, construction of the question<br>classification mechanism, definition of the system<br>interaction flow, construction of the EKG query mechanism,<br>and finally, the construction of the user<br>interaction interface. CONQUEST uses a machine<br>learning-based mechanism to classify input questions to known<br>templates extracted...","title_summary":" CONQUEST: A Framework for Building<br>Template-Based IQA Chatbots for Enterprise Knowledge Graphs","x":-35.5077819824,"y":32.91198349,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.5077819824,"tsne_y":32.91198349,"subcluster":12,"subcluster_description":"Assistancefooling Automatic Short Answer","shape":"p"},{"cord_uid":"nu46ok9w","source_x":"PMC","title":"Natural Language Generation Using Transformer Network in an Open-Domain Setting","doi":"10.1007\/978-3-030-51310-8_8","abstract":"Prior works on dialog generation focus on task-oriented setting and utilize multi-turn conversational utterance-response pairs. However, natural language generation (NLG) in the open-domain environment is more challenging. The conversations in an open-domain chit-chat model are mostly single-turn in nature. Current methods used for modeling single-turn conversations often fail to generate contextually relevant responses for a large dataset. In our work, we develop a transformer-based method for natural language generation (NLG) in an open-domain setting. Experiments on the utterance-response pairs show improvement over the baselines, both in terms of quantitative measures like BLEU and ROUGE and human evaluation metrics like fluency and adequacy.","publish_time":1590451200000,"author_summary":" Varshney, Deeksha; Ekbal, Asif; Nagaraja,<br>Ganesh Prasad; Tiwari, Mrigank; Gopinath, Abhijith<br>Athreya Mysore; Bhattacharyya, Pushpak","abstract_summary":" Prior works on dialog generation focus on<br>task-oriented setting and utilize multi-turn<br>conversational utterance-response pairs. However, natural<br>language generation (NLG) in the open-domain<br>environment is more challenging. The conversations in an<br>open-domain chit-chat model are mostly single-turn in<br>nature. Current methods used for modeling single-turn<br>conversations often fail to generate contextually relevant<br>responses for a large dataset. In our work, we develop a<br>transformer-based method for natural language generation (NLG)<br>in an open-domain setting. Experiments on the<br>utterance-response pairs show improvement over the baselines,<br>both in terms of quantitative measures like BLEU and<br>ROUGE and human evaluation metrics like fluency...","title_summary":" Natural Language Generation Using<br>Transformer Network in an Open-Domain Setting","x":-35.7280387878,"y":31.7809257507,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.7280387878,"tsne_y":31.7809257507,"subcluster":13,"subcluster_description":"Multi-Turn Response Selection Models","shape":"p"},{"cord_uid":"y6dgto5a","source_x":"PMC","title":"Enabling Interactive Answering of Procedural Questions","doi":"10.1007\/978-3-030-51310-8_7","abstract":"A mechanism to enable task oriented procedural question answering system for user assistance in English is presented in this paper. The primary aim is to create an answering \u201ccorpus\u201d in a tree-form from unstructured document passages. This corpus is used to respond to the queries interactively to assist in completing a technical task. Reference manuals, documents or webpages are scraped to identify the sections depicting a \u201cprocedure\u201d through machine learning techniques and then an integrated task tree with extracted procedural knowledge from text is generated. The automated mechanism breaks the procedural sections into steps, the appropriate \u201cdecision points\u201d are identified, the interactive utterances are generated to gain user inputs and the alternative paths are created to complete the tree. Conventional tree traversal mechanism provides step by step guidance to complete a task. Efficacy of the proposed mechanism is tested on documents collected from three different domains and test results are presented.","publish_time":1590451200000,"author_summary":" Maitra, Anutosh; Garg, Shivam; Sengupta,<br>Shubhashis","abstract_summary":" A mechanism to enable task oriented procedural<br>question answering system for user assistance in<br>English is presented in this paper. The primary aim is to<br>create an answering \u201ccorpus\u201d in a tree-form from<br>unstructured document passages. This corpus is used to<br>respond to the queries interactively to assist in<br>completing a technical task. Reference manuals,<br>documents or webpages are scraped to identify the<br>sections depicting a \u201cprocedure\u201d through machine<br>learning techniques and then an integrated task tree<br>with extracted procedural knowledge from text is<br>generated. The automated mechanism breaks the procedural<br>sections into steps, the appropriate \u201cdecision points\u201d<br>are identified,...","title_summary":" Enabling Interactive Answering of Procedural<br>Questions","x":-35.8252944946,"y":32.7262687683,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.8252944946,"tsne_y":32.7262687683,"subcluster":12,"subcluster_description":"Assistancefooling Automatic Short Answer","shape":"p"},{"cord_uid":"0um24bym","source_x":"PMC","title":"A Position Aware Decay Weighted Network for Aspect Based Sentiment Analysis","doi":"10.1007\/978-3-030-51310-8_14","abstract":"Aspect Based Sentiment Analysis (ABSA) is the task of identifying sentiment polarity of a text given another text segment or aspect. In ABSA, a text can have multiple sentiments depending upon each aspect. Aspect Term Sentiment Analysis (ATSA) is a subtask of ABSA, in which aspect terms are contained within the given sentence. Most of the existing approaches proposed for ATSA, incorporate aspect information through a different subnetwork thereby overlooking the advantage of aspect terms\u2019 presence within the sentence. In this paper, we propose a model that leverages the positional information of the aspect. The proposed model introduces a decay mechanism based on position. This decay function mandates the contribution of input words for ABSA. The contribution of a word declines as farther it is positioned from the aspect terms in the sentence. The performance is measured on two standard datasets from SemEval 2014 Task 4. In comparison with recent architectures, the effectiveness of the proposed model is demonstrated.","publish_time":1590451200000,"author_summary":" Madasu, Avinash; Rao, Vijjini Anvesh","abstract_summary":" Aspect Based Sentiment Analysis (ABSA) is the<br>task of identifying sentiment polarity of a text<br>given another text segment or aspect. In ABSA, a text<br>can have multiple sentiments depending upon each<br>aspect. Aspect Term Sentiment Analysis (ATSA) is a<br>subtask of ABSA, in which aspect terms are contained<br>within the given sentence. Most of the existing<br>approaches proposed for ATSA, incorporate aspect<br>information through a different subnetwork thereby<br>overlooking the advantage of aspect terms\u2019 presence within<br>the sentence. In this paper, we propose a model that<br>leverages the positional information of the aspect. The<br>proposed model introduces a decay...","title_summary":" A Position Aware Decay Weighted Network for<br>Aspect Based Sentiment Analysis","x":-33.9099578857,"y":30.7176074982,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9099578857,"tsne_y":30.7176074982,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"rir4urz2","source_x":"PMC","title":"Improving Latent Dirichlet Allocation: On Reliability of the Novel Method LDAPrototype","doi":"10.1007\/978-3-030-51310-8_11","abstract":"A large number of applications in text data analysis use the Latent Dirichlet Allocation (LDA) as one of the most popular methods in topic modeling. Although the instability of the LDA is mentioned sometimes, it is usually not considered systematically. Instead, an LDA is often selected from a small set of LDAs using heuristic means or human codings. Then, conclusions are often drawn based on the to some extent arbitrarily selected model. We present the novel method LDAPrototype, which takes the instability of the LDA into account, and show that by systematically selecting an LDA it improves the reliability of the conclusions drawn from the result and thus provides better reproducibility. The improvement coming from this selection criterion is unveiled by applying the proposed methods to an example corpus consisting of texts published in a German quality newspaper over one month.","publish_time":1590451200000,"author_summary":" Rieger, Jonas; Rahnenf\u00fchrer, J\u00f6rg; Jentsch,<br>Carsten","abstract_summary":" A large number of applications in text data<br>analysis use the Latent Dirichlet Allocation (LDA) as<br>one of the most popular methods in topic modeling.<br>Although the instability of the LDA is mentioned<br>sometimes, it is usually not considered systematically.<br>Instead, an LDA is often selected from a small set of LDAs<br>using heuristic means or human codings. Then,<br>conclusions are often drawn based on the to some extent<br>arbitrarily selected model. We present the novel method<br>LDAPrototype, which takes the instability of the LDA into<br>account, and show that by systematically selecting an<br>LDA it improves the reliability of...","title_summary":" Improving Latent Dirichlet Allocation: On<br>Reliability of the Novel Method LDAPrototype","x":-33.56508255,"y":31.4374160767,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.56508255,"tsne_y":31.4374160767,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"7dhfzb9w","source_x":"PMC","title":"Improving Named Entity Recognition for Biomedical and Patent Data Using Bi-LSTM Deep Neural Network Models","doi":"10.1007\/978-3-030-51310-8_3","abstract":"The daily exponential increase of biomedical information in scientific literature and patents is a main obstacle to foster advances in biomedical research. A fundamental step hereby is to find key information (named entities) inside these publications applying Biomedical Named Entities Recognition (BNER). However, BNER is a complex task compared to traditional NER as biomedical named entities often have irregular expressions, employ complex entity structures, and don\u2019t consider well-defined entity boundaries, etc. In this paper, we propose a deep neural network (NN) architecture, namely the bidirectional Long-Short Term Memory (Bi-LSTM) based model for BNER. We present a detailed neural network architecture showing the different NN layers, their interconnections and transformations. Based on existing gold standard datasets, we evaluated and compared several models for identifying biomedical named entities such as chemicals, diseases, drugs, species and genes\/proteins. Our deep NN based Bi-LSTM model using word and character level embeddings outperforms CRF and Bi-LSTM using only word level embeddings significantly.","publish_time":1590451200000,"author_summary":" Saad, Farag; Aras, Hidir; Hackl-Sommer, Ren\u00e9","abstract_summary":" The daily exponential increase of biomedical<br>information in scientific literature and patents is a main<br>obstacle to foster advances in biomedical research. A<br>fundamental step hereby is to find key information (named<br>entities) inside these publications applying<br>Biomedical Named Entities Recognition (BNER). However,<br>BNER is a complex task compared to traditional NER as<br>biomedical named entities often have irregular<br>expressions, employ complex entity structures, and don\u2019t<br>consider well-defined entity boundaries, etc. In this<br>paper, we propose a deep neural network (NN)<br>architecture, namely the bidirectional Long-Short Term<br>Memory (Bi-LSTM) based model for BNER. We present a<br>detailed neural network architecture...","title_summary":" Improving Named Entity Recognition for<br>Biomedical and Patent Data Using Bi-LSTM Deep Neural<br>Network Models","x":-31.668806076,"y":31.9101219177,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.668806076,"tsne_y":31.9101219177,"subcluster":7,"subcluster_description":"Clef Ehealth Evaluation Lab","shape":"p"},{"cord_uid":"u59lqlw5","source_x":"PMC","title":"Enhancing Subword Embeddings with Open N-grams","doi":"10.1007\/978-3-030-51310-8_1","abstract":"Using subword n-grams for training word embeddings makes it possible to subsequently compute vectors for rare and misspelled words. However, we argue that the subword vector qualities can be degraded for words which have a high orthographic neighbourhood; a property of words that has been extensively studied in the Psycholinguistic literature. Empirical findings about lexical neighbourhood effects constrain models of human word encoding, which must also be consistent with what we know about neurophysiological mechanisms in the visual word recognition system. We suggest that the constraints learned from humans provide novel insights to subword encoding schemes. This paper shows that vectors trained with subword properties informed by psycholinguistic evidence are superior to those trained with ad hoc n-grams. It is argued that physiological mechanisms for reading are key factors in the observed distribution of written word forms, and should therefore inform our choice of word encoding.","publish_time":1590451200000,"author_summary":" Veres, Csaba; Kapustin, Paul","abstract_summary":" Using subword n-grams for training word<br>embeddings makes it possible to subsequently compute<br>vectors for rare and misspelled words. However, we<br>argue that the subword vector qualities can be<br>degraded for words which have a high orthographic<br>neighbourhood; a property of words that has been extensively<br>studied in the Psycholinguistic literature.<br>Empirical findings about lexical neighbourhood effects<br>constrain models of human word encoding, which must also<br>be consistent with what we know about<br>neurophysiological mechanisms in the visual word recognition<br>system. We suggest that the constraints learned from<br>humans provide novel insights to subword encoding<br>schemes. This paper shows that...","title_summary":" Enhancing Subword Embeddings with Open<br>N-grams","x":-34.6937103271,"y":31.8601570129,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6937103271,"tsne_y":31.8601570129,"subcluster":11,"subcluster_description":"Automatic Arabic Diacritizationenhancing","shape":"p"},{"cord_uid":"zqfxlrq7","source_x":"PMC","title":"Human-in-the-Loop Conversation Agent for Customer Service","doi":"10.1007\/978-3-030-51310-8_25","abstract":"This paper describes a prototype system for partial automation of customer service operations of a mobile telecommunications operator with a human-in-the loop conversational agent. The agent consists of an intent detection system for identifying the types of customer requests that it can handle appropriately, a slot filling information extraction system that integrates with the customer service database for a rule-based treatment of the common scenarios, and a template-based language generation system that builds response candidates that can be approved or amended by customer service operators. The main focus of this paper is on the system architecture and machine learning system structure design, and the observations of a limited pilot study performed to evaluate the proposed system on customer messages in Latvian. We also discuss the business requirements and practical application limitations and their influence on the design of the natural language processing components.","publish_time":1590451200000,"author_summary":" Paikens, P\u0113teris; Znoti\u0146\u0161, Art\u016brs; B\u0101rzdi\u0146\u0161,<br>Guntis","abstract_summary":" This paper describes a prototype system for<br>partial automation of customer service operations of a<br>mobile telecommunications operator with a<br>human-in-the loop conversational agent. The agent consists<br>of an intent detection system for identifying the<br>types of customer requests that it can handle<br>appropriately, a slot filling information extraction system<br>that integrates with the customer service database<br>for a rule-based treatment of the common<br>scenarios, and a template-based language generation<br>system that builds response candidates that can be<br>approved or amended by customer service operators. The<br>main focus of this paper is on the system<br>architecture and machine learning system...","title_summary":" Human-in-the-Loop Conversation Agent for<br>Customer Service","x":-36.8163070679,"y":30.9314308167,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.8163070679,"tsne_y":30.9314308167,"subcluster":0,"subcluster_description":"Blocks World Dialogue System","shape":"p"},{"cord_uid":"us4du8km","source_x":"PMC","title":"A Cooking Knowledge Graph and Benchmark for Question Answering Evaluation in Lifelong Learning Scenarios","doi":"10.1007\/978-3-030-51310-8_9","abstract":"In a long term exploitation environment, a Question Answering (QA) system should maintain or even improve its performance over time, trying to overcome the lacks made evident through the interactions with users. We claim that, in order to make progress in the QA over Knowledge Bases (KBs) research field, we must deal with two problems at the same time: the translation of Natural Language (NL) questions into formal queries, and the detection of missing knowledge that impact the way a question is answered. The research on these two challenges has not been addressed jointly until now, what motivates the main goals of this work: (i) the definition of the problem and (ii) the development of a methodology to create the evaluation resources needed to address this challenge.","publish_time":1590451200000,"author_summary":" Veron, Mathilde; Pe\u00f1as, Anselmo; Echegoyen,<br>Guillermo; Banerjee, Somnath; Ghannay, Sahar; Rosset,<br>Sophie","abstract_summary":" In a long term exploitation environment, a<br>Question Answering (QA) system should maintain or even<br>improve its performance over time, trying to overcome<br>the lacks made evident through the interactions<br>with users. We claim that, in order to make progress<br>in the QA over Knowledge Bases (KBs) research<br>field, we must deal with two problems at the same time:<br>the translation of Natural Language (NL)<br>questions into formal queries, and the detection of<br>missing knowledge that impact the way a question is<br>answered. The research on these two challenges has not<br>been addressed jointly until now, what motivates<br>the main...","title_summary":" A Cooking Knowledge Graph and Benchmark for<br>Question Answering Evaluation in Lifelong Learning<br>Scenarios","x":-35.0970191956,"y":33.0178146362,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.0970191956,"tsne_y":33.0178146362,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"lqgkdchf","source_x":"PMC","title":"Improving the Community Question Retrieval Performance Using Attention-Based Siamese LSTM","doi":"10.1007\/978-3-030-51310-8_23","abstract":"In this paper, we focus on the problem of question retrieval in community Question Answering (cQA) which aims to retrieve from the community archives the previous questions that are semantically equivalent to the new queries. The major challenges in this crucial task are the shortness of the questions as well as the word mismatch problem as users can formulate the same query using different wording. While numerous attempts have been made to address this problem, most existing methods relied on supervised models which significantly depend on large training data sets and manual feature engineering. Such methods are mostly constrained by their specificities that put aside the word order and ignore syntactic and semantic relationships. In this work, we rely on Neural Networks (NNs) which can learn rich dense representations of text data and enable the prediction of the textual similarity between the community questions. We propose a deep learning approach based on a Siamese architecture with LSTM networks, augmented with an attention mechanism. We test different similarity measures to predict the semantic similarity between the community questions. Experiments conducted on real cQA data sets in English and Arabic show that the performance of question retrieval is improved as compared to other competitive methods.","publish_time":1590451200000,"author_summary":" Othman, Nouha; Faiz, Rim; Sma\u00efli, Kamel","abstract_summary":" In this paper, we focus on the problem of<br>question retrieval in community Question Answering<br>(cQA) which aims to retrieve from the community<br>archives the previous questions that are semantically<br>equivalent to the new queries. The major challenges in this<br>crucial task are the shortness of the questions as well<br>as the word mismatch problem as users can<br>formulate the same query using different wording. While<br>numerous attempts have been made to address this<br>problem, most existing methods relied on supervised<br>models which significantly depend on large training<br>data sets and manual feature engineering. Such<br>methods are mostly constrained by...","title_summary":" Improving the Community Question Retrieval<br>Performance Using Attention-Based Siamese LSTM","x":-34.2851982117,"y":32.8876152039,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.2851982117,"tsne_y":32.8876152039,"subcluster":23,"subcluster_description":"Community Question Retrieval","shape":"p"},{"cord_uid":"g844u7xg","source_x":"PMC","title":"An Adaptive Response Matching Network for Ranking Multi-turn Chatbot Responses","doi":"10.1007\/978-3-030-51310-8_22","abstract":"With the increasing popularity of personal assistant systems, it is crucial to build a chatbot that can communicate with humans and assist them to complete different tasks. A fundamental problem that any chatbots need to address is how to rank candidate responses based on previous utterances in a multi-turn conversation. A previous utterance could be either a past input from the user or a past response from the chatbot. Intuitively, a correct response needs to match well with both past responses and past inputs, but in a different way. Moreover, the matching process should depend on not only the content of the utterances but also domain knowledge. Although various models have been proposed for response matching, few of them studied how to adapt the matching mechanism to utterance types and domain knowledge. To address this limitation, this paper proposes an adaptive response matching network (ARM) to better model the matching relationship in multi-turn conversations. Specifically, the ARM model has separate response matching encoders to adapt to different matching patterns required by different utterance types. It also has a knowledge embedding component to inject domain-specific knowledge in the matching process. Experiments over two public data sets show that the proposed ARM model can significantly outperform the state of the art methods with much fewer parameters.","publish_time":1590451200000,"author_summary":" Wang, Disen; Fang, Hui","abstract_summary":" With the increasing popularity of personal<br>assistant systems, it is crucial to build a chatbot that<br>can communicate with humans and assist them to<br>complete different tasks. A fundamental problem that<br>any chatbots need to address is how to rank<br>candidate responses based on previous utterances in a<br>multi-turn conversation. A previous utterance could be<br>either a past input from the user or a past response from<br>the chatbot. Intuitively, a correct response<br>needs to match well with both past responses and past<br>inputs, but in a different way. Moreover, the matching<br>process should depend on not only the content...","title_summary":" An Adaptive Response Matching Network for<br>Ranking Multi-turn Chatbot Responses","x":-35.8636245728,"y":31.7947483063,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.8636245728,"tsne_y":31.7947483063,"subcluster":13,"subcluster_description":"Multi-Turn Response Selection Models","shape":"p"},{"cord_uid":"344h02tp","source_x":"PMC","title":"Combining Character and Word Embeddings for Affect in Arabic Informal Social Media Microblogs","doi":"10.1007\/978-3-030-51310-8_20","abstract":"Word representation models have been successfully applied in many natural language processing tasks, including sentiment analysis. However, these models do not always work effectively in some social media contexts. When considering the use of Arabic in microblogs like Twitter, it is important to note that a variety of different linguistic domains are involved. This is mainly because social media users employ various dialects in their communications. While training word-level models with such informal text can lead to words being captured that have the same meanings, these models cannot capture all words that can be encountered in the real world due to out-of-vocabulary (OOV) words. The inability to identify words is one of the main limitations of this word-level model. In contrast, character-level embeddings can work effectively with this problem through their ability to learn the vectors of character n-grams or parts of words. We take advantage of both character- and word-level models to discover more effective methods to represent Arabic affect words in tweets. We evaluate our embeddings by incorporating them into a supervised learning framework for a range of affect tasks. Our models outperform the state-of-the-art Arabic pre-trained word embeddings in these tasks. Moreover, they offer improved state-of-the-art results for the task of Arabic emotion intensity, outperforming the top-performing systems that employ a combination of deep neural networks and several other features.","publish_time":1590451200000,"author_summary":" Alharbi, Abdullah I.; Lee, Mark","abstract_summary":" Word representation models have been<br>successfully applied in many natural language processing<br>tasks, including sentiment analysis. However, these<br>models do not always work effectively in some social<br>media contexts. When considering the use of Arabic in<br>microblogs like Twitter, it is important to note that a<br>variety of different linguistic domains are involved.<br>This is mainly because social media users employ<br>various dialects in their communications. While<br>training word-level models with such informal text can<br>lead to words being captured that have the same<br>meanings, these models cannot capture all words that can<br>be encountered in the real world due...","title_summary":" Combining Character and Word Embeddings for<br>Affect in Arabic Informal Social Media Microblogs","x":-33.8915061951,"y":30.6922302246,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8915061951,"tsne_y":30.6922302246,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"sl15t0h0","source_x":"PMC","title":"Enhancement of Short Text Clustering by Iterative Classification","doi":"10.1007\/978-3-030-51310-8_10","abstract":"Short text clustering is a challenging task due to the lack of signal contained in short texts. In this work, we propose iterative classification as a method to boost the clustering quality of short texts. The idea is to repeatedly reassign (classify) outliers to clusters until the cluster assignment stabilizes. The classifier used in each iteration is trained using the current set of cluster labels of the non-outliers; the input of the first iteration is the output of an arbitrary clustering algorithm. Thus, our method does not require any human-annotated labels for training. Our experimental results show that the proposed clustering enhancement method not only improves the clustering quality of different baseline clustering methods (e.g., k-means, k-means--, and hierarchical clustering) but also outperforms the state-of-the-art short text clustering methods on several short text datasets by a statistically significant margin.","publish_time":1590451200000,"author_summary":" Rakib, Md Rashadul Hasan; Zeh, Norbert;<br>Jankowska, Magdalena; Milios, Evangelos","abstract_summary":" Short text clustering is a challenging task due<br>to the lack of signal contained in short texts. In<br>this work, we propose iterative classification as a<br>method to boost the clustering quality of short texts.<br>The idea is to repeatedly reassign (classify)<br>outliers to clusters until the cluster assignment<br>stabilizes. The classifier used in each iteration is<br>trained using the current set of cluster labels of the<br>non-outliers; the input of the first iteration is the output of<br>an arbitrary clustering algorithm. Thus, our<br>method does not require any human-annotated labels<br>for training. Our experimental results show that<br>the proposed...","title_summary":" Enhancement of Short Text Clustering by<br>Iterative Classification","x":-33.2247619629,"y":31.4875469208,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.2247619629,"tsne_y":31.4875469208,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ov61xw1h","source_x":"PMC","title":"Studying Attention Models in Sentiment Attitude Extraction Task","doi":"10.1007\/978-3-030-51310-8_15","abstract":"In the sentiment attitude extraction task, the aim is to identify \u00abattitudes\u00bb \u2013 sentiment relations between entities mentioned in text. In this paper, we provide a study on attention-based context encoders in the sentiment attitude extraction task. For this task, we adapt attentive context encoders of two types: (I) feature-based; (II) self-based. Our experiments (https:\/\/github.com\/nicolay-r\/attitude-extraction-with-attention) with a corpus of Russian analytical texts RuSentRel illustrate that the models trained with attentive encoders outperform ones that were trained without them and achieve 1.5\u20135.9% increase by [Formula: see text]. We also provide the analysis of attention weight distributions in dependence on the term type.","publish_time":1590451200000,"author_summary":" Rusnachenko, Nicolay; Loukachevitch,<br>Natalia","abstract_summary":" In the sentiment attitude extraction task, the<br>aim is to identify \u00abattitudes\u00bb \u2013 sentiment<br>relations between entities mentioned in text. In this<br>paper, we provide a study on attention-based context<br>encoders in the sentiment attitude extraction task. For<br>this task, we adapt attentive context encoders of<br>two types: (I) feature-based; (II) self-based.<br>Our experiments<br>(https:\/\/github.com\/nicolay-r\/attitude-extraction-with-attention) with a corpus of Russian analytical texts<br>RuSentRel illustrate that the models trained with<br>attentive encoders outperform ones that were trained<br>without them and achieve 1.5\u20135.9% increase by<br>[Formula: see text]. We also provide the analysis of<br>attention weight distributions in dependence on the term...","title_summary":" Studying Attention Models in Sentiment<br>Attitude Extraction Task","x":-33.9943695068,"y":30.4785900116,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9943695068,"tsne_y":30.4785900116,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"lb60m6id","source_x":"PMC","title":"Jointly Linking Visual and Textual Entity Mentions with Background Knowledge","doi":"10.1007\/978-3-030-51310-8_24","abstract":"\u201cA picture is worth a thousand words\u201d, the adage reads. However, pictures cannot replace words in terms of their ability to efficiently convey clear (mostly) unambiguous and concise knowledge. Images and text, indeed, reveal different and complementary information that, if combined, result in more information than the sum of that contained in the single media. The combination of visual and textual information can be obtained through linking the entities mentioned in the text with those shown in the pictures. To further integrate this with agent background knowledge, an additional step is necessary. That is, either finding the entities in the agent knowledge base that correspond to those mentioned in the text or shown in the picture or, extending the knowledge base with the newly discovered entities. We call this complex task Visual-Textual-Knowledge Entity Linking (VTKEL). In this paper, after providing a precise definition of the VTKEL task, we present a dataset composed of about 30K commented pictures, annotated with visual and textual entities, and linked to the YAGO ontology. Successively, we develop a purely unsupervised algorithm for the solution of the VTKEL tasks. The evaluation on the VTKEL dataset shows promising results.","publish_time":1590451200000,"author_summary":" Dost, Shahi; Serafini, Luciano; Rospocher,<br>Marco; Ballan, Lamberto; Sperduti, Alessandro","abstract_summary":" \u201cA picture is worth a thousand words\u201d, the adage<br>reads. However, pictures cannot replace words in<br>terms of their ability to efficiently convey clear<br>(mostly) unambiguous and concise knowledge. Images and<br>text, indeed, reveal different and complementary<br>information that, if combined, result in more information<br>than the sum of that contained in the single media.<br>The combination of visual and textual information<br>can be obtained through linking the entities<br>mentioned in the text with those shown in the pictures. To<br>further integrate this with agent background<br>knowledge, an additional step is necessary. That is,<br>either finding the entities in the...","title_summary":" Jointly Linking Visual and Textual Entity<br>Mentions with Background Knowledge","x":-33.5022850037,"y":34.1677246094,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5022850037,"tsne_y":34.1677246094,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"k8icch7w","source_x":"PMC","title":"DDNE: Discriminative Distance Metric Learning for Network Embedding","doi":"10.1007\/978-3-030-50371-0_42","abstract":"Network embedding is a method to learn low-dimensional representations of nodes in networks, which aims to capture and preserve network structure. Most of the existing methods learn network embedding based on distributional similarity hypothesis while ignoring adjacency similarity property, which may cause distance bias problem in the network embedding space. To solve this problem, this paper proposes a unified framework to encode distributional similarity and measure adjacency similarity simultaneously, named DDNE. The proposed DDNE trains a siamese neural network which learns a set of non-linear transforms to project the node pairs into the same low-dimensional space based on their first-order proximity. Meanwhile, a distance constraint is used to make the distance between a pair of adjacent nodes smaller than a threshold and that of each non-adjacent nodes larger than the same threshold, which highlight the adjacency similarity. We conduct extensive experiments on four real-world datasets in three social network analysis tasks, including network reconstruction, attribute prediction and recommendation. The experimental results demonstrate the competitive and superior performance of our approach in generating effective network embedding vectors over baselines.","publish_time":1590451200000,"author_summary":" Li, Xiaoxue; Li, Yangxi; Shang, Yanmin; Tong,<br>Lingling; Fang, Fang; Yin, Pengfei; Cheng, Jie; Li, Jing","abstract_summary":" Network embedding is a method to learn<br>low-dimensional representations of nodes in networks, which<br>aims to capture and preserve network structure.<br>Most of the existing methods learn network<br>embedding based on distributional similarity<br>hypothesis while ignoring adjacency similarity<br>property, which may cause distance bias problem in the<br>network embedding space. To solve this problem, this<br>paper proposes a unified framework to encode<br>distributional similarity and measure adjacency similarity<br>simultaneously, named DDNE. The proposed DDNE trains a siamese<br>neural network which learns a set of non-linear<br>transforms to project the node pairs into the same<br>low-dimensional space based on their first-order...","title_summary":" DDNE: Discriminative Distance Metric<br>Learning for Network Embedding","x":-31.1391105652,"y":35.6371231079,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.1391105652,"tsne_y":35.6371231079,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"i3a5jwck","source_x":"PMC","title":"Social Recommendation in Heterogeneous Evolving Relation Network","doi":"10.1007\/978-3-030-50371-0_41","abstract":"The appearance and growth of social networking brings an exponential growth of information. One of the main solutions proposed for this information overload problem are recommender systems, which provide personalized results. Most existing social recommendation approaches consider relation information to improve recommendation performance in the static context. However, relations are likely to evolve over time in the dynamic network. Therefore, temporal information is an essential ingredient to making social recommendation. In this paper, we propose a novel social recommendation model based on evolving relation network, named SoERec. The learned evolving relation network is a heterogeneous information network, where the strength of relation between users is a sum of the influence of all historical events. We incorporate temporally evolving relations into the recommendation algorithm. We empirically evaluate the proposed method on two widely-used datasets. Experimental results show that the proposed model outperforms the state-of-the-art social recommendation methods.","publish_time":1590451200000,"author_summary":" Jiang, Bo; Lu, Zhigang; Liu, Yuling; Li, Ning;<br>Cui, Zelin","abstract_summary":" The appearance and growth of social networking<br>brings an exponential growth of information. One of<br>the main solutions proposed for this information<br>overload problem are recommender systems, which<br>provide personalized results. Most existing social<br>recommendation approaches consider relation information to<br>improve recommendation performance in the static<br>context. However, relations are likely to evolve over<br>time in the dynamic network. Therefore, temporal<br>information is an essential ingredient to making social<br>recommendation. In this paper, we propose a novel social<br>recommendation model based on evolving relation network,<br>named SoERec. The learned evolving relation network<br>is a heterogeneous information network, where<br>the strength...","title_summary":" Social Recommendation in Heterogeneous<br>Evolving Relation Network","x":-30.8170967102,"y":34.3707389832,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8170967102,"tsne_y":34.3707389832,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"kjmtspej","source_x":"PMC","title":"OpenGraphGym: A Parallel Reinforcement Learning Framework for Graph Optimization Problems","doi":"10.1007\/978-3-030-50426-7_33","abstract":"This paper presents an open-source, parallel AI environment (named OpenGraphGym) to facilitate the application of reinforcement learning (RL) algorithms to address combinatorial graph optimization problems. This environment incorporates a basic deep reinforcement learning method, and several graph embeddings to capture graph features, it also allows users to rapidly plug in and test new RL algorithms and graph embeddings for graph optimization problems. This new open-source RL framework is targeted at achieving both high performance and high quality of the computed graph solutions. This RL framework forms the foundation of several ongoing research directions, including 1) benchmark works on different RL algorithms and embedding methods for classic graph problems; 2) advanced parallel strategies for extreme-scale graph computations, as well as 3) performance evaluation on real-world graph solutions.","publish_time":1590364800000,"author_summary":" Zheng, Weijian; Wang, Dali; Song, Fengguang","abstract_summary":" This paper presents an open-source, parallel<br>AI environment (named OpenGraphGym) to<br>facilitate the application of reinforcement learning<br>(RL) algorithms to address combinatorial graph<br>optimization problems. This environment incorporates a<br>basic deep reinforcement learning method, and<br>several graph embeddings to capture graph features, it<br>also allows users to rapidly plug in and test new RL<br>algorithms and graph embeddings for graph optimization<br>problems. This new open-source RL framework is targeted<br>at achieving both high performance and high<br>quality of the computed graph solutions. This RL<br>framework forms the foundation of several ongoing<br>research directions, including 1) benchmark works on<br>different RL...","title_summary":" OpenGraphGym: A Parallel Reinforcement<br>Learning Framework for Graph Optimization Problems","x":-30.6966304779,"y":36.4190673828,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6966304779,"tsne_y":36.4190673828,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"bdkg4co9","source_x":"PMC","title":"SciNER: Extracting Named Entities from Scientific Literature","doi":"10.1007\/978-3-030-50417-5_23","abstract":"The automated extraction of claims from scientific papers via computer is difficult due to the ambiguity and variability inherent in natural language. Even apparently simple tasks, such as isolating reported values for physical quantities (e.g., \u201cthe melting point of X is Y\u201d) can be complicated by such factors as domain-specific conventions about how named entities (the X in the example) are referenced. Although there are domain-specific toolkits that can handle such complications in certain areas, a generalizable, adaptable model for scientific texts is still lacking. As a first step towards automating this process, we present a generalizable neural network model, SciNER, for recognizing scientific entities in free text. Based on bidirectional LSTM networks, our model combines word embeddings, subword embeddings, and external knowledge (from DBpedia) to boost its accuracy. Experiments show that our model outperforms a leading domain-specific extraction toolkit by up to 50%, as measured by F1 score, while also being easily adapted to new domains.","publish_time":1592179200000,"author_summary":" Hong, Zhi; Tchoua, Roselyne; Chard, Kyle;<br>Foster, Ian","abstract_summary":" The automated extraction of claims from<br>scientific papers via computer is difficult due to the<br>ambiguity and variability inherent in natural language.<br>Even apparently simple tasks, such as isolating<br>reported values for physical quantities (e.g., \u201cthe<br>melting point of X is Y\u201d) can be complicated by such<br>factors as domain-specific conventions about how<br>named entities (the X in the example) are referenced.<br>Although there are domain-specific toolkits that can<br>handle such complications in certain areas, a<br>generalizable, adaptable model for scientific texts is still<br>lacking. As a first step towards automating this<br>process, we present a generalizable neural network<br>model,...","title_summary":" SciNER: Extracting Named Entities from<br>Scientific Literature","x":-33.4005088806,"y":32.6269111633,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.4005088806,"tsne_y":32.6269111633,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"by02t547","source_x":"PMC","title":"Joint Entity Linking for Web Tables with Hybrid Semantic Matching","doi":"10.1007\/978-3-030-50417-5_46","abstract":"Hundreds of millions of tables on the World-Wide Web contain a considerable wealth of high-quality relational data, which has already been viewed as an important kind of sources for knowledge extraction. In order to extract the semantics of web tables to produce machine-readable knowledge, one of the critical steps is table entity linking, which maps the mentions in table cells to their referent entities in knowledge bases. In this paper, we propose a novel model JHSTabEL, which converts table entity linking into a sequence decision problem and uses hybrid semantic features to disambiguate the mentions in web tables. This model captures local semantics of the mentions and entities from different semantic aspects, and then makes full use of the information of previously referred entities for the subsequent entity disambiguation. The decisions are made from a global perspective to jointly disambiguate the mentions in the same column. Experimental results show that our proposed model significantly outperforms the state-of-the-art methods.","publish_time":1592179200000,"author_summary":" Xie, Jie; Lu, Yuhai; Cao, Cong; Li, Zhenzhen;<br>Guan, Yangyang; Liu, Yanbing","abstract_summary":" Hundreds of millions of tables on the<br>World-Wide Web contain a considerable wealth of<br>high-quality relational data, which has already been viewed<br>as an important kind of sources for knowledge<br>extraction. In order to extract the semantics of web tables<br>to produce machine-readable knowledge, one of<br>the critical steps is table entity linking, which<br>maps the mentions in table cells to their referent<br>entities in knowledge bases. In this paper, we propose a<br>novel model JHSTabEL, which converts table entity<br>linking into a sequence decision problem and uses<br>hybrid semantic features to disambiguate the<br>mentions in web tables. This model...","title_summary":" Joint Entity Linking for Web Tables with Hybrid<br>Semantic Matching","x":-33.6122589111,"y":34.821182251,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6122589111,"tsne_y":34.821182251,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"td7qhivv","source_x":"PMC","title":"Are n-gram Categories Helpful in Text Classification?","doi":"10.1007\/978-3-030-50417-5_39","abstract":"Character n-grams are widely used in text categorization problems and are the single most successful type of feature in authorship attribution. Their primary advantage is language independence, as they can be applied to a new language with no additional effort. Typed character n-grams reflect information about their content and context. According to previous research, typed character n-grams improve the accuracy of authorship attribution. This paper examines their effectiveness in three domains: authorship attribution, author profiling and sentiment analysis. The problem of a very high number of features is tackled with distributed Apache Spark processing.","publish_time":1592179200000,"author_summary":" Kruczek, Jakub; Kruczek, Paulina; Kuta,<br>Marcin","abstract_summary":" Character n-grams are widely used in text<br>categorization problems and are the single most successful<br>type of feature in authorship attribution. Their<br>primary advantage is language independence, as they<br>can be applied to a new language with no additional<br>effort. Typed character n-grams reflect information<br>about their content and context. According to<br>previous research, typed character n-grams improve the<br>accuracy of authorship attribution. This paper<br>examines their effectiveness in three domains:<br>authorship attribution, author profiling and sentiment<br>analysis. The problem of a very high number of features is<br>tackled with distributed Apache Spark processing.","title_summary":" Are n-gram Categories Helpful in Text<br>Classification?","x":-33.7815628052,"y":30.7716388702,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7815628052,"tsne_y":30.7716388702,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"kkzj7nvc","source_x":"PMC","title":"Time Expressions Identification Without Human-Labeled Corpus for Clinical Text Mining in Russian","doi":"10.1007\/978-3-030-50423-6_44","abstract":"To obtain accurate predictive models in medicine, it is necessary to use complete relevant information about the patient. We propose an approach for extracting temporary expressions from unlabeled natural language texts. This approach can be used for the first analysis of the corpus, for data labeling as the first stage, or for obtaining linguistic constructions that can be used for a rule-based approach to retrieve information. Our method includes the sequential use of several machine learning and natural language processing methods: classification of sentences, the transformation of word bag frequencies, clustering of sentences with time expressions, classification of new data into clusters and construction of sentence profiles using feature importances. With this method, we derive the list of the most frequent time expressions and extract events and\/or time events for 9801 sentences of anamnesis in Russian. The proposed approach is independent of the corpus language and can be used for other tasks, for example, extracting an experiencer of a disease.","publish_time":1590192000000,"author_summary":" Funkner, Anastasia A.; Kovalchuk, Sergey V.","abstract_summary":" To obtain accurate predictive models in<br>medicine, it is necessary to use complete relevant<br>information about the patient. We propose an approach for<br>extracting temporary expressions from unlabeled natural<br>language texts. This approach can be used for the first<br>analysis of the corpus, for data labeling as the first<br>stage, or for obtaining linguistic constructions<br>that can be used for a rule-based approach to<br>retrieve information. Our method includes the<br>sequential use of several machine learning and natural<br>language processing methods: classification of<br>sentences, the transformation of word bag frequencies,<br>clustering of sentences with time expressions,<br>classification of new data...","title_summary":" Time Expressions Identification Without<br>Human-Labeled Corpus for Clinical Text Mining in Russian","x":-32.3520698547,"y":31.8929805756,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.3520698547,"tsne_y":31.8929805756,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"rdpgyhbn","source_x":"PMC","title":"Applicability of Machine Learning Methods to Multi-label Medical Text Classification","doi":"10.1007\/978-3-030-50423-6_38","abstract":"Structuring medical text using international standards allows to improve interoperability and quality of predictive modelling. Medical text classification task facilitates information extraction. In this work we investigate the applicability of several machine learning models and classifier chains (CC) to medical unstructured text classification. The experimental study was performed on a corpus of 11671 manually labeled Russian medical notes. The results showed that using CC strategy allows to improve classification performance. Ensemble of classifier chains based on linear SVC showed the best result: 0.924 micro F-measure, 0.872 micro precision and 0.927 micro recall.","publish_time":1590192000000,"author_summary":" Lenivtceva, Iuliia; Slasten, Evgenia;<br>Kashina, Mariya; Kopanitsa, Georgy","abstract_summary":" Structuring medical text using international<br>standards allows to improve interoperability and<br>quality of predictive modelling. Medical text<br>classification task facilitates information extraction. In<br>this work we investigate the applicability of<br>several machine learning models and classifier chains<br>(CC) to medical unstructured text classification.<br>The experimental study was performed on a corpus of<br>11671 manually labeled Russian medical notes. The<br>results showed that using CC strategy allows to improve<br>classification performance. Ensemble of classifier chains<br>based on linear SVC showed the best result: 0.924<br>micro F-measure, 0.872 micro precision and 0.927<br>micro recall.","title_summary":" Applicability of Machine Learning Methods to<br>Multi-label Medical Text Classification","x":-31.8864116669,"y":31.86951828,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.8864116669,"tsne_y":31.86951828,"subcluster":8,"subcluster_description":"Multi-Label Medical Text Classificationexperiencer","shape":"p"},{"cord_uid":"76jkrxi1","source_x":"PMC","title":"Experiencer Detection and Automated Extraction of a Family Disease Tree from Medical Texts in Russian Language","doi":"10.1007\/978-3-030-50423-6_45","abstract":"Text descriptions in natural language are an essential part of electronic health records (EHRs). Such descriptions usually contain facts about patient\u2019s life, events, diseases and other relevant information. Sometimes it may also include facts about their family members. In order to find the facts about the right person (experiencer) and convert the unstructured medical text into structured information, we developed a module of experiencer detection. We compared different vector representations and machine learning models to get the highest quality of 0.96 f-score for binary classification and 0.93 f-score for multi-classification. Additionally, we present the results plotting the family disease tree.","publish_time":1590192000000,"author_summary":" Balabaeva, Ksenia; Kovalchuk, Sergey","abstract_summary":" Text descriptions in natural language are an<br>essential part of electronic health records (EHRs). Such<br>descriptions usually contain facts about patient\u2019s life,<br>events, diseases and other relevant information.<br>Sometimes it may also include facts about their family<br>members. In order to find the facts about the right person<br>(experiencer) and convert the unstructured medical text into<br>structured information, we developed a module of<br>experiencer detection. We compared different vector<br>representations and machine learning models to get the highest<br>quality of 0.96 f-score for binary classification and<br>0.93 f-score for multi-classification.<br>Additionally, we present the results plotting the family<br>disease tree.","title_summary":" Experiencer Detection and Automated<br>Extraction of a Family Disease Tree from Medical Texts in<br>Russian Language","x":-31.9982013702,"y":31.8876667023,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.9982013702,"tsne_y":31.8876667023,"subcluster":8,"subcluster_description":"Multi-Label Medical Text Classificationexperiencer","shape":"p"},{"cord_uid":"uwxvfkbp","source_x":"PMC","title":"From Generality to Specificity: On Matter of Scale in Social Media Topic Communities","doi":"10.1007\/978-3-030-50423-6_23","abstract":"Research question stated in current paper concerns measuring significance of interest topic to a person on the base of digital footprints, observed in on-line social media. Interests are represented by on-line social groups in VK social network, which were marked by topics. Topic significance to a person is supposed to be related to the fraction of representative groups in user\u2019s subscription list. We imply that for each topic, depending on its popularity, relation to geographical region, and social acceptability, there is a value of group size which is significant. In addition, we suppose, that professional clusters of groups demonstrate relatively higher inner density and unify common groups. Therefore, following groups from more specific clusters indicate higher personal involvement to a topic \u2013 in this way, representative topical groups are marked. We build social group similarity graph, which is based on the number of common followers, extract subgraphs related to a single topic, and analyse bins of groups, build with increase of group sizes. Results show topics of general interests have higher density at larger groups in contrast to specific interests, which is in correspondence with initial hypothesis.","publish_time":1590192000000,"author_summary":" Vaganov, Danila; Bardina, Mariia; Guleva,<br>Valentina","abstract_summary":" Research question stated in current paper<br>concerns measuring significance of interest topic to a<br>person on the base of digital footprints, observed in<br>on-line social media. Interests are represented by<br>on-line social groups in VK social network, which were<br>marked by topics. Topic significance to a person is<br>supposed to be related to the fraction of representative<br>groups in user\u2019s subscription list. We imply that for<br>each topic, depending on its popularity, relation<br>to geographical region, and social<br>acceptability, there is a value of group size which is<br>significant. In addition, we suppose, that professional<br>clusters of groups demonstrate relatively...","title_summary":" From Generality to Specificity: On Matter of<br>Scale in Social Media Topic Communities","x":-30.3822345734,"y":33.5430526733,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.3822345734,"tsne_y":33.5430526733,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"20plzyqd","source_x":"PMC","title":"Hybrid Text Feature Modeling for Disease Group Prediction Using Unstructured Physician Notes","doi":"10.1007\/978-3-030-50423-6_24","abstract":"Existing Clinical Decision Support Systems (CDSSs) largely depend on the availability of structured patient data and Electronic Health Records (EHRs) to aid caregivers. However, in case of hospitals in developing countries, structured patient data formats are not widely adopted, where medical professionals still rely on clinical notes in the form of unstructured text. Such unstructured clinical notes recorded by medical personnel can also be a potential source of rich patient-specific information which can be leveraged to build CDSSs, even for hospitals in developing countries. If such unstructured clinical text can be used, the manual and time-consuming process of EHR generation will no longer be required, with huge person-hours and cost savings. In this article, we propose a generic ICD9 disease group prediction CDSS built on unstructured physician notes modeled using hybrid word embeddings. These word embeddings are used to train a deep neural network for effectively predicting ICD9 disease groups. Experimental evaluation showed that the proposed approach outperformed the state-of-the-art disease group prediction model built on structured EHRs by 15% in terms of AUROC and 40% in terms of AUPRC, thus proving our hypothesis and eliminating dependency on availability of structured patient data.","publish_time":1590192000000,"author_summary":" Krishnan, Gokul S.; Kamath, S. Sowmya","abstract_summary":" Existing Clinical Decision Support Systems<br>(CDSSs) largely depend on the availability of<br>structured patient data and Electronic Health Records<br>(EHRs) to aid caregivers. However, in case of<br>hospitals in developing countries, structured patient<br>data formats are not widely adopted, where medical<br>professionals still rely on clinical notes in the form of<br>unstructured text. Such unstructured clinical notes<br>recorded by medical personnel can also be a potential<br>source of rich patient-specific information which<br>can be leveraged to build CDSSs, even for hospitals<br>in developing countries. If such unstructured<br>clinical text can be used, the manual and time-consuming<br>process of EHR generation...","title_summary":" Hybrid Text Feature Modeling for Disease Group<br>Prediction Using Unstructured Physician Notes","x":-31.4744777679,"y":31.8972892761,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.4744777679,"tsne_y":31.8972892761,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"echxuw74","source_x":"PMC","title":"Detecting the Most Insightful Parts of Documents Using a Regularized Attention-Based Model","doi":"10.1007\/978-3-030-50420-5_20","abstract":"Every individual text or document is generated for specific purpose(s). Sometime, the text is deployed to convey a specific message about an event or a product. Other occasions, it may be communicating a scientific breakthrough, development or new model and so on. Given any specific objective, the creators and the users of documents may like to know which part(s) of the documents are more influential in conveying their specific messages or achieving their objectives. Understanding which parts of a document has more impact on the viewer\u2019s perception would allow the content creators to design more effective content. Detecting the more impactful parts of a content would help content users, such as advertisers, to concentrate their efforts more on those parts of the content and thus to avoid spending resources on the rest of the document. This work uses a regularized attention-based method to detect the most influential part(s) of any given document or text. The model uses an encoder-decoder architecture based on attention-based decoder with regularization applied to the corresponding weights.","publish_time":1590105600000,"author_summary":" Modarresi, Kourosh","abstract_summary":" Every individual text or document is generated<br>for specific purpose(s). Sometime, the text is<br>deployed to convey a specific message about an event or a<br>product. Other occasions, it may be communicating a<br>scientific breakthrough, development or new model and so<br>on. Given any specific objective, the creators and<br>the users of documents may like to know which<br>part(s) of the documents are more influential in<br>conveying their specific messages or achieving their<br>objectives. Understanding which parts of a document has<br>more impact on the viewer\u2019s perception would allow<br>the content creators to design more effective<br>content. Detecting the more...","title_summary":" Detecting the Most Insightful Parts of<br>Documents Using a Regularized Attention-Based Model","x":-33.7509269714,"y":32.5820846558,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.7509269714,"tsne_y":32.5820846558,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"qd3xej8l","source_x":"PMC","title":"The Concept of System for Automated Scientific Literature Reviews Generation","doi":"10.1007\/978-3-030-50420-5_32","abstract":"We present a concept of system which is aimed to create a literature review of scientific articles having a small sketch of statements as the input. Key elements of the system include transformer-based BERT encoder, deep LSTM decoder and a loss function which combines auto-encoder loss and forces generated summaries to be in the input text domain. We propose to use PMC open access subset for model learning.","publish_time":1590105600000,"author_summary":" Teslyuk, Anton","abstract_summary":" We present a concept of system which is aimed to<br>create a literature review of scientific articles<br>having a small sketch of statements as the input. Key<br>elements of the system include transformer-based BERT<br>encoder, deep LSTM decoder and a loss function which<br>combines auto-encoder loss and forces generated<br>summaries to be in the input text domain. We propose to use<br>PMC open access subset for model learning.","title_summary":" The Concept of System for Automated Scientific<br>Literature Reviews Generation","x":-34.8802986145,"y":32.4056930542,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.8802986145,"tsne_y":32.4056930542,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"1bcqwnfs","source_x":"PMC","title":"Effect of Dataset Size on Efficiency of Collaborative Filtering Recommender Systems with Multi-clustering as a Neighbourhood Identification Strategy","doi":"10.1007\/978-3-030-50420-5_25","abstract":"Determination of accurate neighbourhood of an active user (a user to whom recommendations are generated) is one of the essential problems that collaborative filtering based recommender systems encounter. Properly adjusted neighbourhood leads to more accurate recommendation generated by a recommender system. In classical collaborative filtering technique, the neighbourhood is modelled by kNN algorithm, but this approach has poor scalability. Clustering techniques, although improved time efficiency of recommender systems, can negatively affect the quality (precision or accuracy) of recommendations. This article presents a new approach to collaborative filtering recommender systems that focuses on the problem of an active user\u2019s neighbourhood modelling. Instead of one clustering scheme, it works on a set of partitions, therefore it selects the most appropriate one that models the neighbourhood precisely. This article presents the results of the experiments validating the advantage of multi-clustering approach, [Formula: see text], over the traditional methods based on single-scheme clustering. The experiments particularly focus on the effect of great size of datasets concerning overall recommendation performance including accuracy and coverage.","publish_time":1590105600000,"author_summary":" Ku\u017celewska, Urszula","abstract_summary":" Determination of accurate neighbourhood of an<br>active user (a user to whom recommendations are<br>generated) is one of the essential problems that<br>collaborative filtering based recommender systems<br>encounter. Properly adjusted neighbourhood leads to more<br>accurate recommendation generated by a recommender<br>system. In classical collaborative filtering<br>technique, the neighbourhood is modelled by kNN<br>algorithm, but this approach has poor scalability.<br>Clustering techniques, although improved time<br>efficiency of recommender systems, can negatively affect<br>the quality (precision or accuracy) of<br>recommendations. This article presents a new approach to<br>collaborative filtering recommender systems that focuses on<br>the problem of an active user\u2019s neighbourhood<br>modelling....","title_summary":" Effect of Dataset Size on Efficiency of<br>Collaborative Filtering Recommender Systems with<br>Multi-clustering as a Neighbourhood Identification Strategy","x":-30.7744293213,"y":34.0654525757,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.7744293213,"tsne_y":34.0654525757,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"nul9jewu","source_x":"PMC","title":"An Empirical Evaluation of Attention and Pointer Networks for Paraphrase Generation","doi":"10.1007\/978-3-030-50420-5_29","abstract":"In computer vision, one of the common practices to augment the image dataset is by creating new images using geometric transformation preserving similarity. This data augmentation was one of the most significant factors for winning the Image Net competition in 2012 with vast neural networks. Unlike in computer vision and speech data, there have not been many techniques explored to augment data in natural language processing (NLP). The only technique explored in the text data is lexical substitution, which only focuses on replacing words by synonyms. In this paper, we investigate the use of different pointer networks with the sequence-to-sequence models, which have shown excellent results in neural machine translation (NMT) and text simplification tasks, in generating similar sentences using a sequence-to-sequence model and the paraphrase dataset (PPDB). The evaluation of these paraphrases is carried out by augmenting the training dataset of IMDb movie review dataset and comparing its performance with the baseline model. To our best knowledge, this is the first study on generating paraphrases using these models with the help of PPDB dataset.","publish_time":1590105600000,"author_summary":" Gupta, Varun; Krzy\u017cak, Adam","abstract_summary":" In computer vision, one of the common practices<br>to augment the image dataset is by creating new<br>images using geometric transformation preserving<br>similarity. This data augmentation was one of the most<br>significant factors for winning the Image Net competition<br>in 2012 with vast neural networks. Unlike in<br>computer vision and speech data, there have not been many<br>techniques explored to augment data in natural language<br>processing (NLP). The only technique explored in the text<br>data is lexical substitution, which only focuses on<br>replacing words by synonyms. In this paper, we<br>investigate the use of different pointer networks with the<br>sequence-to-sequence...","title_summary":" An Empirical Evaluation of Attention and<br>Pointer Networks for Paraphrase Generation","x":-34.4967460632,"y":32.251209259,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.4967460632,"tsne_y":32.251209259,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ldfgi0vr","source_x":"PMC","title":"GCN-IA: User Profile Based on Graph Convolutional Network with Implicit Association Labels","doi":"10.1007\/978-3-030-50420-5_26","abstract":"Inferring multi-label user profile plays a significant role in providing individual recommendations and exact-marketing, etc. Current researches on multi-label user profile either ignore the implicit associations among labels or do not consider the user and label semantic information in the social networks. Therefore, the user profile inferred always does not take full advantage of the global information sufficiently. To solve above problem, a new insight is presented to introduce implicit association labels as the prior knowledge enhancement and jointly embed the user and label semantic information. In this paper, a graph convolutional network with implicit associations (GCN-IA) method is proposed to obtain user profile. Specifically, a probability matrix is first designed to capture the implicit associations among labels for user representation. Then, we learn user embedding and label embedding jointly based on user-generated texts, relationships and label information. On four real-world datasets in Weibo, experimental results demonstrate that GCN-IA produces a significant improvement compared with some state-of-the-art methods.","publish_time":1590105600000,"author_summary":" Wen, Jie; Wei, Lingwei; Zhou, Wei; Han,<br>Jizhong; Guo, Tao","abstract_summary":" Inferring multi-label user profile plays a<br>significant role in providing individual recommendations<br>and exact-marketing, etc. Current researches on<br>multi-label user profile either ignore the implicit<br>associations among labels or do not consider the user and<br>label semantic information in the social networks.<br>Therefore, the user profile inferred always does not take<br>full advantage of the global information<br>sufficiently. To solve above problem, a new insight is<br>presented to introduce implicit association labels as<br>the prior knowledge enhancement and jointly embed<br>the user and label semantic information. In this<br>paper, a graph convolutional network with implicit<br>associations (GCN-IA) method is proposed...","title_summary":" GCN-IA: User Profile Based on Graph<br>Convolutional Network with Implicit Association Labels","x":-31.1359653473,"y":34.6660461426,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.1359653473,"tsne_y":34.6660461426,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"pf3dn2me","source_x":"PMC","title":"PDPNN: Modeling User Personal Dynamic Preference for Next Point-of-Interest Recommendation","doi":"10.1007\/978-3-030-50433-5_4","abstract":"Next Point of Interest (POI) recommendation is an important aspect of information feeds for Location Based Social Networks (LSBNs). The boom in LSBN platforms such as Foursquare, Twitter, and Yelp has motivated a considerable amount of research focused on POI recommendations within the last decade. Inspired by the success of deep neural networks in many fields, researchers are increasingly interested in using neural networks such as Recurrent Neural Network (RNN) to make POI recommendation. Compared to traditional methods like Factorizing Personalized Markov Chain (FPMC) and Tensor Factorization (TF), neural network methods show great improvement in general sequences prediction. However, the user\u2019s personal preference, which is crucial for personalized POI recommendation, is not addressed well in existing works. Moreover, the user\u2019s personal preference is dynamic rather than static, which can guide predictions in different temporal and spatial contexts. To this end, we propose a new deep neural network model called Personal Dynamic Preference Neural Network(PDPNN). The core of the PDPNN model includes two parts: one part learns the user\u2019s personal long-term preferences from the historical trajectories, and the other part learns the user\u2019s short-term preferences from the current trajectory. By introducing a similarity function that evaluates the similarity between spatiotemporal contexts of user\u2019s current trajectory and historical trajectories, PDPNN learns the user\u2019s personal dynamic preference from user\u2019s long-term and short-term preferences. We conducted experiments on three real-world datasets, and the results show that our model outperforms current well-known methods.","publish_time":1590364800000,"author_summary":" Zhong, Jinwen; Ma, Can; Zhou, Jiang; Wang,<br>Weiping","abstract_summary":" Next Point of Interest (POI) recommendation is<br>an important aspect of information feeds for<br>Location Based Social Networks (LSBNs). The boom in LSBN<br>platforms such as Foursquare, Twitter, and Yelp has<br>motivated a considerable amount of research focused on<br>POI recommendations within the last decade.<br>Inspired by the success of deep neural networks in many<br>fields, researchers are increasingly interested in<br>using neural networks such as Recurrent Neural<br>Network (RNN) to make POI recommendation. Compared to<br>traditional methods like Factorizing Personalized Markov<br>Chain (FPMC) and Tensor Factorization (TF), neural<br>network methods show great improvement in general<br>sequences prediction. However, the...","title_summary":" PDPNN: Modeling User Personal Dynamic<br>Preference for Next Point-of-Interest Recommendation","x":-30.6887397766,"y":34.397693634,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6887397766,"tsne_y":34.397693634,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"8uvyduay","source_x":"PMC","title":"An Ontological Approach to Knowledge Building by Data Integration","doi":"10.1007\/978-3-030-50436-6_35","abstract":"This paper discusses the uncertainty in the automation of knowledge building from heterogeneous raw datasets. Ontologies play a critical role in such a process by providing a well consolidated support to link and semantically integrate datasets via interoperability, as well as semantic enrichment and annotations. By adopting Semantic Web technology, the resulting ecosystem is fully machine consumable. However, while the manual alignment of concepts from different vocabularies is reasonable at a small scale, fully automatic mechanisms are required once the target system scales up, leading to a significant uncertainty.","publish_time":1590364800000,"author_summary":" Pileggi, Salvatore Flavio; Crain, Hayden;<br>Yahia, Sadok Ben","abstract_summary":" This paper discusses the uncertainty in the<br>automation of knowledge building from heterogeneous raw<br>datasets. Ontologies play a critical role in such a<br>process by providing a well consolidated support to<br>link and semantically integrate datasets via<br>interoperability, as well as semantic enrichment and<br>annotations. By adopting Semantic Web technology, the<br>resulting ecosystem is fully machine consumable.<br>However, while the manual alignment of concepts from<br>different vocabularies is reasonable at a small scale,<br>fully automatic mechanisms are required once the<br>target system scales up, leading to a significant<br>uncertainty.","title_summary":" An Ontological Approach to Knowledge Building<br>by Data Integration","x":-33.9594955444,"y":36.6281394958,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9594955444,"tsne_y":36.6281394958,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"ltbviybt","source_x":"PMC","title":"Node Classification in Complex Social Graphs via Knowledge-Graph Embeddings and Convolutional Neural Network","doi":"10.1007\/978-3-030-50433-5_15","abstract":"The interactions between humans and their environment, comprising living and non-living entities, can be studied via Social Network Analysis (SNA). Node classification, as well as community detection tasks, are still open research problems in SNA. Hence, SNA has become an interesting and appealing domain in Artificial Intelligence (AI) research. Immanent facts about social network structures can be effectively harnessed for training AI models in a bid to solve node classification and community detection problems in SNA. Hence, crucial aspects such as the individual attributes of spatial social actors, and the underlying patterns of relationship binding these social actors must be taken into consideration in the course of analyzing the social network. These factors determine the nature and dynamics of a given social network. In this paper, we have proposed a unique framework, Representation Learning via Knowledge-Graph Embeddings and ConvNet (RLVECN), for studying and extracting meaningful facts from social network structures to aid in node classification as well as community detection tasks. Our proposition utilizes an edge sampling approach for exploiting features of the social graph, via learning the context of each actor with respect to neighboring actors\/nodes, with the goal of generating vector-space embedding per actor. Successively, these relatively low-dimensional vector embeddings are fed as input features to a downstream classifier for classification tasks about the social graph\/network. Herein RLVECN has been trained, tested, and evaluated on real-world social networks.","publish_time":1590364800000,"author_summary":" Molokwu, Bonaventure C.; Shuvo, Shaon Bhatta;<br>Kar, Narayan C.; Kobti, Ziad","abstract_summary":" The interactions between humans and their<br>environment, comprising living and non-living entities,<br>can be studied via Social Network Analysis (SNA).<br>Node classification, as well as community<br>detection tasks, are still open research problems in SNA.<br>Hence, SNA has become an interesting and appealing<br>domain in Artificial Intelligence (AI) research.<br>Immanent facts about social network structures can be<br>effectively harnessed for training AI models in a bid to<br>solve node classification and community detection<br>problems in SNA. Hence, crucial aspects such as the<br>individual attributes of spatial social actors, and the<br>underlying patterns of relationship binding these social<br>actors must be...","title_summary":" Node Classification in Complex Social Graphs<br>via Knowledge-Graph Embeddings and<br>Convolutional Neural Network","x":-31.263299942,"y":35.5112190247,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.263299942,"tsne_y":35.5112190247,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"mviang5a","source_x":"PMC","title":"Learn More from Context: Joint Modeling of Local and Global Attention for Aspect Sentiment Classification","doi":"10.1007\/978-3-030-50436-6_20","abstract":"Aspect sentiment classification identifies the sentiment polarity of the target that appears in a sentence. The key point of aspect sentiment classification is to capture valuable information from sentence. Existing methods have acknowledged the importance of the relationship between the target and the sentence. However, these approaches only focus on the local information of the target, such as the positional relationship and the semantic similarity between the words in a sentence and the target. Moreover, the global information of the interaction of words in sentence and their influence on the final prediction of sentiment polarity are ignored in related works. To tackle this issue, the present paper proposes Joint Modeling of Local and Global Attention (LGAJM), with the following two aspects: (1) the study develops a position-based attention network concentrating on the local information of semantic similarity and position information of the target. (2) In order to fetch global information, such as context information and interaction between words in sentences, the self-attention network is introduced. Besides, a BiGRU-based gating mechanism is proposed to weight the outputs of these two attention networks. The model is evaluated on two datasets: laptop and restaurant from SemEval 2014. Experimental results demonstrate the high effectiveness of the proposed method in aspect sentiment classification.","publish_time":1590364800000,"author_summary":" Wang, Siyuan; Liu, Peng; Shi, Jinqiao; Wang,<br>Xuebin; Zhao, Can; Yin, Zelin","abstract_summary":" Aspect sentiment classification identifies<br>the sentiment polarity of the target that appears<br>in a sentence. The key point of aspect sentiment<br>classification is to capture valuable information from<br>sentence. Existing methods have acknowledged the<br>importance of the relationship between the target and the<br>sentence. However, these approaches only focus on the<br>local information of the target, such as the<br>positional relationship and the semantic similarity<br>between the words in a sentence and the target.<br>Moreover, the global information of the interaction of<br>words in sentence and their influence on the final<br>prediction of sentiment polarity are ignored in related<br>works....","title_summary":" Learn More from Context: Joint Modeling of<br>Local and Global Attention for Aspect Sentiment<br>Classification","x":-33.8788146973,"y":30.8989887238,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8788146973,"tsne_y":30.8989887238,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"g76pbauj","source_x":"PMC","title":"ENIGMA Anonymous: Symbol-Independent Inference Guiding Machine (System Description)","doi":"10.1007\/978-3-030-51054-1_29","abstract":"We describe an implementation of gradient boosting and neural guidance of saturation-style automated theorem provers that does not depend on consistent symbol names across problems. For the gradient-boosting guidance, we manually create abstracted features by considering arity-based encodings of formulas. For the neural guidance, we use symbol-independent graph neural networks (GNNs) and their embedding of the terms and clauses. The two methods are efficiently implemented in the E prover and its ENIGMA learning-guided framework. To provide competitive real-time performance of the GNNs, we have developed a new context-based approach to evaluation of generated clauses in E. Clauses are evaluated jointly in larger batches and with respect to a large number of already selected clauses (context) by the GNN that estimates their collectively most useful subset in several rounds of message passing. This means that approximative inference rounds done by the GNN are efficiently interleaved with precise symbolic inference rounds done inside E. The methods are evaluated on the MPTP large-theory benchmark and shown to achieve comparable real-time performance to state-of-the-art symbol-based methods. The methods also show high complementarity, solving a large number of hard Mizar problems.","publish_time":1591401600000,"author_summary":" Jakub\u016fv, Jan; Chvalovsk\u00fd, Karel; Ol\u0161\u00e1k,<br>Miroslav; Piotrowski, Bartosz; Suda, Martin; Urban,<br>Josef","abstract_summary":" We describe an implementation of gradient<br>boosting and neural guidance of saturation-style<br>automated theorem provers that does not depend on<br>consistent symbol names across problems. For the<br>gradient-boosting guidance, we manually create abstracted<br>features by considering arity-based encodings of<br>formulas. For the neural guidance, we use<br>symbol-independent graph neural networks (GNNs) and their<br>embedding of the terms and clauses. The two methods are<br>efficiently implemented in the E prover and its ENIGMA<br>learning-guided framework. To provide competitive real-time<br>performance of the GNNs, we have developed a new<br>context-based approach to evaluation of generated clauses in<br>E. Clauses are evaluated jointly...","title_summary":" ENIGMA Anonymous: Symbol-Independent<br>Inference Guiding Machine (System Description)","x":-34.6062278748,"y":32.8371887207,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6062278748,"tsne_y":32.8371887207,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"3izj33j6","source_x":"PMC","title":"Deep Generation of Coq Lemma Names Using Elaborated Terms","doi":"10.1007\/978-3-030-51054-1_6","abstract":"Coding conventions for naming, spacing, and other essentially stylistic properties are necessary for developers to effectively understand, review, and modify source code in large software projects. Consistent conventions in verification projects based on proof assistants, such as Coq, increase in importance as projects grow in size and scope. While conventions can be documented and enforced manually at high cost, emerging approaches automatically learn and suggest idiomatic names in Java-like languages by applying statistical language models on large code corpora. However, due to its powerful language extension facilities and fusion of type checking and computation, Coq is a challenging target for automated learning techniques. We present novel generation models for learning and suggesting lemma names for Coq projects. Our models, based on multi-input neural networks, are the first to leverage syntactic and semantic information from Coq \u2019s lexer (tokens in lemma statements), parser (syntax tree s), and kernel (elaborated terms) for naming; the key insight is that learning from elaborated terms can substantially boost model performance. We implemented our models in a toolchain, dubbed Roosterize, and applied it on a large corpus of code derived from the Mathematical Components family of projects, known for its stringent coding conventions. Our results show that Roosterize substantially outperforms baselines for suggesting lemma names, highlighting the importance of using multi-input models and elaborated terms.","publish_time":1591401600000,"author_summary":" Nie, Pengyu; Palmskog, Karl; Li, Junyi Jessy;<br>Gligoric, Milos","abstract_summary":" Coding conventions for naming, spacing, and<br>other essentially stylistic properties are<br>necessary for developers to effectively understand,<br>review, and modify source code in large software<br>projects. Consistent conventions in verification<br>projects based on proof assistants, such as Coq,<br>increase in importance as projects grow in size and<br>scope. While conventions can be documented and<br>enforced manually at high cost, emerging approaches<br>automatically learn and suggest idiomatic names in Java-like<br>languages by applying statistical language models on<br>large code corpora. However, due to its powerful<br>language extension facilities and fusion of type<br>checking and computation, Coq is a challenging target<br>for...","title_summary":" Deep Generation of Coq Lemma Names Using<br>Elaborated Terms","x":-34.8254508972,"y":32.8349723816,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.8254508972,"tsne_y":32.8349723816,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"y8nk9xeb","source_x":"PMC","title":"Evaluating Crowdsourcing and Topic Modeling in Generating Knowledge Components from Explanations","doi":"10.1007\/978-3-030-52237-7_32","abstract":"Associating assessment items with hypothesized knowledge components (KCs) enables us to gain fine-grained data on students\u2019 performance within an ed-tech system. However, creating this association is a time consuming process and requires substantial instructor effort. In this study, we present the results of crowdsourcing valuable insights into the underlying concepts of problems in mathematics and English writing, as a first step in leveraging the crowd to expedite the task of generating KCs. We presented crowdworkers with two problems in each domain and asked them to provide three explanations about why one problem is more challenging than the other. These explanations were then independently analyzed through (1) a series of qualitative coding methods and (2) several topic modeling techniques, to compare how they might assist in extracting KCs and other insights from the participant contributions. Results of our qualitative coding showed that crowdworkers were able to generate KCs that approximately matched those generated by domain experts. At the same time, the topic models\u2019 outputs were evaluated against both the domain expert generated KCs and the results of the previous coding to determine effectiveness. Ultimately we found that while the topic modeling was not up to parity with the qualitative coding methods, it did assist in identifying useful clusters of explanations. This work demonstrates a method to leverage both the crowd\u2019s knowledge and topic modeling to assist in the process of generating KCs for assessment items.","publish_time":1591660800000,"author_summary":" Moore, Steven; Nguyen, Huy A.; Stamper, John","abstract_summary":" Associating assessment items with<br>hypothesized knowledge components (KCs) enables us to gain<br>fine-grained data on students\u2019 performance within an<br>ed-tech system. However, creating this association is<br>a time consuming process and requires<br>substantial instructor effort. In this study, we present<br>the results of crowdsourcing valuable insights<br>into the underlying concepts of problems in<br>mathematics and English writing, as a first step in<br>leveraging the crowd to expedite the task of generating<br>KCs. We presented crowdworkers with two problems in<br>each domain and asked them to provide three<br>explanations about why one problem is more challenging than<br>the other. These explanations...","title_summary":" Evaluating Crowdsourcing and Topic Modeling<br>in Generating Knowledge Components from<br>Explanations","x":-36.0894088745,"y":32.322467804,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.0894088745,"tsne_y":32.322467804,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"olv7h0mg","source_x":"PMC","title":"Robust Neural Automated Essay Scoring Using Item Response Theory","doi":"10.1007\/978-3-030-52237-7_44","abstract":"Automated essay scoring (AES) is the task of automatically assigning scores to essays as an alternative to human grading. Conventional AES methods typically rely on manually tuned features, which are laborious to effectively develop. To obviate the need for feature engineering, many deep neural network (DNN)-based AES models have been proposed and have achieved state-of-the-art accuracy. DNN-AES models require training on a large dataset of graded essays. However, assigned grades in such datasets are known to be strongly biased due to effects of rater bias when grading is conducted by assigning a few raters in a rater set to each essay. Performance of DNN models rapidly drops when such biased data are used for model training. In the fields of educational and psychological measurement, item response theory (IRT) models that can estimate essay scores while considering effects of rater characteristics have recently been proposed. This study therefore proposes a new DNN-AES framework that integrates IRT models to deal with rater bias within training data. To our knowledge, this is a first attempt at addressing rating bias effects in training data, which is a crucial but overlooked problem.","publish_time":1591660800000,"author_summary":" Uto, Masaki; Okano, Masashi","abstract_summary":" Automated essay scoring (AES) is the task of<br>automatically assigning scores to essays as an alternative to<br>human grading. Conventional AES methods typically<br>rely on manually tuned features, which are<br>laborious to effectively develop. To obviate the need for<br>feature engineering, many deep neural network<br>(DNN)-based AES models have been proposed and have achieved<br>state-of-the-art accuracy. DNN-AES models require training on a<br>large dataset of graded essays. However, assigned<br>grades in such datasets are known to be strongly biased<br>due to effects of rater bias when grading is<br>conducted by assigning a few raters in a rater set to each...","title_summary":" Robust Neural Automated Essay Scoring Using<br>Item Response Theory","x":-35.8971557617,"y":31.9406661987,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.8971557617,"tsne_y":31.9406661987,"subcluster":13,"subcluster_description":"Multi-Turn Response Selection Models","shape":"p"},{"cord_uid":"b36f5lhb","source_x":"PMC","title":"Exploring Automated Question Answering Methods for Teaching Assistance","doi":"10.1007\/978-3-030-52237-7_49","abstract":"One important aspect of learning is through verbal interactions with teachers or teaching assistants (TAs), which requires significant effort and puts a heavy burden on teachers. Artificial intelligence has the potential to reduce their burden by automatically addressing the routine part of this interaction, which will free them up to focus on more important aspects of learning. We explore the use of automated question answering methods to power virtual TAs in online course discussion forums, which are heavily relied on during the COVID-19 pandemic as classes transition online. First, we focus on answering frequent and repetitive logistical questions and adopt a question answering framework that consists of two steps: retrieving relevant documents from a repository and extracting answers from retrieved documents. The document repository consists of course materials that contain information on course logistics, e.g., the syllabus, lecture slides, course emails, and prior discussion forum posts. This question answering framework can help virtual TAs decide whether a question is answerable and how to answer it. Second, we analyze the timing of student posts in discussion threads and develop a classifier to predict the timing of follow-up posts. This classifier can help virtual TAs decide whether to respond to a question and when to do so. We conduct experiments on data collected from an introductory physics course and discuss both the utility and limitations of our approach .","publish_time":1591660800000,"author_summary":" Zylich, Brian; Viola, Adam; Toggerson, Brokk;<br>Al-Hariri, Lara; Lan, Andrew","abstract_summary":" One important aspect of learning is through<br>verbal interactions with teachers or teaching<br>assistants (TAs), which requires significant effort and<br>puts a heavy burden on teachers. Artificial<br>intelligence has the potential to reduce their burden by<br>automatically addressing the routine part of this<br>interaction, which will free them up to focus on more<br>important aspects of learning. We explore the use of<br>automated question answering methods to power virtual<br>TAs in online course discussion forums, which are<br>heavily relied on during the COVID-19 pandemic as<br>classes transition online. First, we focus on<br>answering frequent and repetitive logistical questions<br>and adopt...","title_summary":" Exploring Automated Question Answering<br>Methods for Teaching Assistance","x":-35.713684082,"y":32.5776634216,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.713684082,"tsne_y":32.5776634216,"subcluster":12,"subcluster_description":"Assistancefooling Automatic Short Answer","shape":"p"},{"cord_uid":"63ik6ty3","source_x":"PMC","title":"Using Neural Tensor Networks for Open Ended Short Answer Assessment","doi":"10.1007\/978-3-030-52237-7_16","abstract":"In this paper, we present a novel approach to leverage the power of Neural Tensor Networks (NTN) for student answer assessment in intelligent tutoring systems. The approach was evaluated on data collected using a dialogue based intelligent tutoring system (ITS). Particularly, we have experimented with different assessment models that were trained using features generated from knowledge graph embeddings derived with NTN. Our experiments showed that the model trained with the feature vectors generated with NTN, when trained with a combination of domain specific and domain general triplets, performs better than a previously proposed LSTM based approach.","publish_time":1591660800000,"author_summary":" Gautam, Dipesh; Rus, Vasile","abstract_summary":" In this paper, we present a novel approach to<br>leverage the power of Neural Tensor Networks (NTN) for<br>student answer assessment in intelligent tutoring<br>systems. The approach was evaluated on data collected<br>using a dialogue based intelligent tutoring system<br>(ITS). Particularly, we have experimented with<br>different assessment models that were trained using<br>features generated from knowledge graph embeddings<br>derived with NTN. Our experiments showed that the model<br>trained with the feature vectors generated with NTN,<br>when trained with a combination of domain specific<br>and domain general triplets, performs better than<br>a previously proposed LSTM based approach.","title_summary":" Using Neural Tensor Networks for Open Ended<br>Short Answer Assessment","x":-35.3512001038,"y":32.1235618591,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.3512001038,"tsne_y":32.1235618591,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"z8sv9f5k","source_x":"PMC","title":"Fooling Automatic Short Answer Grading Systems","doi":"10.1007\/978-3-030-52237-7_15","abstract":"With the rising success of adversarial attacks on many NLP tasks, systems which actually operate in an adversarial scenario need to be reevaluated. For this purpose, we pose the following research question: How difficult is it to fool automatic short answer grading systems? In particular, we investigate the robustness of the state of the art automatic short answer grading system proposed by Sung et al. towards cheating in the form of universal adversarial trigger employment. These are short token sequences that can be prepended to students\u2019 answers in an exam to artificially improve their automatically assigned grade. Such triggers are especially critical as they can easily be used by anyone once they are found. In our experiments, we discovered triggers which allow students to pass exams with passing thresholds of [Formula: see text] without answering a single question correctly. Furthermore, we show that such triggers generalize across models and datasets in this scenario, nullifying the defense strategy of keeping grading models or data secret.","publish_time":1591660800000,"author_summary":" Filighera, Anna; Steuer, Tim; Rensing,<br>Christoph","abstract_summary":" With the rising success of adversarial attacks<br>on many NLP tasks, systems which actually operate<br>in an adversarial scenario need to be<br>reevaluated. For this purpose, we pose the following<br>research question: How difficult is it to fool automatic<br>short answer grading systems? In particular, we<br>investigate the robustness of the state of the art automatic<br>short answer grading system proposed by Sung et al.<br>towards cheating in the form of universal adversarial<br>trigger employment. These are short token sequences<br>that can be prepended to students\u2019 answers in an exam<br>to artificially improve their automatically<br>assigned grade. Such triggers are...","title_summary":" Fooling Automatic Short Answer Grading<br>Systems","x":-35.6361198425,"y":32.5884590149,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.6361198425,"tsne_y":32.5884590149,"subcluster":12,"subcluster_description":"Assistancefooling Automatic Short Answer","shape":"p"},{"cord_uid":"f1cbrwi2","source_x":"PMC","title":"Introducing a Framework to Assess Newly Created Questions with Natural Language Processing","doi":"10.1007\/978-3-030-52237-7_4","abstract":"Statistical models such as those derived from Item Response Theory (IRT) enable the assessment of students on a specific subject, which can be useful for several purposes (e.g., learning path customization, drop-out prediction). However, the questions have to be assessed as well and, although it is possible to estimate with IRT the characteristics of questions that have already been answered by several students, this technique cannot be used on newly generated questions. In this paper, we propose a framework to train and evaluate models for estimating the difficulty and discrimination of newly created Multiple Choice Questions by extracting meaningful features from the text of the question and of the possible choices. We implement one model using this framework and test it on a real-world dataset provided by CloudAcademy, showing that it outperforms previously proposed models, reducing by 6.7% the RMSE for difficulty estimation and by 10.8% the RMSE for discrimination estimation. We also present the results of an ablation study performed to support our features choice and to show the effects of different characteristics of the questions\u2019 text on difficulty and discrimination.","publish_time":1591660800000,"author_summary":" Benedetto, Luca; Cappelli, Andrea; Turrin,<br>Roberto; Cremonesi, Paolo","abstract_summary":" Statistical models such as those derived from<br>Item Response Theory (IRT) enable the assessment of<br>students on a specific subject, which can be useful for<br>several purposes (e.g., learning path customization,<br>drop-out prediction). However, the questions have to be<br>assessed as well and, although it is possible to estimate<br>with IRT the characteristics of questions that have<br>already been answered by several students, this<br>technique cannot be used on newly generated questions. In<br>this paper, we propose a framework to train and<br>evaluate models for estimating the difficulty and<br>discrimination of newly created Multiple Choice Questions by<br>extracting meaningful features...","title_summary":" Introducing a Framework to Assess Newly<br>Created Questions with Natural Language Processing","x":-35.4224357605,"y":32.3203659058,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.4224357605,"tsne_y":32.3203659058,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"m04k6jmc","source_x":"PMC","title":"Remember the Facts? Investigating Answer-Aware Neural Question Generation for Text Comprehension","doi":"10.1007\/978-3-030-52237-7_41","abstract":"Reading is a crucial skill in the 21st century. Thus, scaffolding text comprehension by automatically generated questions may greatly profit learners. Yet, the state-of-the-art methods for automatic question generation, answer-aware neural question generators (NQGs), are rarely seen in the educational domain. Hence, we investigate the quality of questions generated by a novel approach comprising an answer-aware NQG and two novel answer candidate selection strategies based on semantic graph matching. In median, the approach generates clear, answerable and useful factual questions outperforming an answer-unaware NQG on educational datasets as shown by automatic and human evaluation. Furthermore, we analyze the types of questions generated, showing that the question types differ across answer selection strategies yet remain factual.","publish_time":1591660800000,"author_summary":" Steuer, Tim; Filighera, Anna; Rensing,<br>Christoph","abstract_summary":" Reading is a crucial skill in the 21st century.<br>Thus, scaffolding text comprehension by<br>automatically generated questions may greatly profit<br>learners. Yet, the state-of-the-art methods for<br>automatic question generation, answer-aware neural<br>question generators (NQGs), are rarely seen in the<br>educational domain. Hence, we investigate the quality of<br>questions generated by a novel approach comprising an<br>answer-aware NQG and two novel answer candidate selection<br>strategies based on semantic graph matching. In median,<br>the approach generates clear, answerable and<br>useful factual questions outperforming an<br>answer-unaware NQG on educational datasets as shown by<br>automatic and human evaluation. Furthermore, we analyze<br>the types of...","title_summary":" Remember the Facts? Investigating<br>Answer-Aware Neural Question Generation for Text<br>Comprehension","x":-35.6984558105,"y":32.8255996704,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.6984558105,"tsne_y":32.8255996704,"subcluster":12,"subcluster_description":"Assistancefooling Automatic Short Answer","shape":"p"},{"cord_uid":"9exbq5po","source_x":"PMC","title":"Bridging Over from Learning Videos to Learning Resources Through Automatic Keyword Extraction","doi":"10.1007\/978-3-030-52240-7_69","abstract":"The presented system and approach facilitate intelligent, contextualized information access for learners based on automatic learning video analysis. The underlying workflow starts with automatically extracting keywords from learning videos followed by the generation of recommendations of learning materials. The approach has been implemented and investigated in a user study in a real-world VET setting. The study investigated the acceptance, perceived quality and relevance of automatically extracted keywords and automatically generated learning resource recommendations in the context of a set of learning videos related to chemistry and chemical engineering. The results indicate that such extracted keywords are in line with user-generated keywords and summarize the content of videos quite well. Also, they can be used as search key to find relevant learning resources.","publish_time":1591747200000,"author_summary":" Schulten, Cleo; Manske, Sven;<br>Langner-Thiele, Angela; Hoppe, H. Ulrich","abstract_summary":" The presented system and approach facilitate<br>intelligent, contextualized information access for<br>learners based on automatic learning video analysis.<br>The underlying workflow starts with<br>automatically extracting keywords from learning videos<br>followed by the generation of recommendations of<br>learning materials. The approach has been implemented<br>and investigated in a user study in a real-world VET<br>setting. The study investigated the acceptance,<br>perceived quality and relevance of automatically<br>extracted keywords and automatically generated<br>learning resource recommendations in the context of a<br>set of learning videos related to chemistry and<br>chemical engineering. The results indicate that such<br>extracted keywords are in line with user-generated...","title_summary":" Bridging Over from Learning Videos to Learning<br>Resources Through Automatic Keyword Extraction","x":-34.9790077209,"y":33.7916412354,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.9790077209,"tsne_y":33.7916412354,"subcluster":5,"subcluster_description":"Variantsrandom Steinhaus Distances","shape":"p"},{"cord_uid":"ef97jzc4","source_x":"PMC","title":"Deep Knowledge Tracing with Transformers","doi":"10.1007\/978-3-030-52240-7_46","abstract":"In this work, we propose a Transformer-based model to trace students\u2019 knowledge acquisition. We modified the Transformer structure to utilize 1) the association between questions and skills and 2) the elapsed time between question steps. The use of question-skill associations allows the model to learn specific representation for frequently encountered questions while representing rare questions with their underline skill representations. The inclusion of elapsed time opens the opportunity to address forgetting. Our approach outperforms the state-of-the-art methods in the literature by roughly 10% in AUC with frequently used public datasets.","publish_time":1591747200000,"author_summary":" Pu, Shi; Yudelson, Michael; Ou, Lu; Huang,<br>Yuchi","abstract_summary":" In this work, we propose a Transformer-based<br>model to trace students\u2019 knowledge acquisition. We<br>modified the Transformer structure to utilize 1) the<br>association between questions and skills and 2) the elapsed<br>time between question steps. The use of<br>question-skill associations allows the model to learn<br>specific representation for frequently encountered<br>questions while representing rare questions with their<br>underline skill representations. The inclusion of<br>elapsed time opens the opportunity to address<br>forgetting. Our approach outperforms the<br>state-of-the-art methods in the literature by roughly 10% in AUC<br>with frequently used public datasets.","title_summary":" Deep Knowledge Tracing with Transformers","x":-35.8979301453,"y":32.8162956238,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.8979301453,"tsne_y":32.8162956238,"subcluster":12,"subcluster_description":"Assistancefooling Automatic Short Answer","shape":"p"},{"cord_uid":"vqcw4huk","source_x":"PMC","title":"Sequence-to-Sequence Models for Automated Text Simplification","doi":"10.1007\/978-3-030-52240-7_6","abstract":"A key writing skill is the capability to clearly convey desired meaning using available linguistic knowledge. Consequently, writers must select from a large array of idioms, vocabulary terms that are semantically equivalent, and discourse features that simultaneously reflect content and allow readers to grasp meaning. In many cases, a simplified version of a text is needed to ensure comprehension on the part of a targeted audience (e.g., second language learners). To address this need, we propose an automated method to simplify texts based on paraphrasing. Specifically, we explore the potential for a deep learning model, previously used for machine translation, to learn a simplified version of the English language within the context of short phrases. The best model, based on an Universal Transformer architecture, achieved a BLEU score of 66.01. We also evaluated this model\u2019s capability to perform similar transformation to texts that were simplified by human experts at different levels.","publish_time":1591747200000,"author_summary":" Botarleanu, Robert-Mihai; Dascalu, Mihai;<br>Crossley, Scott Andrew; McNamara, Danielle S.","abstract_summary":" A key writing skill is the capability to clearly<br>convey desired meaning using available linguistic<br>knowledge. Consequently, writers must select from a large<br>array of idioms, vocabulary terms that are<br>semantically equivalent, and discourse features that<br>simultaneously reflect content and allow readers to grasp<br>meaning. In many cases, a simplified version of a text is<br>needed to ensure comprehension on the part of a<br>targeted audience (e.g., second language learners). To<br>address this need, we propose an automated method to<br>simplify texts based on paraphrasing. Specifically, we<br>explore the potential for a deep learning model,<br>previously used for machine translation,...","title_summary":" Sequence-to-Sequence Models for Automated<br>Text Simplification","x":-34.9051780701,"y":32.2665367126,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.9051780701,"tsne_y":32.2665367126,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"ljf5fgk9","source_x":"PMC","title":"Predicting Learners Need for Recommendation Using Dynamic Graph-Based Knowledge Tracing","doi":"10.1007\/978-3-030-52240-7_9","abstract":"Personalized recommendation as a practical approach to overcoming information overloading has been widely used in e-learning. Based on learners individual knowledge level, we propose a new model that can predict learners needs for recommendation using dynamic graph-based knowledge tracing. By applying the Gated Recurrent Unit (GRU) and the Attention model, this approach designs a dynamic graph over different time steps. Through learning feature information and topology representation of nodes\/learners, this model can predict with high accuracy of 80,63% learners with low knowledge acquisition and prepare them for further recommendation.","publish_time":1591747200000,"author_summary":" Chanaa, Abdessamad; El Faddouli, Nour-Eddine","abstract_summary":" Personalized recommendation as a practical<br>approach to overcoming information overloading has<br>been widely used in e-learning. Based on learners<br>individual knowledge level, we propose a new model that can<br>predict learners needs for recommendation using<br>dynamic graph-based knowledge tracing. By applying<br>the Gated Recurrent Unit (GRU) and the Attention<br>model, this approach designs a dynamic graph over<br>different time steps. Through learning feature<br>information and topology representation of<br>nodes\/learners, this model can predict with high accuracy of<br>80,63% learners with low knowledge acquisition and<br>prepare them for further recommendation.","title_summary":" Predicting Learners Need for Recommendation<br>Using Dynamic Graph-Based Knowledge Tracing","x":-31.2095317841,"y":34.7767333984,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2095317841,"tsne_y":34.7767333984,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"jb7g6lwf","source_x":"PMC","title":"Investigating Transformers for Automatic Short Answer Grading","doi":"10.1007\/978-3-030-52240-7_8","abstract":"Recent advancements in the field of deep learning for natural language processing made it possible to use novel deep learning architectures, such as the Transformer, for increasingly complex natural language processing tasks. Combined with novel unsupervised pre-training tasks such as masked language modeling, sentence ordering or next sentence prediction, those natural language processing models became even more accurate. In this work, we experiment with fine-tuning different pre-trained Transformer based architectures. We train the newest and most powerful, according to the glue benchmark, transformers on the SemEval-2013 dataset. We also explore the impact of transfer learning a model fine-tuned on the MNLI dataset to the SemEval-2013 dataset on generalization and performance. We report up to 13% absolute improvement in macro-average-F1 over state-of-the-art results. We show that models trained with knowledge distillation are feasible for use in short answer grading. Furthermore, we compare multilingual models on a machine-translated version of the SemEval-2013 dataset.","publish_time":1591747200000,"author_summary":" Camus, Leon; Filighera, Anna","abstract_summary":" Recent advancements in the field of deep<br>learning for natural language processing made it<br>possible to use novel deep learning architectures, such<br>as the Transformer, for increasingly complex<br>natural language processing tasks. Combined with<br>novel unsupervised pre-training tasks such as<br>masked language modeling, sentence ordering or next<br>sentence prediction, those natural language<br>processing models became even more accurate. In this work,<br>we experiment with fine-tuning different<br>pre-trained Transformer based architectures. We train the<br>newest and most powerful, according to the glue<br>benchmark, transformers on the SemEval-2013 dataset. We<br>also explore the impact of transfer learning a model<br>fine-tuned on the...","title_summary":" Investigating Transformers for Automatic<br>Short Answer Grading","x":-34.9634933472,"y":32.3526306152,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.9634933472,"tsne_y":32.3526306152,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"mllkrppi","source_x":"PMC","title":"Extended Multi-document Cohesion Network Analysis Centered on Comprehension Prediction","doi":"10.1007\/978-3-030-52240-7_42","abstract":"Theories of discourse argue that comprehension depends on the coherence of the learner\u2019s mental representation. Our aim is to create a reliable automated representation to estimate readers\u2019 level of comprehension based on different productions, namely self-explanations and answers to open-ended questions. Previous work relied on Cohesion Network Analysis to model a cohesion graph composed of semantic links between multiple reference texts and student productions. From this graph, a set of features was derived and used to build machine learning models to predict student comprehension scores. In this paper, we build on top of the previous study by: a) extending the CNA graph by adding new semantic links targeting specific sentences that should have been captured within the learner\u2019s productions, and b) cleaning the self-explanations by eliminating frozen expression, as well as entries which seemed nearly identical to the source text. The results are in line with the conclusions of the previous study regarding the importance of both self-explanations and question answers in predicting the students\u2019 reading comprehension level. They also outline the limitations of our feature generation approach, in which no substantial improvements were detected, despite adding more fine-grained features.","publish_time":1591747200000,"author_summary":" Nicula, Bogdan; Perret, Cecile A.; Dascalu,<br>Mihai; McNamara, Danielle S.","abstract_summary":" Theories of discourse argue that<br>comprehension depends on the coherence of the learner\u2019s<br>mental representation. Our aim is to create a reliable<br>automated representation to estimate readers\u2019 level of<br>comprehension based on different productions, namely<br>self-explanations and answers to open-ended questions. Previous<br>work relied on Cohesion Network Analysis to model a<br>cohesion graph composed of semantic links between<br>multiple reference texts and student productions. From<br>this graph, a set of features was derived and used to<br>build machine learning models to predict student<br>comprehension scores. In this paper, we build on top of the<br>previous study by: a) extending the...","title_summary":" Extended Multi-document Cohesion Network<br>Analysis Centered on Comprehension Prediction","x":-35.757648468,"y":33.5681037903,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.757648468,"tsne_y":33.5681037903,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"94s7yvfs","source_x":"PMC","title":"Deep-Cross-Attention Recommendation Model for Knowledge Sharing Micro Learning Service","doi":"10.1007\/978-3-030-52240-7_31","abstract":"Aims to provide flexible, effective and personalized online learning service, micro learning has gained wide attention in recent years as more people turn to use fragment time to grasp fragmented knowledge. Widely available online knowledge sharing is one of the most representative approaches to micro learning, and it is well accepted by online learners. However, information overload challenges such personalized online learning services. In this paper, we propose a deep cross attention recommendation model to provide online users with personalized resources based on users\u2019 profile and historical online behaviours. This model benefits from the deep neural network, feature crossing, and attention mechanism mutually. The experiment result showed that the proposed model outperformed the state-of-the-art baselines.","publish_time":1591747200000,"author_summary":" Lin, Jiayin; Sun, Geng; Shen, Jun; Pritchard,<br>David; Cui, Tingru; Xu, Dongming; Li, Li; Beydoun,<br>Ghassan; Chen, Shiping","abstract_summary":" Aims to provide flexible, effective and<br>personalized online learning service, micro learning has<br>gained wide attention in recent years as more people<br>turn to use fragment time to grasp fragmented<br>knowledge. Widely available online knowledge sharing is<br>one of the most representative approaches to micro<br>learning, and it is well accepted by online learners.<br>However, information overload challenges such<br>personalized online learning services. In this paper, we<br>propose a deep cross attention recommendation model to<br>provide online users with personalized resources<br>based on users\u2019 profile and historical online<br>behaviours. This model benefits from the deep neural<br>network, feature crossing, and...","title_summary":" Deep-Cross-Attention Recommendation Model<br>for Knowledge Sharing Micro Learning Service","x":-30.890460968,"y":34.6235313416,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.890460968,"tsne_y":34.6235313416,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"cm4ftw3o","source_x":"PMC","title":"Automated Short-Answer Grading Using Deep Neural Networks and Item Response Theory","doi":"10.1007\/978-3-030-52240-7_61","abstract":"Automated short-answer grading (ASAG) methods using deep neural networks (DNN) have achieved state-of-the-art accuracy. However, further improvement is required for high-stakes and large-scale examinations because even a small scoring error will affect many test-takers. To improve scoring accuracy, we propose a new ASAG method that combines a conventional DNN-ASAG model and an item response theory (IRT) model. Our method uses an IRT model to estimate the test-taker\u2019s ability from his\/her true-false responses to objective questions that are offered with a target short-answer question in the same test. Then, the target short-answer score is predicted by jointly using the ability value and a distributed short-answer representation, which is obtained from an intermediate layer of a DNN-ASAG model.","publish_time":1591747200000,"author_summary":" Uto, Masaki; Uchida, Yuto","abstract_summary":" Automated short-answer grading (ASAG)<br>methods using deep neural networks (DNN) have achieved<br>state-of-the-art accuracy. However, further improvement is<br>required for high-stakes and large-scale examinations<br>because even a small scoring error will affect many<br>test-takers. To improve scoring accuracy, we propose a new<br>ASAG method that combines a conventional DNN-ASAG<br>model and an item response theory (IRT) model. Our<br>method uses an IRT model to estimate the test-taker\u2019s<br>ability from his\/her true-false responses to<br>objective questions that are offered with a target<br>short-answer question in the same test. Then, the target<br>short-answer score is predicted by jointly using the ability<br>value...","title_summary":" Automated Short-Answer Grading Using Deep<br>Neural Networks and Item Response Theory","x":-35.9197998047,"y":32.0413551331,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.9197998047,"tsne_y":32.0413551331,"subcluster":13,"subcluster_description":"Multi-Turn Response Selection Models","shape":"p"},{"cord_uid":"va82gnps","source_x":"PMC","title":"Exploring Automatic Short Answer Grading as a Tool to Assist in Human Rating","doi":"10.1007\/978-3-030-52240-7_14","abstract":"This project proposes using BERT (Bidirectional Encoder Representations from Transformers) as a tool to assist educators with automated short answer grading (ASAG) as opposed to replacing human judgement in high-stakes scenarios. Many educators are hesitant to give authority to an automated system, especially in assessment tasks such as grading constructed response items. However, evaluating free-response text can be time and labor costly for one rater, let alone multiple raters. In addition, some degree of inconsistency exists within and between raters for assessing a given task. Recent advances in Natural Language Processing have resulted in subsequent improvements for technologies that rely on artificial intelligence and human language. New, state-of-the-art models such as BERT, an open source, pre-trained language model, have decreased the amount of training data needed for specific tasks and in turn, have reduced the amount of human annotation necessary for producing a high-quality classification model. After training BERT on expert ratings of constructed responses, we use subsequent automated grading to calculate Cohen\u2019s Kappa as a measure of inter-rater reliability between the automated system and the human rater. For practical application, when the inter-rater reliability metric is unsatisfactory, we suggest that the human rater(s) use the automated model to call attention to ratings where a second opinion might be needed to confirm the rater\u2019s correctness and consistency of judgement.","publish_time":1591747200000,"author_summary":" Condor, Aubrey","abstract_summary":" This project proposes using BERT<br>(Bidirectional Encoder Representations from Transformers)<br>as a tool to assist educators with automated short<br>answer grading (ASAG) as opposed to replacing human<br>judgement in high-stakes scenarios. Many educators are<br>hesitant to give authority to an automated system,<br>especially in assessment tasks such as grading<br>constructed response items. However, evaluating<br>free-response text can be time and labor costly for one rater,<br>let alone multiple raters. In addition, some<br>degree of inconsistency exists within and between<br>raters for assessing a given task. Recent advances in<br>Natural Language Processing have resulted in<br>subsequent improvements for technologies that rely...","title_summary":" Exploring Automatic Short Answer Grading as a<br>Tool to Assist in Human Rating","x":-35.7860450745,"y":32.1502609253,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.7860450745,"tsne_y":32.1502609253,"subcluster":13,"subcluster_description":"Multi-Turn Response Selection Models","shape":"p"},{"cord_uid":"uodby95t","source_x":"PMC","title":"A Graph-Based Keyphrase Extraction Model with Three-Way Decision","doi":"10.1007\/978-3-030-52705-1_8","abstract":"Keyphrase extraction has been a popular research topic in the field of natural language processing in recent years. But how to extract keyphrases precisely and effectively is still a challenge. The mainstream methods are supervised learning methods and graph-based methods. Generally, the effects of supervised methods are better than unsupervised methods. However, there are many problems in supervised methods such as the difficulty in obtaining training data, the cost of labeling and the limitation of the classification function trained by training data. In recent years, the development of the graph-based method has made great progress and its performance of extraction is getting closer and closer to the supervised method, so the graph-based method of keyphrase extraction has got a wide concern from researchers. In this paper, we propose a new model that applies the three-way decision theory to graph-based keyphrase extraction model. In our model, we propose algorithms dividing the set of candidate phrases into the positive domain, the boundary domain and the negative domain depending on graph-based attributes, and combining candidate phrases in the positive domain and the boundary domain qualified by graph-based attributes and non- graph-based attributes to get keyphrases. Experimental results show that our model can effectively improve the extraction precision compared with baseline methods.","publish_time":1591747200000,"author_summary":" Chen, Tianlei; Miao, Duoqian; Zhang, Yuebing","abstract_summary":" Keyphrase extraction has been a popular<br>research topic in the field of natural language<br>processing in recent years. But how to extract keyphrases<br>precisely and effectively is still a challenge. The<br>mainstream methods are supervised learning methods and<br>graph-based methods. Generally, the effects of supervised<br>methods are better than unsupervised methods.<br>However, there are many problems in supervised methods<br>such as the difficulty in obtaining training data,<br>the cost of labeling and the limitation of the<br>classification function trained by training data. In recent<br>years, the development of the graph-based method has<br>made great progress and its performance of<br>extraction...","title_summary":" A Graph-Based Keyphrase Extraction Model with<br>Three-Way Decision","x":-33.5671768188,"y":32.6381568909,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5671768188,"tsne_y":32.6381568909,"subcluster":18,"subcluster_description":"Graph-Based Keyphrase Extraction","shape":"p"},{"cord_uid":"5uzl1jpu","source_x":"PMC","title":"Multi-granularity Complex Network Representation Learning","doi":"10.1007\/978-3-030-52705-1_18","abstract":"Network representation learning aims to learn the low dimensional vector of the nodes in a network while maintaining the inherent properties of the original information. Existing algorithms focus on the single coarse-grained topology of nodes or text information alone, which cannot describe complex information networks. However, node structure and attribution are interdependent, indecomposable. Therefore, it is essential to learn the representation of node based on both the topological structure and node additional attributes. In this paper, we propose a multi-granularity complex network representation learning model (MNRL), which integrates topological structure and additional information at the same time, and presents these fused information learning into the same granularity semantic space that through fine-to-coarse to refine the complex network. Experiments show that our method can not only capture indecomposable multi-granularity information, but also retain various potential similarities of both topology and node attributes. It has achieved effective results in the downstream work of node classification and the link prediction on real-world datasets.","publish_time":1591747200000,"author_summary":" Li, Peisen; Wang, Guoyin; Hu, Jun; Li, Yun","abstract_summary":" Network representation learning aims to learn<br>the low dimensional vector of the nodes in a network<br>while maintaining the inherent properties of the<br>original information. Existing algorithms focus on the<br>single coarse-grained topology of nodes or text<br>information alone, which cannot describe complex<br>information networks. However, node structure and<br>attribution are interdependent, indecomposable.<br>Therefore, it is essential to learn the representation of<br>node based on both the topological structure and<br>node additional attributes. In this paper, we<br>propose a multi-granularity complex network<br>representation learning model (MNRL), which integrates<br>topological structure and additional information at the<br>same time, and presents these...","title_summary":" Multi-granularity Complex Network<br>Representation Learning","x":-31.320230484,"y":35.675819397,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.320230484,"tsne_y":35.675819397,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"wzp9eapq","source_x":"PMC","title":"HGAR: Hybrid Granular Algorithm for Rating Recommendation","doi":"10.1007\/978-3-030-52705-1_20","abstract":"Recommendation algorithms based on collaborative filtering show products which people might like and play an important role in personalized service. Nevertheless, the most of them just adopt explicit information feedback and achieve low recommendation accuracy. In recent years, deep learning methods utilize non-linear network framework to receive feature representation of massive data, which can obtain implicit information feedback. Therefore, many algorithms are designed based on deep learning to improve recommendation effects. Even so, the results are unsatisfactory. The reason is that they never consider explicit information feedback. In this paper, we propose a Hybrid Granular Algorithm for Rating Recommendation (HGAR), which is based on granulation computing. The core idea is to explore the multi-granularity of interaction information for both explicit and implicit feedback to predict the users ratings. Thus, we used Singular Value Decomposition model to get explicit information and implicit information can be received by multi-layer perception of deep learning. In addition, we fused the two part information when the two models are jointly trained. Therefore, HGAR can explore the multi-granularity of interaction information which learned explicit interaction information and mined implicit information in different information granular level. Experiment results show that HGAR significantly improved recommendation accuracy compared with different recommendation models including collaborative filtering and deep learning methods.","publish_time":1591747200000,"author_summary":" Qian, Fulan; Huang, Yafan; Li, Jianhong; Zhao,<br>Shu; Chen, Jie; Wang, Xiangyang; Zhang, Yanping","abstract_summary":" Recommendation algorithms based on<br>collaborative filtering show products which people might<br>like and play an important role in personalized<br>service. Nevertheless, the most of them just adopt<br>explicit information feedback and achieve low<br>recommendation accuracy. In recent years, deep learning<br>methods utilize non-linear network framework to<br>receive feature representation of massive data, which<br>can obtain implicit information feedback.<br>Therefore, many algorithms are designed based on deep<br>learning to improve recommendation effects. Even so,<br>the results are unsatisfactory. The reason is that<br>they never consider explicit information<br>feedback. In this paper, we propose a Hybrid Granular<br>Algorithm for Rating Recommendation (HGAR),...","title_summary":" HGAR: Hybrid Granular Algorithm for Rating<br>Recommendation","x":-30.8831863403,"y":34.3828201294,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.8831863403,"tsne_y":34.3828201294,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"m0fqzpch","source_x":"PMC","title":"Mathematical World Knowledge Contained in the Multilingual Wikipedia Project","doi":"10.1007\/978-3-030-52200-1_35","abstract":"The purpose of this project is to test and evaluate an approach for Formula Concept Discovery (FCD). FCD aims at retrieving a formula concept (in the form of a Wikidata item) together with its defining formula within documents, in this case 100 English Wikipedia articles. To correctly identify the defining formula of a Wikipedia article, this approach searches for shared formulae across Wikipedia articles available in different languages. The formula shared in the most languages is then assumed to be the defining formula. The results show that neither this approach alone nor a combination with an existing approach that considers the order of the formulae inside an article leads to satisfying results. It is thus concluded that the number of times a formula is shared across a Wikipedia article in different languages is not a good indicator to determine the defining formula with the current approach. Consequently, several ideas for further research are proposed which could improve the results.","publish_time":1591401600000,"author_summary":" Halbach, Dennis Tobias","abstract_summary":" The purpose of this project is to test and<br>evaluate an approach for Formula Concept Discovery<br>(FCD). FCD aims at retrieving a formula concept (in the<br>form of a Wikidata item) together with its defining<br>formula within documents, in this case 100 English<br>Wikipedia articles. To correctly identify the defining<br>formula of a Wikipedia article, this approach searches<br>for shared formulae across Wikipedia articles<br>available in different languages. The formula shared in<br>the most languages is then assumed to be the<br>defining formula. The results show that neither this<br>approach alone nor a combination with an existing<br>approach that considers...","title_summary":" Mathematical World Knowledge Contained in the<br>Multilingual Wikipedia Project","x":-33.6800613403,"y":35.155960083,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6800613403,"tsne_y":35.155960083,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"pxkghxh0","source_x":"PMC","title":"Graph-Based Image Retrieval: State of the Art","doi":"10.1007\/978-3-030-51935-3_32","abstract":"The paper deals with the problem of semantic Image Retrieval. Indeed, the image has recently gained popularity in several domains such as medical domain, marketing, etc. Image plays a very vital role in documentation. However, finding visual and relevant information in an image is a huge task for Image Retrieval community and a very discussed issue in digital image processing. In fact, image can be extracted from a big collection of images, in the purpose of responding to user\u2019s need. Image Retrieval processes based on classical techniques may not be sufficient to user. For several years, great efforts have been devoted to integrate semantic aspect, in order to enhance relevance of the result and ensure high-level content consideration in image. This paper presents a state of the art of Image Retrieval approaches using graph theory due to the growing interest given to graphs in terms of performance, representation and its ability to ingrate semantic aspect. We review a number of recently available graph-based approaches in Image Retrieval aiming to determine factors adding semantic aspect in Image Retrieval system.","publish_time":1591315200000,"author_summary":" Belahyane, Imane; Mammass, Mouad; Abioui,<br>Hasna; Idarrou, Ali","abstract_summary":" The paper deals with the problem of semantic<br>Image Retrieval. Indeed, the image has recently<br>gained popularity in several domains such as medical<br>domain, marketing, etc. Image plays a very vital role in<br>documentation. However, finding visual and relevant<br>information in an image is a huge task for Image Retrieval<br>community and a very discussed issue in digital image<br>processing. In fact, image can be extracted from a big<br>collection of images, in the purpose of responding to<br>user\u2019s need. Image Retrieval processes based on<br>classical techniques may not be sufficient to user. For<br>several years, great efforts have been...","title_summary":" Graph-Based Image Retrieval: State of the Art","x":-32.6893310547,"y":33.9943161011,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.6893310547,"tsne_y":33.9943161011,"subcluster":21,"subcluster_description":"Cross-Modal Retrievalgraph-Based Image Retrieval","shape":"p"},{"cord_uid":"2mnx7itu","source_x":"PMC","title":"Operational Research Literature as a Use Case for the Open Research Knowledge Graph","doi":"10.1007\/978-3-030-52200-1_32","abstract":"The Open Research Knowledge Graph (ORKG) provides machine-actionable access to scholarly literature that habitually is written in prose. Following the FAIR principles, the ORKG makes traditional, human-coded knowledge findable, accessible, interoperable, and reusable in a structured manner in accordance with the Linked Open Data paradigm. At the moment, in ORKG papers are described manually, but in the long run the semantic depth of the literature at scale needs automation. Operational Research is a suitable test case for this vision because the mathematical field and, hence, its publication habits are highly structured: A mundane problem is formulated as a mathematical model, solved or approximated numerically, and evaluated systematically. We study the existing literature with respect to the Assembly Line Balancing Problem and derive a semantic description in accordance with the ORKG. Eventually, selected papers are ingested to test the semantic description and refine it further.","publish_time":1591401600000,"author_summary":" Runnwerth, Mila; Stocker, Markus; Auer, S\u00f6ren","abstract_summary":" The Open Research Knowledge Graph (ORKG)<br>provides machine-actionable access to scholarly<br>literature that habitually is written in prose. Following<br>the FAIR principles, the ORKG makes traditional,<br>human-coded knowledge findable, accessible,<br>interoperable, and reusable in a structured manner in<br>accordance with the Linked Open Data paradigm. At the<br>moment, in ORKG papers are described manually, but in<br>the long run the semantic depth of the literature at<br>scale needs automation. Operational Research is a<br>suitable test case for this vision because the<br>mathematical field and, hence, its publication habits are<br>highly structured: A mundane problem is formulated as<br>a mathematical model,...","title_summary":" Operational Research Literature as a Use Case<br>for the Open Research Knowledge Graph","x":-33.6556243896,"y":36.4371833801,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.6556243896,"tsne_y":36.4371833801,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"m0hfczyz","source_x":"PMC","title":"Optimizing Hydrography Ontology Alignment Through Compact Particle Swarm Optimization Algorithm","doi":"10.1007\/978-3-030-53956-6_14","abstract":"With the explosive growth in generating data in the hydrographical domain, many hydrography ontologies have been developed and maintained to describe hydrographical features and the relationships between them. However, the existing hydrography ontologies are developed with varying project perspectives and objectives, which inevitably results in the differences in terms of knowledge representation. Determining various relationships between two entities in different ontologies offers the opportunity to link hydrographical data for multiple purposes, though the research on this topic is in its infancy. Different from the traditional ontology alignment whose cardinality is 1:1, i.e. one source ontology entity is mapping with one target ontology entity and vice versa, and the relationship is the equivalence, matching hydrography ontologies is a more complex task, whose cardinality could be 1:1, 1:n or m:n and the relationships could be equivalence or subsumption. To efficiently optimize the ontology alignment, in this paper, a discrete optimal model is first constructed for the ontology matching problem, and then a Compact Particle Swarm Optimization algorithm (CPSO) based matching technique is proposed to efficiently solve it. CPSO utilizes the compact real-value encoding and decoding mechanism and the objective-decomposing strategy to approximate the PSO\u2019s evolving process, which can dramatically reduce PSO\u2019s memory consumption and runtime while at the same time ensure the solution\u2019s quality. The experiment exploits the Hydrography dataset in Complex track provided by the Ontology Alignment Evaluation Initiative (OAEI) to test our proposal\u2019s performance. The experimental results show that CPSO-based approach can effectively reduce PSO\u2019s runtime and memory consumption, and determine high-quality hydrography ontology alignments.","publish_time":1592784000000,"author_summary":" Wang, Yifeng; Yao, Hanguang; Wan, Liangpeng;<br>Li, Hua; Jiang, Junjun; Zhang, Yun; Wu, Fangmin;<br>Chen, Junfeng; Xue, Xingsi; Dai, Cai","abstract_summary":" With the explosive growth in generating data in<br>the hydrographical domain, many hydrography<br>ontologies have been developed and maintained to describe<br>hydrographical features and the relationships between them.<br>However, the existing hydrography ontologies are<br>developed with varying project perspectives and<br>objectives, which inevitably results in the differences in<br>terms of knowledge representation. Determining<br>various relationships between two entities in<br>different ontologies offers the opportunity to link<br>hydrographical data for multiple purposes, though the<br>research on this topic is in its infancy. Different from<br>the traditional ontology alignment whose<br>cardinality is 1:1, i.e. one source ontology entity is<br>mapping with one...","title_summary":" Optimizing Hydrography Ontology Alignment<br>Through Compact Particle Swarm Optimization<br>Algorithm","x":-33.8908004761,"y":36.429485321,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.8908004761,"tsne_y":36.429485321,"subcluster":10,"subcluster_description":"Collaborative Decision Makingmodular Graphical","shape":"p"},{"cord_uid":"ko03vn8l","source_x":"PMC","title":"Developing an automated mechanism to identify medical articles from Wikipedia for knowledge extraction","doi":"10.1016\/j.ijmedinf.2020.104234","abstract":"Wikipedia contains rich biomedical information that can support medical informatics studies and applications. Identifying the subset of medical articles of Wikipedia has many benefits, such as facilitating medical knowledge extraction, serving as a corpus for language modeling, or simply making the size of data easy to work with. However, due to the extremely low prevalence of medical articles in the entire Wikipedia, articles identified by generic text classifiers would be bloated by irrelevant pages. To control the false discovery rate while maintaining a high recall, we developed a mechanism that leverages the rich page elements and the connected nature of Wikipedia and uses a crawling classification strategy to achieve accurate classification. Structured assertional knowledge in Infoboxes and Wikidata items associated with the identified medical articles were also extracted. This automatic mechanism is aimed to run periodically to update the results and share them with the informatics community.","publish_time":1594598400000,"author_summary":" Yu, Lishan; Yu, Sheng","abstract_summary":" Wikipedia contains rich biomedical<br>information that can support medical informatics studies<br>and applications. Identifying the subset of<br>medical articles of Wikipedia has many benefits, such<br>as facilitating medical knowledge extraction,<br>serving as a corpus for language modeling, or simply<br>making the size of data easy to work with. However, due<br>to the extremely low prevalence of medical<br>articles in the entire Wikipedia, articles identified<br>by generic text classifiers would be bloated by<br>irrelevant pages. To control the false discovery rate<br>while maintaining a high recall, we developed a<br>mechanism that leverages the rich page elements and the<br>connected nature of...","title_summary":" Developing an automated mechanism to identify<br>medical articles from Wikipedia for knowledge<br>extraction","x":-32.2622337341,"y":32.53723526,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.2622337341,"tsne_y":32.53723526,"subcluster":6,"subcluster_description":"Pubmed Knowledge Graphbuilding","shape":"p"},{"cord_uid":"bb79b5xx","source_x":"Medline","title":"weg2vec: Event embedding for temporal networks.","doi":"10.1038\/s41598-020-63221-2","abstract":"Network embedding techniques are powerful to capture structural regularities in networks and to identify similarities between their local fabrics. However, conventional network embedding models are developed for static structures, commonly consider nodes only and they are seriously challenged when the network is varying in time. Temporal networks may provide an advantage in the description of real systems, but they code more complex information, which could be effectively represented only by a handful of methods so far. Here, we propose a new method of event embedding of temporal networks, called weg2vec, which builds on temporal and structural similarities of events to learn a low dimensional representation of a temporal network. This projection successfully captures latent structures and similarities between events involving different nodes at different times and provides ways to predict the final outcome of spreading processes unfolding on the temporal structure.","publish_time":1588032000000,"author_summary":" Torricelli, Maddalena; Karsai, M\u00e1rton;<br>Gauvin, Laetitia","abstract_summary":" Network embedding techniques are powerful to<br>capture structural regularities in networks and to<br>identify similarities between their local fabrics.<br>However, conventional network embedding models are<br>developed for static structures, commonly consider<br>nodes only and they are seriously challenged when the<br>network is varying in time. Temporal networks may<br>provide an advantage in the description of real<br>systems, but they code more complex information, which<br>could be effectively represented only by a handful of<br>methods so far. Here, we propose a new method of event<br>embedding of temporal networks, called weg2vec, which<br>builds on temporal and structural similarities of<br>events to...","title_summary":" weg2vec: Event embedding for temporal<br>networks.","x":-30.9229335785,"y":35.6251525879,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.9229335785,"tsne_y":35.6251525879,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"a0bbw3er","source_x":"BioRxiv","title":"A protocol for adding knowledge to Wikidata, a case report","doi":"10.1101\/2020.04.05.026336","abstract":"Pandemics, even more than other medical problems, require swift integration of knowledge. When caused by a new virus, understanding the underlying biology may help finding solutions. In a setting where there are a large number of loosely related projects and initiatives, we need common ground, also known as a \u201ccommons\u201d. Wikidata, a public knowledge graph aligned with Wikipedia, is such a commons and uses unique identifiers to link knowledge in other knowledge bases However, Wikidata may not always have the right schema for the urgent questions. In this paper, we address this problem by showing how a data schema required for the integration can be modelled with entity schemas represented by Shape Expressions. As a telling example, we describe the process of aligning resources on the genomes and proteomes of the SARS-CoV-2 virus and related viruses as well as how Shape Expressions can be defined for Wikidata to model the knowledge, helping others studying the SARS-CoV-2 pandemic. How this model can be used to make data between various resources interoperable, is demonstrated by integrating data from NCBI Taxonomy, NCBI Genes, UniProt, and WikiPathways. Based on that model, a set of automated applications or bots were written for regular updates of these sources in Wikidata and added to a platform for automatically running these updates. Although this workflow is developed and applied in the context of the COVID-19 pandemic, to demonstrate its broader applicability it was also applied to other human coronaviruses (MERS, SARS, Human Coronavirus NL63, Human coronavirus 229E, Human coronavirus HKU1, Human coronavirus OC4).","publish_time":1591228800000,"author_summary":" Waagmeester, Andra; Willighagen, Egon L.; Su,<br>Andrew I; Kutmon, Martina; Labra Gayo, Jose Emilio;<br>Fern\u00e1ndez-\u00c1lvarez, Daniel; Groom, Quentin; Schaap, Peter J.;<br>Verhagen, Lisa M.; Koehorst, Jasper J.","abstract_summary":" Pandemics, even more than other medical<br>problems, require swift integration of knowledge. When<br>caused by a new virus, understanding the underlying<br>biology may help finding solutions. In a setting where<br>there are a large number of loosely related projects<br>and initiatives, we need common ground, also known<br>as a \u201ccommons\u201d. Wikidata, a public knowledge<br>graph aligned with Wikipedia, is such a commons and<br>uses unique identifiers to link knowledge in other<br>knowledge bases However, Wikidata may not always have the<br>right schema for the urgent questions. In this paper,<br>we address this problem by showing how a data<br>schema required...","title_summary":" A protocol for adding knowledge to Wikidata, a<br>case report","x":-33.2643966675,"y":35.7704772949,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.2643966675,"tsne_y":35.7704772949,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"tqnidcv9","source_x":"ArXiv","title":"Mining Implicit Relevance Feedback from User Behavior for Web Question Answering","doi":"10.1145\/3394486.3403343","abstract":"Training and refreshing a web-scale Question Answering (QA) system for a multi-lingual commercial search engine often requires a huge amount of training examples. One principled idea is to mine implicit relevance feedback from user behavior recorded in search engine logs. All previous works on mining implicit relevance feedback target at relevance of web documents rather than passages. Due to several unique characteristics of QA tasks, the existing user behavior models for web documents cannot be applied to infer passage relevance. In this paper, we make the first study to explore the correlation between user behavior and passage relevance, and propose a novel approach for mining training data for Web QA. We conduct extensive experiments on four test datasets and the results show our approach significantly improves the accuracy of passage ranking without extra human labeled data. In practice, this work has proved effective to substantially reduce the human labeling cost for the QA service in a global commercial search engine, especially for languages with low resources. Our techniques have been deployed in multi-language services.","publish_time":1592006400000,"author_summary":" Shou, Linjun; Bo, Shining; Cheng, Feixiang;<br>Gong, Ming; Pei, Jian; Jiang, Daxin","abstract_summary":" Training and refreshing a web-scale Question<br>Answering (QA) system for a multi-lingual commercial<br>search engine often requires a huge amount of training<br>examples. One principled idea is to mine implicit<br>relevance feedback from user behavior recorded in search<br>engine logs. All previous works on mining implicit<br>relevance feedback target at relevance of web documents<br>rather than passages. Due to several unique<br>characteristics of QA tasks, the existing user behavior models<br>for web documents cannot be applied to infer<br>passage relevance. In this paper, we make the first<br>study to explore the correlation between user<br>behavior and passage relevance, and propose...","title_summary":" Mining Implicit Relevance Feedback from User<br>Behavior for Web Question Answering","x":-33.959072113,"y":34.0165290833,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.959072113,"tsne_y":34.0165290833,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"q8tz529g","source_x":"MedRxiv","title":"A Light-weight Text Summarizer for Fast Access to Medical Evidence","doi":"10.1101\/2020.05.22.20110742","abstract":"The performances of current medical text summarization systems rely on resource-heavy domain-specific knowledge sources, and preprocessing methods (e.g., classification or deep learning) for deriving semantic information. Consequently, these systems are often difficult to customize, extend or deploy in low-resource settings, and are operationally slow. We propose a fast summarization system that can aid practitioners at point-of-care, and, thus, improve evidence-based healthcare. At runtime, our system utilizes similarity measurements derived from pre-trained domain-specific word embeddings in addition to simple features, rather than clunky knowledge bases and resource-heavy preprocessing. Automatic evaluation on a public dataset for evidence-based medicine shows that our system's performance, despite the simple implementation, is statistically comparable with the state-of-the-art.","publish_time":1590451200000,"author_summary":" Sarker, A.; Yang, Y.-C.; Al-Garadi, M. A.","abstract_summary":" The performances of current medical text<br>summarization systems rely on resource-heavy<br>domain-specific knowledge sources, and preprocessing methods<br>(e.g., classification or deep learning) for deriving<br>semantic information. Consequently, these systems are<br>often difficult to customize, extend or deploy in<br>low-resource settings, and are operationally slow. We<br>propose a fast summarization system that can aid<br>practitioners at point-of-care, and, thus, improve<br>evidence-based healthcare. At runtime, our system utilizes<br>similarity measurements derived from pre-trained<br>domain-specific word embeddings in addition to simple<br>features, rather than clunky knowledge bases and<br>resource-heavy preprocessing. Automatic evaluation on a<br>public dataset for evidence-based medicine shows<br>that our system's...","title_summary":" A Light-weight Text Summarizer for Fast Access<br>to Medical Evidence","x":-32.2575950623,"y":32.1556739807,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.2575950623,"tsne_y":32.1556739807,"subcluster":9,"subcluster_description":"Large-Scale Biomedical Semantic Indexing","shape":"p"},{"cord_uid":"tsv3qlpd","source_x":"BioRxiv","title":"SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search","doi":"10.1101\/2020.05.23.112284","abstract":"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, already generating thousands of new papers that join a litany of previous biomedical work in related areas. This deluge of information makes it hard for researchers to keep track of their own field, let alone explore new directions. Standard search engines are designed primarily for targeted search and are not geared for discovery or making connections that are not obvious from reading individual papers. In this paper, we present our ongoing work on SciSight, a novel framework for exploratory search of COVID-19 research. Based on formative interviews with scientists and a review of existing tools, we build and integrate two key capabilities: first, exploring interactions between biomedical facets (e.g., proteins, genes, drugs, diseases, patient characteristics); and second, discovering groups of researchers and how they are connected. We extract entities using a language model pre-trained on several biomedical information extraction tasks, and enrich them with data from the Microsoft Academic Graph (MAG). To find research groups automatically, we use hierarchical clustering with overlap to allow authors, as they do, to belong to multiple groups. Finally, we introduce a novel presentation of these groups based on both topical and social affinities, allowing users to drill down from groups to papers to associations between entities, and update query suggestions on the fly with the goal of facilitating exploratory navigation. SciSight1 has thus far served over 10K users with over 30K page views and 13% returning users. Preliminary user interviews with biomedical researchers suggest that SciSight complements current approaches and helps find new and relevant knowledge.","publish_time":1590451200000,"author_summary":" Hope, Tom; Portenoy, Jason; Vasan, Kishore;<br>Borchardt, Jonathan; Horvitz, Eric; Weld, Daniel S.;<br>Hearst, Marti A.; West, Jevin","abstract_summary":" The COVID-19 pandemic has sparked<br>unprecedented mobilization of scientists, already<br>generating thousands of new papers that join a litany of<br>previous biomedical work in related areas. This deluge<br>of information makes it hard for researchers to<br>keep track of their own field, let alone explore new<br>directions. Standard search engines are designed<br>primarily for targeted search and are not geared for<br>discovery or making connections that are not obvious from<br>reading individual papers. In this paper, we present<br>our ongoing work on SciSight, a novel framework for<br>exploratory search of COVID-19 research. Based on<br>formative interviews with scientists and a...","title_summary":" SciSight: Combining faceted navigation and<br>research group detection for COVID-19 exploratory<br>scientific search","x":-30.6605319977,"y":30.97930336,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6605319977,"tsne_y":30.97930336,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"f21dknmb","source_x":"MedRxiv","title":"TWIRLS, an automated topic-wise inference method based on massive literature, suggests a possible mechanism via ACE2 for the pathological changes in the human host after coronavirus infection","doi":"10.1101\/2020.02.24.20025437","abstract":"Faced with the current large-scale public health emergency, collecting, sorting, and analyzing biomedical information related to the \"coronavirus\" should be done as quickly as possible to gain a global perspective, which is a basic requirement for strengthening epidemic control capacity. However, for human researchers studying the viruses and the hosts, the vast amount of information available cannot be processed effectively and in a timely manner, particularly when the scientific understanding may be limited, which can further lower the information processing efficiency. We present TWIRLS, a method that can automatically acquire, organize, and classify information. Additionally, independent functional data sources can be added to build an inference system using a machine-based approach, which can provide relevant knowledge to help human researchers quickly establish subject cognition and to make more effective decisions. TWIRLS can automatically analyze more than three million words in more than 14,000 literature articles in only 4 hours. Combining with generalized gene interaction databases creates a data interface that can help researchers to further analyze the information. Using the TWIRLS system, we found that an important regulatory factor angiotensin-converting enzyme 2 (ACE2) may be involved in the host pathological changes on binding to the coronavirus after infection. After triggering functional changes in ACE2\/AT2R, an imbalance in the steady-state cytokine regulatory axis involving the Renin-Angiotensin System and IP-10 leads to a cytokine storm.","publish_time":1582675200000,"author_summary":" Ji, Xiaoyang; Zhang, Chunming; Zhai, Yubo;<br>Zhang, Zhonghai; Xue, Yiqing; Zhang, Chunli; Tan,<br>Guangming; Niu, Gang","abstract_summary":" Faced with the current large-scale public<br>health emergency, collecting, sorting, and<br>analyzing biomedical information related to the<br>\"coronavirus\" should be done as quickly as possible to gain a<br>global perspective, which is a basic requirement for<br>strengthening epidemic control capacity. However, for human<br>researchers studying the viruses and the hosts, the vast<br>amount of information available cannot be processed<br>effectively and in a timely manner, particularly when the<br>scientific understanding may be limited, which can<br>further lower the information processing efficiency.<br>We present TWIRLS, a method that can<br>automatically acquire, organize, and classify information.<br>Additionally, independent functional data sources can...","title_summary":" TWIRLS, an automated topic-wise inference<br>method based on massive literature, suggests a<br>possible mechanism via ACE2 for the pathological<br>changes in the human host after coronavirus infection","x":-29.9613361359,"y":31.3829441071,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-29.9613361359,"tsne_y":31.3829441071,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ovv34qxm","source_x":"BioRxiv","title":"Applying Lexical Link Analysis to Discover Insights from Public Information on COVID-19","doi":"10.1101\/2020.05.06.079798","abstract":"SARS-Cov-2, the deadly and novel virus, which has caused a worldwide pandemic and drastic loss of human lives and economic activities. An open data set called the COVID-19 Open Research Dataset or CORD-19 contains large set full text scientific literature on SARS-CoV-2. The Next Strain consists of a database of SARS-CoV-2 viral genomes from since 12\/3\/2019. We applied an unique information mining method named lexical link analysis (LLA) to answer the call to action and help the science community answer high-priority scientific questions related to SARS-CoV-2. We first text-mined the CORD-19. We also data-mined the next strain database. Finally, we linked two databases. The linked databases and information can be used to discover the insights and help the research community to address high-priority questions related to the SARS-CoV-2\u2019s genetics, tests, and prevention. Significance Statement In this paper, we show how to apply an unique information mining method lexical link analysis (LLA) to link unstructured (CORD-19) and structured (Next Strain) data sets to relevant publications, integrate text and data mining into a single platform to discover the insights that can be visualized, and validated to answer the high-priority questions of genetics, incubation, treatment, symptoms, and prevention of COVID-19.","publish_time":1588723200000,"author_summary":" Zhao, Ying; Zhou, Charles C.","abstract_summary":" SARS-Cov-2, the deadly and novel virus, which<br>has caused a worldwide pandemic and drastic loss of<br>human lives and economic activities. An open data set<br>called the COVID-19 Open Research Dataset or CORD-19<br>contains large set full text scientific literature on<br>SARS-CoV-2. The Next Strain consists of a database of<br>SARS-CoV-2 viral genomes from since 12\/3\/2019. We applied<br>an unique information mining method named<br>lexical link analysis (LLA) to answer the call to action<br>and help the science community answer<br>high-priority scientific questions related to SARS-CoV-2.<br>We first text-mined the CORD-19. We also<br>data-mined the next strain database. Finally, we...","title_summary":" Applying Lexical Link Analysis to Discover<br>Insights from Public Information on COVID-19","x":-30.3894805908,"y":30.850276947,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.3894805908,"tsne_y":30.850276947,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"g53it2al","source_x":"MedRxiv","title":"CovidNLP: A Web Application for Distilling Systemic Implications of COVID-19 Pandemic with Natural Language Processing","doi":"10.1101\/2020.04.25.20079129","abstract":"The flood of conflicting COVID-19 research has revealed that COVID-19 continues to be an enigma. Although more than 14,000 research articles on COVID-19 have been published with the disease taking a pandemic proportion, clinicians and researchers are struggling to distill knowledge for furthering clinical management and research. In this study, we address this gap for a targeted user group, i.e. clinicians, researchers, and policymakers by applying natural language processing to develop a CovidNLP dashboard in order to speed up knowledge discovery. The WHO has created a repository of about more than 5000 peer-reviewed and curated research articles on varied aspects including epidemiology, clinical features, diagnosis, treatment, social factors, and economics. We summarised all the articles in the WHO Database through an extractive summarizer followed by an exploration of the feature space using word embeddings which were then used to visualize the summarized associations of COVID-19 as found in the text. Clinicians, researchers, and policymakers will not only discover the direct effects of COVID-19 but also the systematic implications such as the anticipated rise in TB and cancer mortality due to the non-availability of drugs during the export lockdown as highlighted by our models. These demonstrate the utility of mining massive literature with natural language processing for rapid distillation and knowledge updates. This can help the users understand, synthesize, and take pre-emptive action with the available peer-reviewed evidence on COVID-19. Our models will be continuously updated with new literature and we have made our resource CovidNLP publicly available in a user-friendly fashion at http:\/\/covidnlp.tavlab.iiitd.edu.in\/.","publish_time":1588118400000,"author_summary":" Awasthi, R.; Pal, R.; Singh, P.; Nagori, A.;<br>Reddy, S.; Gulati, A.; Kumaraguru, P.; Sethi, T.","abstract_summary":" The flood of conflicting COVID-19 research has<br>revealed that COVID-19 continues to be an enigma.<br>Although more than 14,000 research articles on COVID-19<br>have been published with the disease taking a<br>pandemic proportion, clinicians and researchers are<br>struggling to distill knowledge for furthering clinical<br>management and research. In this study, we address this gap<br>for a targeted user group, i.e. clinicians,<br>researchers, and policymakers by applying natural language<br>processing to develop a CovidNLP dashboard in order to<br>speed up knowledge discovery. The WHO has created a<br>repository of about more than 5000 peer-reviewed and<br>curated research articles on varied aspects...","title_summary":" CovidNLP: A Web Application for Distilling<br>Systemic Implications of COVID-19 Pandemic with<br>Natural Language Processing","x":-30.29545784,"y":30.2359313965,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.29545784,"tsne_y":30.2359313965,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"aeyf0yu1","source_x":"BioRxiv","title":"deepMINE - Natural Language Processing based Automatic Literature Mining and Research Summarization for Early-Stage Comprehension in Pandemic Situations specifically for COVID-19","doi":"10.1101\/2020.03.30.014555","abstract":"The recent pandemic created due to Novel Coronavirus (nCOV-2019) from Wuhan, China demanding a large scale of a general health emergency. This demands novel research on the vaccine to fight against this pandemic situation, re-purposing of the existing drugs, phylogenetic analysis to identify the origin and determine the similarity with other known viruses, etc. The very preliminary task from the research community is to analyze the wide verities of existing related research articles, which is very much time-consuming in such situations where each minute counts for saving hundreds of human lives. The entire manual processing is even lower down the efficiency in mining the information. We have developed a complete automatic literature mining system that delivers efficient and fast mining from existing biomedical literature databases. With the help of modern-day deep learning algorithms, our system also delivers a summarization of important research articles that provides ease and fast comprehension of critical research articles. The system is currently scanning nearly 1,46,115,136 English words from 29,315 research articles in not greater than 1.5 seconds with multiple search keywords. Our research article presents the criticality of literature mining, especially in pandemic situations with the implementation and online deployment of the system.","publish_time":1585785600000,"author_summary":" Joshi, Bhrugesh; Bakarola, Vishvajit; Shah,<br>Parth; Krishnamurthy, Ramar","abstract_summary":" The recent pandemic created due to Novel<br>Coronavirus (nCOV-2019) from Wuhan, China demanding a<br>large scale of a general health emergency. This<br>demands novel research on the vaccine to fight against<br>this pandemic situation, re-purposing of the<br>existing drugs, phylogenetic analysis to identify the<br>origin and determine the similarity with other known<br>viruses, etc. The very preliminary task from the<br>research community is to analyze the wide verities of<br>existing related research articles, which is very much<br>time-consuming in such situations where each minute counts for<br>saving hundreds of human lives. The entire manual<br>processing is even lower down the...","title_summary":" deepMINE - Natural Language Processing based<br>Automatic Literature Mining and Research Summarization<br>for Early-Stage Comprehension in Pandemic<br>Situations specifically for COVID-19","x":-30.182554245,"y":31.1538391113,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.182554245,"tsne_y":31.1538391113,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"3j4atdda","source_x":"BioRxiv","title":"Machine Learning Maps Research Needs in COVID-19 Literature","doi":"10.1101\/2020.06.11.145425","abstract":"Summary Manually assessing the scope of the thousands of publications on the COVID-19 (coronavirus disease 2019) pandemic is an overwhelming task. Shortcuts through metadata analysis (e.g., keywords) assume that studies are properly tagged. However, machine learning approaches can rapidly survey the actual text of coronavirus abstracts to identify research overlap between COVID-19 and other coronavirus diseases, research hotspots, and areas warranting exploration. We propose a fast, scalable, and reusable framework to parse novel disease literature. When applied to the COVID-19 Open Research Dataset (CORD-19), dimensionality reduction suggested that COVID-19 studies to date are primarily clinical-, modeling- or field-based, in contrast to the vast quantity of laboratory-driven research for other (non-COVID-19) coronavirus diseases. Topic modeling also indicated that COVID-19 publications have thus far focused primarily on public health, outbreak reporting, clinical care, and testing for coronaviruses, as opposed to the more limited number focused on basic microbiology, including pathogenesis and transmission.","publish_time":1591920000000,"author_summary":" Doanvo, Anhvinh; Qian, Xiaolu; Ramjee, Divya;<br>Piontkivska, Helen; Desai, Angel; Majumder, Maimuna","abstract_summary":" Summary Manually assessing the scope of the<br>thousands of publications on the COVID-19 (coronavirus<br>disease 2019) pandemic is an overwhelming task.<br>Shortcuts through metadata analysis (e.g., keywords)<br>assume that studies are properly tagged. However,<br>machine learning approaches can rapidly survey the<br>actual text of coronavirus abstracts to identify<br>research overlap between COVID-19 and other<br>coronavirus diseases, research hotspots, and areas<br>warranting exploration. We propose a fast, scalable, and<br>reusable framework to parse novel disease literature.<br>When applied to the COVID-19 Open Research Dataset<br>(CORD-19), dimensionality reduction suggested that<br>COVID-19 studies to date are primarily clinical-,<br>modeling- or field-based, in contrast...","title_summary":" Machine Learning Maps Research Needs in<br>COVID-19 Literature","x":-30.4535427094,"y":30.7078876495,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.4535427094,"tsne_y":30.7078876495,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"pt8nh7wx","source_x":"Medline","title":"CORD-19: The Covid-19 Open Research Dataset.","doi":null,"abstract":"The Covid-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on Covid-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 75K times and has served as the basis of many Covid-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and preview tools and upcoming shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for Covid-19.","publish_time":1587513600000,"author_summary":" Lu Wang, Lucy; Lo, Kyle; Chandrasekhar,<br>Yoganand; Reas, Russell; Yang, Jiangjiang; Eide,<br>Darrin; Funk, Kathryn; Kinney, Rodney; Liu, Ziyang;<br>Merrill, William; Mooney, Paul; Murdick, Dewey; Rishi,<br>Devvret; Sheehan, Jerry; Shen, Zhihong; Stilson,<br>Brandon; Wade, Alex D; Wang, Kuansan; Wilhelm, Chris;<br>Xie, Boya; Raymond, Douglas; Weld, Daniel S;<br>Etzioni, Oren; Kohlmeier, Sebastian","abstract_summary":" The Covid-19 Open Research Dataset (CORD-19)<br>is a growing resource of scientific papers on<br>Covid-19 and related historical coronavirus research.<br>CORD-19 is designed to facilitate the development of<br>text mining and information retrieval systems over<br>its rich collection of metadata and structured<br>full text papers. Since its release, CORD-19 has<br>been downloaded over 75K times and has served as the<br>basis of many Covid-19 text mining and discovery<br>systems. In this article, we describe the mechanics of<br>dataset construction, highlighting challenges and<br>key design decisions, provide an overview of how<br>CORD-19 has been used, and preview tools and upcoming<br>shared...","title_summary":" CORD-19: The Covid-19 Open Research Dataset.","x":-30.3910083771,"y":30.7219867706,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.3910083771,"tsne_y":30.7219867706,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"lz7mse5q","source_x":"Medline","title":"A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19.","doi":null,"abstract":"COVID-19 has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf, BERT, BioBERT, and USE to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot, we created a user-friendly interactive web application to be hosted online.","publish_time":1592524800000,"author_summary":" Oniani, David; Wang, Yanshan","abstract_summary":" COVID-19 has resulted in an ongoing pandemic<br>and as of 12 June 2020, has caused more than 7.4<br>million cases and over 418,000 deaths. The highly<br>dynamic and rapidly evolving situation with COVID-19<br>has made it difficult to access accurate,<br>on-demand information regarding the disease. Online<br>communities, forums, and social media provide potential<br>venues to search for relevant questions and answers,<br>or post questions and seek answers from other<br>members. However, due to the nature of such sites, there<br>are always a limited number of relevant questions<br>and responses to search from, and posted questions<br>are rarely answered immediately. With...","title_summary":" A Qualitative Evaluation of Language Models on<br>Automatic Question-Answering for COVID-19.","x":-31.2282180786,"y":30.6562156677,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2282180786,"tsne_y":30.6562156677,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"3xwbq8ov","source_x":"ArXiv","title":"Getting Insights from a Large Corpus of Scientific Papers on Specialisted Comprehensive Topics -- the Case of COVID-19","doi":null,"abstract":"COVID-19 is one of the most important topic these days, specifically on search engines and news. While fake news are easily shared, scientific papers are reliable sources where information can be extracted. With about 24,000 scientific publications on COVID-19 and related research on PUBMED, automatic computer-assisted analysis is required. In this paper, we develop two methodologies to get insights on specific sub-topics of interest and latest research sub-topics. They rely on natural language processing and graph-based visualizations. We run these methodologies on two cases: the virus origin and the uses of existing drugs.","publish_time":1588204800000,"author_summary":" Dousset, Bernard; Mothe, Josiane","abstract_summary":" COVID-19 is one of the most important topic<br>these days, specifically on search engines and news.<br>While fake news are easily shared, scientific papers<br>are reliable sources where information can be<br>extracted. With about 24,000 scientific publications on<br>COVID-19 and related research on PUBMED, automatic<br>computer-assisted analysis is required. In this paper, we develop<br>two methodologies to get insights on specific<br>sub-topics of interest and latest research sub-topics.<br>They rely on natural language processing and<br>graph-based visualizations. We run these methodologies on<br>two cases: the virus origin and the uses of existing<br>drugs.","title_summary":" Getting Insights from a Large Corpus of<br>Scientific Papers on Specialisted Comprehensive Topics<br>-- the Case of COVID-19","x":-30.5661087036,"y":30.4805717468,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.5661087036,"tsne_y":30.4805717468,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"95o2v09d","source_x":"ArXiv","title":"Google Dataset Search by the Numbers","doi":null,"abstract":"Scientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadata -- expressed using schema.org and similar vocabularies -- from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.","publish_time":1591920000000,"author_summary":" Benjelloun, Omar; Chen, Shiyu; Noy, Natasha","abstract_summary":" Scientists, governments, and companies<br>increasingly publish datasets on the Web. Google's Dataset<br>Search extracts dataset metadata -- expressed using<br>schema.org and similar vocabularies -- from Web pages in<br>order to make datasets discoverable. Since we<br>started the work on Dataset Search in 2016, the number of<br>datasets described in schema.org has grown from about<br>500K to almost 30M. Thus, this corpus has become a<br>valuable snapshot of data on the Web. To the best of our<br>knowledge, this corpus is the largest and most diverse of<br>its kind. We analyze this corpus and discuss where<br>the datasets originate from, what...","title_summary":" Google Dataset Search by the Numbers","x":-33.5909233093,"y":34.9942054749,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.5909233093,"tsne_y":34.9942054749,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"0m9yngn2","source_x":"ArXiv","title":"A Semantic Web Framework for Automated Smart Assistants: COVID-19 Case Study","doi":null,"abstract":"COVID-19 pandemic elucidated that knowledge systems will be instrumental in cases where accurate information needs to be communicated to a substantial group of people with different backgrounds and technological resources. However, several challenges and obstacles hold back the wide adoption of virtual assistants by public health departments and organizations. This paper presents the Instant Expert, an open-source semantic web framework to build and integrate voice-enabled smart assistants (i.e. chatbots) for any web platform regardless of the underlying domain and technology. The component allows non-technical domain experts to effortlessly incorporate an operational assistant with voice recognition capability into their websites. Instant Expert is capable of automatically parsing, processing, and modeling Frequently Asked Questions pages as an information resource as well as communicating with an external knowledge engine for ontology-powered inference and dynamic data utilization. The presented framework utilizes advanced web technologies to ensure reusability and reliability, and an inference engine for natural language understanding powered by deep learning and heuristic algorithms. A use case for creating an informatory assistant for COVID-19 based on the Centers for Disease Control and Prevention (CDC) data is presented to demonstrate the framework's usage and benefits.","publish_time":1593561600000,"author_summary":" Sermet, Yusuf; Demir, Ibrahim","abstract_summary":" COVID-19 pandemic elucidated that knowledge<br>systems will be instrumental in cases where accurate<br>information needs to be communicated to a substantial group<br>of people with different backgrounds and<br>technological resources. However, several challenges and<br>obstacles hold back the wide adoption of virtual<br>assistants by public health departments and<br>organizations. This paper presents the Instant Expert, an<br>open-source semantic web framework to build and integrate<br>voice-enabled smart assistants (i.e. chatbots) for any web<br>platform regardless of the underlying domain and<br>technology. The component allows non-technical domain<br>experts to effortlessly incorporate an operational<br>assistant with voice recognition capability into their<br>websites. Instant...","title_summary":" A Semantic Web Framework for Automated Smart<br>Assistants: COVID-19 Case Study","x":-30.5350170135,"y":31.4798851013,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.5350170135,"tsne_y":31.4798851013,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"8lu58q4k","source_x":"ArXiv","title":"A Semantically Enriched Dataset based on Biomedical NER for the COVID19 Open Research Dataset Challenge","doi":null,"abstract":"Research into COVID-19 is a big challenge and highly relevant at the moment. New tools are required to assist medical experts in their research with relevant and valuable information. The COVID-19 Open Research Dataset Challenge (CORD-19) is a\"call to action\"for computer scientists to develop these innovative tools. Many of these applications are empowered by entity information, i. e. knowing which entities are used within a sentence. For this paper, we have developed a pipeline upon the latest Named Entity Recognition tools for Chemicals, Diseases, Genes and Species. We apply our pipeline to the COVID-19 research challenge and share the resulting entity mentions with the community.","publish_time":1589760000000,"author_summary":" Kroll, Hermann; Pirklbauer, Jan; Ruthmann,<br>Johannes; Balke, Wolf-Tilo","abstract_summary":" Research into COVID-19 is a big challenge and<br>highly relevant at the moment. New tools are required<br>to assist medical experts in their research with<br>relevant and valuable information. The COVID-19 Open<br>Research Dataset Challenge (CORD-19) is a\"call to<br>action\"for computer scientists to develop these<br>innovative tools. Many of these applications are<br>empowered by entity information, i. e. knowing which<br>entities are used within a sentence. For this paper, we<br>have developed a pipeline upon the latest Named<br>Entity Recognition tools for Chemicals, Diseases,<br>Genes and Species. We apply our pipeline to the<br>COVID-19 research challenge and share the resulting...","title_summary":" A Semantically Enriched Dataset based on<br>Biomedical NER for the COVID19 Open Research Dataset<br>Challenge","x":-30.7621974945,"y":31.054561615,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.7621974945,"tsne_y":31.054561615,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"0c992hzv","source_x":"ArXiv","title":"Self-supervised edge features for improved Graph Neural Network training","doi":null,"abstract":"Graph Neural Networks (GNN) have been extensively used to extract meaningful representations from graph structured data and to perform predictive tasks such as node classification and link prediction. In recent years, there has been a lot of work incorporating edge features along with node features for prediction tasks. One of the main difficulties in using edge features is that they are often handcrafted, hard to get, specific to a particular domain, and may contain redundant information. In this work, we present a framework for creating new edge features, applicable to any domain, via a combination of self-supervised and unsupervised learning. In addition to this, we use Forman-Ricci curvature as an additional edge feature to encapsulate the local geometry of the graph. We then encode our edge features via a Set Transformer and combine them with node features extracted from popular GNN architectures for node classification in an end-to-end training scheme. We validate our work on three biological datasets comprising of single-cell RNA sequencing data of neurological disease, \\textit{in vitro} SARS-CoV-2 infection, and human COVID-19 patients. We demonstrate that our method achieves better performance on node classification tasks over baseline Graph Attention Network (GAT) and Graph Convolutional Network (GCN) models. Furthermore, given the attention mechanism on edge and node features, we are able to interpret the cell types and genes that determine the course and severity of COVID-19, contributing to a growing list of potential disease biomarkers and therapeutic targets.","publish_time":1592870400000,"author_summary":" Sehanobish, Arijit; Ravindra, Neal G.; Dijk,<br>David van","abstract_summary":" Graph Neural Networks (GNN) have been<br>extensively used to extract meaningful representations<br>from graph structured data and to perform<br>predictive tasks such as node classification and link<br>prediction. In recent years, there has been a lot of work<br>incorporating edge features along with node features for<br>prediction tasks. One of the main difficulties in using<br>edge features is that they are often handcrafted,<br>hard to get, specific to a particular domain, and may<br>contain redundant information. In this work, we<br>present a framework for creating new edge features,<br>applicable to any domain, via a combination of<br>self-supervised and unsupervised learning....","title_summary":" Self-supervised edge features for improved<br>Graph Neural Network training","x":-31.2548904419,"y":35.7852172852,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2548904419,"tsne_y":35.7852172852,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"40ah0a27","source_x":"ArXiv","title":"Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction","doi":null,"abstract":"Many practical graph problems, such as knowledge graph construction and drug-to-drug interaction, require to handle multi-relational graphs. However, handling real-world multi-label graphs with Graph Neural Networks (GNNs) is often challenging due to their evolving nature, where new entities (nodes) can emerge over time. Moreover, newly emerged entities often have few links, which makes the learning even more difficult. Motivated by this challenge, we introduce a realistic problem of few-shot out-of-graph link prediction, where we not only predict the links between the seen and unseen nodes as in a conventional out-of-knowledge link prediction but also between the unseen nodes, with only few edges per node. We tackle this problem with a novel transductive meta-learning framework which we refer to as Graph Extrapolation Networks (GEN). GEN meta-learns both the node embedding network for inductive inference (seen-to-unseen) and the link prediction network for transductive inference (unseen-to-unseen). For transductive link prediction, we further propose a stochastic embedding layer to model uncertainty in the link prediction between unseen entities. We validate our model on multiple benchmark datasets for knowledge graph link prediction and drug-to-drug interaction prediction. The results show that our model significantly outperforms relevant baselines for out-of-graph link prediction tasks.","publish_time":1591833600000,"author_summary":" Baek, Jinheon; Lee, Dong Bok; Hwang, Sung Ju","abstract_summary":" Many practical graph problems, such as<br>knowledge graph construction and drug-to-drug<br>interaction, require to handle multi-relational graphs.<br>However, handling real-world multi-label graphs with<br>Graph Neural Networks (GNNs) is often challenging<br>due to their evolving nature, where new entities<br>(nodes) can emerge over time. Moreover, newly emerged<br>entities often have few links, which makes the learning<br>even more difficult. Motivated by this challenge,<br>we introduce a realistic problem of few-shot<br>out-of-graph link prediction, where we not only predict the<br>links between the seen and unseen nodes as in a<br>conventional out-of-knowledge link prediction but also<br>between the unseen nodes, with only...","title_summary":" Learning to Extrapolate Knowledge:<br>Transductive Few-shot Out-of-Graph Link Prediction","x":-31.4116687775,"y":35.616317749,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.4116687775,"tsne_y":35.616317749,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"osukknqr","source_x":"ArXiv","title":"Informational Space of Meaning for Scientific Texts","doi":null,"abstract":"In Natural Language Processing, automatic extracting the meaning of texts constitutes an important problem. Our focus is the computational analysis of meaning of short scientific texts (abstracts or brief reports). In this paper, a vector space model is developed for quantifying the meaning of words and texts. We introduce the Meaning Space, in which the meaning of a word is represented by a vector of Relative Information Gain (RIG) about the subject categories that the text belongs to, which can be obtained from observing the word in the text. This new approach is applied to construct the Meaning Space based on Leicester Scientific Corpus (LSC) and Leicester Scientific Dictionary-Core (LScDC). The LSC is a scientific corpus of 1,673,350 abstracts and the LScDC is a scientific dictionary which words are extracted from the LSC. Each text in the LSC belongs to at least one of 252 subject categories of Web of Science (WoS). These categories are used in construction of vectors of information gains. The Meaning Space is described and statistically analysed for the LSC with the LScDC. The usefulness of the proposed representation model is evaluated through top-ranked words in each category. The most informative n words are ordered. We demonstrated that RIG-based word ranking is much more useful than ranking based on raw word frequency in determining the science-specific meaning and importance of a word. The proposed model based on RIG is shown to have ability to stand out topic-specific words in categories. The most informative words are presented for 252 categories. The new scientific dictionary and the 103,998 x 252 Word-Category RIG Matrix are available online. Analysis of the Meaning Space provides us with a tool to further explore quantifying the meaning of a text using more complex and context-dependent meaning models that use co-occurrence of words and their combinations.","publish_time":1588032000000,"author_summary":" Suzen, Neslihan; Mirkes, Evgeny M.; Gorban,<br>Alexander N.","abstract_summary":" In Natural Language Processing, automatic<br>extracting the meaning of texts constitutes an important<br>problem. Our focus is the computational analysis of<br>meaning of short scientific texts (abstracts or brief<br>reports). In this paper, a vector space model is developed<br>for quantifying the meaning of words and texts. We<br>introduce the Meaning Space, in which the meaning of a word<br>is represented by a vector of Relative<br>Information Gain (RIG) about the subject categories that<br>the text belongs to, which can be obtained from<br>observing the word in the text. This new approach is<br>applied to construct the Meaning Space based...","title_summary":" Informational Space of Meaning for Scientific<br>Texts","x":-33.544002533,"y":32.2005004883,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.544002533,"tsne_y":32.2005004883,"subcluster":17,"subcluster_description":"Aspect Miningaspect Term Extraction","shape":"p"},{"cord_uid":"wxkog4oi","source_x":"ArXiv","title":"Fact or Fiction: Verifying Scientific Claims","doi":null,"abstract":"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that supports or refutes a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that these models benefit from combined training on a large dataset of claims about Wikipedia articles, together with the new SciFact data. We show that our claim verification system is able to identify plausible evidence for 23 \/ 36 claims relevant to COVID-19 on the CORD-19 corpus. Our results and experiments strongly suggest that our new task and data will support significant future research efforts.","publish_time":1588204800000,"author_summary":" Wadden, David; Lin, Shanchuan; Lo, Kyle; Wang,<br>Lucy Lu; Zuylen, Madeleine van; Cohan, Arman;<br>Hajishirzi, Hannaneh","abstract_summary":" We introduce scientific claim verification, a<br>new task to select abstracts from the research<br>literature containing evidence that supports or refutes a<br>given scientific claim, and to identify rationales<br>justifying each decision. To study this task, we construct<br>SciFact, a dataset of 1.4K expert-written scientific<br>claims paired with evidence-containing abstracts<br>annotated with labels and rationales. We develop<br>baseline models for SciFact, and demonstrate that these<br>models benefit from combined training on a large<br>dataset of claims about Wikipedia articles, together<br>with the new SciFact data. We show that our claim<br>verification system is able to identify plausible evidence<br>for 23...","title_summary":" Fact or Fiction: Verifying Scientific Claims","x":-35.2607879639,"y":34.179977417,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.2607879639,"tsne_y":34.179977417,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"bm99zrxz","source_x":"ArXiv","title":"What Are People Asking About COVID-19? A Question Classification Dataset","doi":null,"abstract":"We present COVID-Q, a set of 1,690 questions about COVID-19 from 13 sources, which we annotate into 15 question categories and 207 question classes. The most common questions in our dataset asked about transmission, prevention, and societal effects of COVID, and we found that many questions that appeared in multiple sources were not answered by any FAQ websites of reputable organizations such as the CDC and FDA. We post our dataset publicly at https:\/\/github.com\/JerryWei03\/COVID-Q . For classifying questions into 15 categories, a BERT baseline scored 58.1% accuracy when trained on 20 examples per class, and for classifying questions into 89 question classes, the baseline achieved 54.6% accuracy. We hope COVID-Q can be helpful either for direct use in developing applied systems or as a domain-specific resource for model evaluation.","publish_time":1590451200000,"author_summary":" Wei, Jerry; Huang, Chengyu; Vosoughi,<br>Soroush; Wei, Jason","abstract_summary":" We present COVID-Q, a set of 1,690 questions<br>about COVID-19 from 13 sources, which we annotate<br>into 15 question categories and 207 question<br>classes. The most common questions in our dataset asked<br>about transmission, prevention, and societal<br>effects of COVID, and we found that many questions that<br>appeared in multiple sources were not answered by any FAQ<br>websites of reputable organizations such as the CDC and<br>FDA. We post our dataset publicly at<br>https:\/\/github.com\/JerryWei03\/COVID-Q . For classifying questions into 15<br>categories, a BERT baseline scored 58.1% accuracy when<br>trained on 20 examples per class, and for classifying<br>questions into 89...","title_summary":" What Are People Asking About COVID-19? A<br>Question Classification Dataset","x":-34.6761550903,"y":32.925327301,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6761550903,"tsne_y":32.925327301,"subcluster":24,"subcluster_description":"Benchmarksemi-Supervised Extractive Question Summarization","shape":"p"},{"cord_uid":"elphxl9s","source_x":"ArXiv","title":"Document Classification for COVID-19 Literature","doi":null,"abstract":"The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a growing collection of 8,000 research papers regarding the novel 2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that the BioBERT and novel Longformer models surpass all others with almost equivalent micro-F1 and accuracy scores of around 81% and 69% on the test set. We evaluate the data efficiency and generalizability of these models as essential features of any system prepared to deal with an urgent situation like the current health crisis. Finally, we explore 50 errors made by the best performing models on LitCovid documents and find that they often (1) correlate certain labels too closely together and (2) fail to focus on discriminative sections of the articles; both of which are important issues to address in future work. Both data and code are available on GitHub.","publish_time":1592179200000,"author_summary":" Guti'errez, Bernal Jim'enez; Zeng, Juncheng;<br>Zhang, Dongdong; Zhang, Ping; Su, Yu","abstract_summary":" The global pandemic has made it more important<br>than ever to quickly and accurately retrieve<br>relevant scientific literature for effective<br>consumption by researchers in a wide range of fields. We<br>provide an analysis of several multi-label document<br>classification models on the LitCovid dataset, a growing<br>collection of 8,000 research papers regarding the novel<br>2019 coronavirus. We find that pre-trained<br>language models fine-tuned on this dataset outperform<br>all other baselines and that the BioBERT and novel<br>Longformer models surpass all others with almost<br>equivalent micro-F1 and accuracy scores of around 81% and<br>69% on the test set. We evaluate the data...","title_summary":" Document Classification for COVID-19<br>Literature","x":-30.5855846405,"y":30.7183036804,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.5855846405,"tsne_y":30.7183036804,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"496p6sld","source_x":"ArXiv","title":"Searching Scientific Literature for Answers on COVID-19 Questions","doi":null,"abstract":"Finding answers related to a pandemic of a novel disease raises new challenges for information seeking and retrieval, as the new information becomes available gradually. TREC COVID search track aims to assist in creating search tools to aid scientists, clinicians, policy makers and others with similar information needs in finding reliable answers from the scientific literature. We experiment with different ranking algorithms as part of our participation in this challenge. We propose a novel method for neural retrieval, and demonstrate its effectiveness on the TREC COVID search.","publish_time":1593993600000,"author_summary":" Nguyen, Vincent; Rybinski, Maciek; Karimi,<br>Sarvnaz; Xing, Zhenchang","abstract_summary":" Finding answers related to a pandemic of a novel<br>disease raises new challenges for information seeking<br>and retrieval, as the new information becomes<br>available gradually. TREC COVID search track aims to<br>assist in creating search tools to aid scientists,<br>clinicians, policy makers and others with similar<br>information needs in finding reliable answers from the<br>scientific literature. We experiment with different<br>ranking algorithms as part of our participation in this<br>challenge. We propose a novel method for neural retrieval,<br>and demonstrate its effectiveness on the TREC<br>COVID search.","title_summary":" Searching Scientific Literature for Answers<br>on COVID-19 Questions","x":-30.5686588287,"y":30.5209884644,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.5686588287,"tsne_y":30.5209884644,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"ux0f10ay","source_x":"ArXiv","title":"CAiRE-COVID: A Question Answering and Multi-Document Summarization System for COVID-19 Research","doi":null,"abstract":"To address the need for refined information in COVID-19 pandemic, we propose a deep learning-based system that uses state-of-the-art natural language processing (NLP) question answering (QA) techniques combined with summarization for mining the available scientific literature. Our system leverages the Information Retrieval (IR) system and QA models to extract relevant snippets from the existing literature given a query. Fluent summaries are also provided to help understand the content in a more efficient way. In this paper, we describe our CAiRE-COVID system architecture and methodology for building the system. To bootstrap the further study, the code for our system is available at https:\/\/github.com\/HLTCHKUST\/CAiRE-COVID","publish_time":1588550400000,"author_summary":" Su, Dan; Xu, Yan; Yu, Tiezheng; Siddique,<br>Farhad Bin; Barezi, Elham J.; Fung, Pascale","abstract_summary":" To address the need for refined information in<br>COVID-19 pandemic, we propose a deep learning-based<br>system that uses state-of-the-art natural language<br>processing (NLP) question answering (QA) techniques<br>combined with summarization for mining the available<br>scientific literature. Our system leverages the<br>Information Retrieval (IR) system and QA models to extract<br>relevant snippets from the existing literature given a<br>query. Fluent summaries are also provided to help<br>understand the content in a more efficient way. In this<br>paper, we describe our CAiRE-COVID system<br>architecture and methodology for building the system. To<br>bootstrap the further study, the code for our system is<br>available...","title_summary":" CAiRE-COVID: A Question Answering and<br>Multi-Document Summarization System for COVID-19 Research","x":-34.3517532349,"y":32.9481086731,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.3517532349,"tsne_y":32.9481086731,"subcluster":23,"subcluster_description":"Community Question Retrieval","shape":"p"},{"cord_uid":"szdzw6tc","source_x":"ArXiv","title":"How does this interaction affect me? Interpretable attribution for feature interactions","doi":null,"abstract":"Machine learning transparency calls for interpretable explanations of how inputs relate to predictions. Feature attribution is a way to analyze the impact of features on predictions. Feature interactions are the contextual dependence between features that jointly impact predictions. There are a number of methods that extract feature interactions in prediction models; however, the methods that assign attributions to interactions are either uninterpretable, model-specific, or non-axiomatic. We propose an interaction attribution and detection framework called Archipelago which addresses these problems and is also scalable in real-world settings. Our experiments on standard annotation labels indicate our approach provides significantly more interpretable explanations than comparable methods, which is important for analyzing the impact of interactions on predictions. We also provide accompanying visualizations of our approach that give new insights into deep neural networks.","publish_time":1592524800000,"author_summary":" Tsang, Michael; Rambhatla, Sirisha; Liu, Yan","abstract_summary":" Machine learning transparency calls for<br>interpretable explanations of how inputs relate to<br>predictions. Feature attribution is a way to analyze the<br>impact of features on predictions. Feature<br>interactions are the contextual dependence between<br>features that jointly impact predictions. There are a<br>number of methods that extract feature interactions<br>in prediction models; however, the methods that<br>assign attributions to interactions are either<br>uninterpretable, model-specific, or non-axiomatic. We propose<br>an interaction attribution and detection<br>framework called Archipelago which addresses these<br>problems and is also scalable in real-world settings.<br>Our experiments on standard annotation labels<br>indicate our approach provides significantly more<br>interpretable explanations...","title_summary":" How does this interaction affect me?<br>Interpretable attribution for feature interactions","x":-31.9521598816,"y":34.8944473267,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.9521598816,"tsne_y":34.8944473267,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"mmtmpey2","source_x":"ArXiv","title":"Integrating Prior Knowledge in Mixed Initiative Social Network Clustering","doi":null,"abstract":"We propose a new paradigm---called PK-clustering---to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering paradigm and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 5) evaluates the consensus between user-selected algorithms and 6) allows users to review details and iteratively update the acquired knowledge. We describe our paradigm using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering paradigm offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often-randomly selected black-box clustering algorithms.","publish_time":1588723200000,"author_summary":" Pister, Alexis; Buono, Paolo; Fekete,<br>Jean-Daniel; Plaisant, Catherine; Valdivia, Paola","abstract_summary":" We propose a new paradigm---called<br>PK-clustering---to help social scientists create meaningful<br>clusters in social networks. Many clustering<br>algorithms exist but most social scientists find them<br>difficult to understand, and tools do not provide any<br>guidance to choose algorithms, or to evaluate results<br>taking into account the prior knowledge of the<br>scientists. Our work introduces a new clustering paradigm<br>and a visual analytics user interface that address<br>this issue. It is based on a process that 1) captures<br>the prior knowledge of the scientists as a set of<br>incomplete clusters, 2) runs multiple clustering<br>algorithms (similarly to clustering ensemble methods),<br>3)...","title_summary":" Integrating Prior Knowledge in Mixed<br>Initiative Social Network Clustering","x":-31.5239887238,"y":34.6587715149,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.5239887238,"tsne_y":34.6587715149,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"9aj5qieh","source_x":"ArXiv","title":"Interactive Extractive Search over Biomedical Corpora","doi":null,"abstract":"We present a system that allows life-science researchers to search a linguistically annotated corpus of scientific texts using patterns over dependency graphs, as well as using patterns over token sequences and a powerful variant of boolean keyword queries. In contrast to previous attempts to dependency-based search, we introduce a light-weight query language that does not require the user to know the details of the underlying linguistic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to efficient linguistic graph-indexing and retrieval engine. This allows for rapid exploration, development and refinement of user queries. We demonstrate the system using example workflows over two corpora: the PubMed corpus including 14,446,243 PubMed abstracts and the CORD-19 dataset, a collection of over 45,000 research papers focused on COVID-19 research. The system is publicly available at https:\/\/allenai.github.io\/spike","publish_time":1591488000000,"author_summary":" Taub-Tabib, Hillel; Shlain, Micah; Sadde,<br>Shoval; Lahav, Dan; Eyal, Matan; Cohen, Yaara;<br>Goldberg, Yoav","abstract_summary":" We present a system that allows life-science<br>researchers to search a linguistically annotated corpus of<br>scientific texts using patterns over dependency graphs,<br>as well as using patterns over token sequences and<br>a powerful variant of boolean keyword queries.<br>In contrast to previous attempts to<br>dependency-based search, we introduce a light-weight query<br>language that does not require the user to know the<br>details of the underlying linguistic<br>representations, and instead to query the corpus by providing an<br>example sentence coupled with simple markup. Search is<br>performed at an interactive speed due to efficient<br>linguistic graph-indexing and retrieval engine. This<br>allows for...","title_summary":" Interactive Extractive Search over<br>Biomedical Corpora","x":-33.3637428284,"y":33.2507896423,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3637428284,"tsne_y":33.2507896423,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"ag0xt2nj","source_x":"ArXiv","title":"Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks","doi":null,"abstract":"This research on data extraction methods applies recent advances in natural language processing to evidence synthesis based on medical texts. Texts of interest include abstracts of clinical trials in English and in multilingual contexts. The main focus is on information characterized via the Population, Intervention, Comparator, and Outcome (PICO) framework, but data extraction is not limited to these fields. Recent neural network architectures based on transformers show capacities for transfer learning and increased performance on downstream natural language processing tasks such as universal reading comprehension, brought forward by this architecture's use of contextualized word embeddings and self-attention mechanisms. This paper contributes to solving problems related to ambiguity in PICO sentence prediction tasks, as well as highlighting how annotations for training named entity recognition systems are used to train a high-performing, but nevertheless flexible architecture for question answering in systematic review automation. Additionally, it demonstrates how the problem of insufficient amounts of training annotations for PICO entity extraction is tackled by augmentation. All models in this paper were created with the aim to support systematic review (semi)automation. They achieve high F1 scores, and demonstrate the feasibility of applying transformer-based classification methods to support data mining in the biomedical literature.","publish_time":1580342400000,"author_summary":" Schmidt, Lena; Weeds, Julie; Higgins, Julian<br>P. T.","abstract_summary":" This research on data extraction methods<br>applies recent advances in natural language<br>processing to evidence synthesis based on medical texts.<br>Texts of interest include abstracts of clinical<br>trials in English and in multilingual contexts. The<br>main focus is on information characterized via the<br>Population, Intervention, Comparator, and Outcome (PICO)<br>framework, but data extraction is not limited to these<br>fields. Recent neural network architectures based on<br>transformers show capacities for transfer learning and<br>increased performance on downstream natural language<br>processing tasks such as universal reading<br>comprehension, brought forward by this architecture's use of<br>contextualized word embeddings and self-attention<br>mechanisms. This paper...","title_summary":" Data Mining in Clinical Trial Text:<br>Transformers for Classification and Question Answering<br>Tasks","x":-32.3831520081,"y":32.0270996094,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.3831520081,"tsne_y":32.0270996094,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"8tq08b4d","source_x":"ArXiv","title":"Fast Graph Attention Networks Using Effective Resistance Based Graph Sparsification","doi":null,"abstract":"The attention mechanism has demonstrated superior performance for inference over nodes in graph neural networks (GNNs), however, they result in a high computational burden during both training and inference. We propose FastGAT, a method to make attention based GNNs lightweight by using spectral sparsification to generate an optimal pruning of the input graph. This results in a per-epoch time that is almost linear in the number of graph nodes as opposed to quadratic. Further, we provide a re-formulation of a specific attention based GNN, Graph Attention Network (GAT) that interprets it as a graph convolution method using the random walk normalized graph Laplacian. Using this framework, we theoretically prove that spectral sparsification preserves the features computed by the GAT model, thereby justifying our FastGAT algorithm. We experimentally evaluate FastGAT on several large real world graph datasets for node classification tasks, FastGAT can dramatically reduce (up to 10x) the computational time and memory requirements, allowing the usage of attention based GNNs on large graphs.","publish_time":1592179200000,"author_summary":" Srinivasa, Rakshith S; Xiao, Cao; Glass,<br>Lucas; Romberg, Justin; Sun, Jimeng","abstract_summary":" The attention mechanism has demonstrated<br>superior performance for inference over nodes in graph<br>neural networks (GNNs), however, they result in a high<br>computational burden during both training and inference. We<br>propose FastGAT, a method to make attention based GNNs<br>lightweight by using spectral sparsification to generate<br>an optimal pruning of the input graph. This<br>results in a per-epoch time that is almost linear in the<br>number of graph nodes as opposed to quadratic.<br>Further, we provide a re-formulation of a specific<br>attention based GNN, Graph Attention Network (GAT) that<br>interprets it as a graph convolution method using the<br>random walk...","title_summary":" Fast Graph Attention Networks Using Effective<br>Resistance Based Graph Sparsification","x":-31.0487270355,"y":35.9896965027,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0487270355,"tsne_y":35.9896965027,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"36ni2tar","source_x":"ArXiv","title":"Graph Neural Network Based Coarse-Grained Mapping Prediction","doi":null,"abstract":"The selection of coarse-grained (CG) mapping operators is a critical step for CG molecular dynamics (MD) simulation. It is still an open question about what is optimal for this choice and there is a need for theory. The current state-of-the art method is mapping operators manually selected by experts. In this work, we demonstrate an automated approach by viewing this problem as supervised learning where we seek to reproduce the mapping operators produced by experts. We present a graph neural network based CG mapping predictor called DEEP SUPERVISED GRAPH PARTITIONING MODEL(DSGPM) that treats mapping operators as a graph segmentation problem. DSGPM is trained on a novel dataset, Human-annotated Mappings (HAM), consisting of 1,206 molecules with expert annotated mapping operators. HAM can be used to facilitate further research in this area. Our model uses a novel metric learning objective to produce high-quality atomic features that are used in spectral clustering. The results show that the DSGPM outperforms state-of-the-art methods in the field of graph segmentation.","publish_time":1592956800000,"author_summary":" Li, Zhiheng; Wellawatte, Geemi P.;<br>Chakraborty, Maghesree; Gandhi, Heta A.; Xu, Chenliang;<br>White, Andrew D.","abstract_summary":" The selection of coarse-grained (CG) mapping<br>operators is a critical step for CG molecular dynamics<br>(MD) simulation. It is still an open question about<br>what is optimal for this choice and there is a need for<br>theory. The current state-of-the art method is mapping<br>operators manually selected by experts. In this work, we<br>demonstrate an automated approach by viewing this problem<br>as supervised learning where we seek to reproduce<br>the mapping operators produced by experts. We<br>present a graph neural network based CG mapping<br>predictor called DEEP SUPERVISED GRAPH PARTITIONING<br>MODEL(DSGPM) that treats mapping operators as a graph<br>segmentation problem....","title_summary":" Graph Neural Network Based Coarse-Grained<br>Mapping Prediction","x":-31.0411987305,"y":36.028339386,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0411987305,"tsne_y":36.028339386,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"9t9a0kn2","source_x":"ArXiv","title":"The Russian Drug Reaction Corpus and Neural Models for Drug Reactions and Effectiveness Detection in User Reviews","doi":null,"abstract":"The Russian Drug Reaction Corpus (RuDReC) is a new partially annotated corpus of consumer reviews in Russian about pharmaceutical products for the detection of health-related named entities and the effectiveness of pharmaceutical products. The corpus itself consists of two parts, the raw one and the labelled one. The raw part includes 1.4 million health-related user-generated texts collected from various Internet sources, including social media. The labelled part contains 500 consumer reviews about drug therapy with drug- and disease-related information. Labels for sentences include health-related issues or their absence. The sentences with one are additionally labelled at the expression level for identification of fine-grained subtypes such as drug classes and drug forms, drug indications, and drug reactions. Further, we present a baseline model for named entity recognition (NER) and multi-label sentence classification tasks on this corpus. The macro F1 score of 74.85% in the NER task was achieved by our RuDR-BERT model. For the sentence classification task, our model achieves the macro F1 score of 68.82% gaining 7.47% over the score of BERT model trained on Russian data. We make the RuDReC corpus and pretrained weights of domain-specific BERT models freely available at https:\/\/github.com\/cimm-kzn\/RuDReC","publish_time":1586217600000,"author_summary":" Tutubalina, Elena; Alimova, Ilseyar;<br>Miftahutdinov, Zulfat; Sakhovskiy, Andrey; Malykh,<br>Valentin; Nikolenko, Sergey","abstract_summary":" The Russian Drug Reaction Corpus (RuDReC) is a<br>new partially annotated corpus of consumer<br>reviews in Russian about pharmaceutical products for<br>the detection of health-related named entities<br>and the effectiveness of pharmaceutical<br>products. The corpus itself consists of two parts, the raw<br>one and the labelled one. The raw part includes 1.4<br>million health-related user-generated texts<br>collected from various Internet sources, including<br>social media. The labelled part contains 500 consumer<br>reviews about drug therapy with drug- and<br>disease-related information. Labels for sentences include<br>health-related issues or their absence. The sentences with one<br>are additionally labelled at the expression level<br>for...","title_summary":" The Russian Drug Reaction Corpus and Neural<br>Models for Drug Reactions and Effectiveness<br>Detection in User Reviews","x":-32.1502494812,"y":31.5865402222,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.1502494812,"tsne_y":31.5865402222,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"4n6v5kfv","source_x":"ArXiv","title":"CORD-19: The COVID-19 Open Research Dataset","doi":null,"abstract":"The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.","publish_time":1587513600000,"author_summary":" Wang, Lucy Lu; Lo, Kyle; Chandrasekhar,<br>Yoganand; Reas, Russell; Yang, Jiangjiang; Burdick,<br>Doug; Eide, Darrin; Funk, Kathryn; Katsis, Yannis;<br>Kinney, Rodney; Li, Yunyao; Liu, Ziyang; Merrill,<br>William; Mooney, Paul; Murdick, Dewey; Rishi, Devvret;<br>Sheehan, Jerry; Shen, Zhihong; Stilson, Brandon; Wade,<br>Alex; Wang, Kuansan; Wang, Nancy Xin Ru; Wilhelm,<br>Chris; Xie, Boya; Raymond, Douglas; Weld, Daniel S.;<br>Etzioni, Oren; Kohlmeier, Sebastian","abstract_summary":" The COVID-19 Open Research Dataset (CORD-19)<br>is a growing resource of scientific papers on<br>COVID-19 and related historical coronavirus research.<br>CORD-19 is designed to facilitate the development of<br>text mining and information retrieval systems over<br>its rich collection of metadata and structured<br>full text papers. Since its release, CORD-19 has<br>been downloaded over 200K times and has served as the<br>basis of many COVID-19 text mining and discovery<br>systems. In this article, we describe the mechanics of<br>dataset construction, highlighting challenges and<br>key design decisions, provide an overview of how<br>CORD-19 has been used, and describe several shared<br>tasks built...","title_summary":" CORD-19: The COVID-19 Open Research Dataset","x":-30.400138855,"y":30.7251491547,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.400138855,"tsne_y":30.7251491547,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"pw1dtx7m","source_x":"ArXiv","title":"KGTK: A Toolkit for Large Knowledge Graph Manipulation and Analysis","doi":null,"abstract":"Knowledge graphs (KGs) have become the preferred technology for representing, sharing and adding knowledge to modern AI applications. While KGs have become a mainstream technology, the RDF\/SPARQL-centric toolset for operating with them at scale is heterogeneous, difficult to integrate and only covers a subset of the operations that are commonly needed in data science applications. In this paper, we present KGTK, a data science-centric toolkit to represent, create, transform, enhance and analyze KGs. KGTK represents graphs in tables and leverages popular libraries developed for data science applications, enabling a wide audience of developers to easily construct knowledge graph pipelines for their applications. We illustrate KGTK with real-world scenarios in which we have used KGTK to integrate and manipulate large KGs, such as Wikidata, DBpedia and ConceptNet, in our own work.","publish_time":1590710400000,"author_summary":" Ilievski, Filip; Garijo, Daniel; Chalupsky,<br>Hans; Divvala, Naren Teja; Yao, Yixiang; Rogers,<br>Craig; Li, Ronpeng; Liu, Jun; Singh, Amandeep;<br>Schwabe, Daniel; Szekely, Pedro","abstract_summary":" Knowledge graphs (KGs) have become the<br>preferred technology for representing, sharing and<br>adding knowledge to modern AI applications. While KGs<br>have become a mainstream technology, the<br>RDF\/SPARQL-centric toolset for operating with them at scale is<br>heterogeneous, difficult to integrate and only covers a subset<br>of the operations that are commonly needed in data<br>science applications. In this paper, we present KGTK, a<br>data science-centric toolkit to represent,<br>create, transform, enhance and analyze KGs. KGTK<br>represents graphs in tables and leverages popular<br>libraries developed for data science applications,<br>enabling a wide audience of developers to easily<br>construct knowledge graph pipelines for...","title_summary":" KGTK: A Toolkit for Large Knowledge Graph<br>Manipulation and Analysis","x":-33.2296257019,"y":35.9704704285,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.2296257019,"tsne_y":35.9704704285,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"qg5m6x66","source_x":"ArXiv","title":"Knowledge Base Completion: Baseline strikes back (Again)","doi":null,"abstract":"Knowledge Base Completion has been a very active area recently, where multiplicative models have generally outperformed additive and other deep learning methods -- like GNN, CNN, path-based models. Several recent KBC papers propose architectural changes, new training methods, or even a new problem reformulation. They evaluate their methods on standard benchmark datasets - FB15k, FB15k-237, WN18, WN18RR, and Yago3-10. Recently, some papers discussed how 1-N scoring can speed up training and evaluation. In this paper, we discuss how by just applying this training regime to a basic model like Complex gives near SOTA performance on all the datasets -- we call this model COMPLEX-V2. We also highlight how various multiplicative methods recently proposed in literature benefit from this trick and become indistinguishable in terms of performance on most datasets. This paper calls for a reassessment of their individual value, in light of these findings.","publish_time":1588377600000,"author_summary":" Jain, Prachi; Rathi, Sushant; Mausam,;<br>Chakrabarti, Soumen","abstract_summary":" Knowledge Base Completion has been a very<br>active area recently, where multiplicative models<br>have generally outperformed additive and other<br>deep learning methods -- like GNN, CNN, path-based<br>models. Several recent KBC papers propose<br>architectural changes, new training methods, or even a new<br>problem reformulation. They evaluate their methods on<br>standard benchmark datasets - FB15k, FB15k-237, WN18,<br>WN18RR, and Yago3-10. Recently, some papers discussed<br>how 1-N scoring can speed up training and<br>evaluation. In this paper, we discuss how by just applying<br>this training regime to a basic model like Complex<br>gives near SOTA performance on all the datasets -- we...","title_summary":" Knowledge Base Completion: Baseline strikes<br>back (Again)","x":-30.4921188354,"y":35.4585342407,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.4921188354,"tsne_y":35.4585342407,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"41xa7asm","source_x":"ArXiv","title":"SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search","doi":null,"abstract":"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, already generating thousands of new papers that join a litany of previous biomedical work in related areas. This deluge of information makes it hard for researchers to keep track of their own research area, let alone explore new directions. Standard search engines are designed primarily for targeted search and are not geared for discovery or making connections that are not obvious from reading individual papers. In this paper, we present our ongoing work on SciSight, a novel framework for exploratory search of COVID-19 research. Based on formative interviews with scientists and a review of existing tools, we build and integrate two key capabilities: first, exploring interactions between biomedical facets (e.g., proteins, genes, drugs, diseases, patient characteristics); and second, discovering groups of researchers and how they are connected. We extract entities using a language model pre-trained on several biomedical information extraction tasks, and enrich them with data from the Microsoft Academic Graph (MAG). To find research groups automatically, we use hierarchical clustering with overlap to allow authors, as they do, to belong to multiple groups. Finally, we introduce a novel presentation of these groups based on both topical and social affinities, allowing users to drill down from groups to papers to associations between entities, and update query suggestions on the fly with the goal of facilitating exploratory navigation. SciSight has thus far served over 10K users with over 30K page views and 13% returning users. Preliminary user interviews with biomedical researchers suggest that SciSight complements current approaches and helps find new and relevant knowledge.","publish_time":1589932800000,"author_summary":" Hope, Tom; Portenoy, Jason; Vasan, Kishore;<br>Borchardt, Jonathan; Horvitz, Eric; Weld, Daniel S.;<br>Hearst, Marti A.; West, Jevin","abstract_summary":" The COVID-19 pandemic has sparked<br>unprecedented mobilization of scientists, already<br>generating thousands of new papers that join a litany of<br>previous biomedical work in related areas. This deluge<br>of information makes it hard for researchers to<br>keep track of their own research area, let alone<br>explore new directions. Standard search engines are<br>designed primarily for targeted search and are not<br>geared for discovery or making connections that are<br>not obvious from reading individual papers. In<br>this paper, we present our ongoing work on SciSight,<br>a novel framework for exploratory search of<br>COVID-19 research. Based on formative interviews with<br>scientists and...","title_summary":" SciSight: Combining faceted navigation and<br>research group detection for COVID-19 exploratory<br>scientific search","x":-30.6583194733,"y":30.9747867584,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6583194733,"tsne_y":30.9747867584,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"8251x771","source_x":"ArXiv","title":"Sentiment Analysis: Detecting Valence, Emotions, and Other Affectual States from Text","doi":null,"abstract":"Recent advances in machine learning have led to computer systems that are human-like in behaviour. Sentiment analysis, the automatic determination of emotions in text, is allowing us to capitalize on substantial previously unattainable opportunities in commerce, public health, government policy, social sciences, and art. Further, analysis of emotions in text, from news to social media posts, is improving our understanding of not just how people convey emotions through language but also how emotions shape our behaviour. This article presents a sweeping overview of sentiment analysis research that includes: the origins of the field, the rich landscape of tasks, challenges, a survey of the methods and resources used, and applications. We also discuss discuss how, without careful fore-thought, sentiment analysis has the potential for harmful outcomes. We outline the latest lines of research in pursuit of fairness in sentiment analysis.","publish_time":1590364800000,"author_summary":" Mohammad, Saif M.","abstract_summary":" Recent advances in machine learning have led to<br>computer systems that are human-like in behaviour.<br>Sentiment analysis, the automatic determination of<br>emotions in text, is allowing us to capitalize on<br>substantial previously unattainable opportunities in<br>commerce, public health, government policy, social<br>sciences, and art. Further, analysis of emotions in text,<br>from news to social media posts, is improving our<br>understanding of not just how people convey emotions through<br>language but also how emotions shape our behaviour. This<br>article presents a sweeping overview of sentiment<br>analysis research that includes: the origins of the<br>field, the rich landscape of tasks, challenges, a...","title_summary":" Sentiment Analysis: Detecting Valence,<br>Emotions, and Other Affectual States from Text","x":-33.9051971436,"y":29.859577179,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9051971436,"tsne_y":29.859577179,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"eozy4ng5","source_x":"ArXiv","title":"CORD19STS: COVID-19 Semantic Textual Similarity Dataset","doi":null,"abstract":"In order to combat the COVID-19 pandemic, society can benefit from various natural language processing applications, such as dialog medical diagnosis systems and information retrieval engines calibrated specifically for COVID-19. These applications rely on the ability to measure semantic textual similarity (STS), making STS a fundamental task that can benefit several downstream applications. However, existing STS datasets and models fail to translate their performance to a domain-specific environment such as COVID-19. To overcome this gap, we introduce CORD19STS dataset which includes 13,710 annotated sentence pairs collected from COVID-19 open research dataset (CORD-19) challenge. To be specific, we generated one million sentence pairs using different sampling strategies. We then used a finetuned BERT-like language model, which we call Sen-SCI-CORD19-BERT, to calculate the similarity scores between sentence pairs to provide a balanced dataset with respect to the different semantic similarity levels, which gives us a total of 32K sentence pairs. Each sentence pair was annotated by five Amazon Mechanical Turk (AMT) crowd workers, where the labels represent different semantic similarity levels between the sentence pairs (i.e. related, somewhat-related, and not-related). After employing a rigorous qualification tasks to verify collected annotations, our final CORD19STS dataset includes 13,710 sentence pairs.","publish_time":1593907200000,"author_summary":" Guo, Xiao; Mirzaalian, Hengameh; Sabir,<br>Ekraam; Jaiswal, Aysush; Abd-Almageed, Wael","abstract_summary":" In order to combat the COVID-19 pandemic,<br>society can benefit from various natural language<br>processing applications, such as dialog medical<br>diagnosis systems and information retrieval engines<br>calibrated specifically for COVID-19. These<br>applications rely on the ability to measure semantic textual<br>similarity (STS), making STS a fundamental task that can<br>benefit several downstream applications. However,<br>existing STS datasets and models fail to translate their<br>performance to a domain-specific environment such as<br>COVID-19. To overcome this gap, we introduce CORD19STS<br>dataset which includes 13,710 annotated sentence<br>pairs collected from COVID-19 open research dataset<br>(CORD-19) challenge. To be specific, we generated one<br>million...","title_summary":" CORD19STS: COVID-19 Semantic Textual<br>Similarity Dataset","x":-31.428478241,"y":31.4335193634,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.428478241,"tsne_y":31.4335193634,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"qfeo6sch","source_x":"ArXiv","title":"CO-Search: COVID-19 Information Retrieval with Semantic Search, Question Answering, and Abstractive Summarization","doi":null,"abstract":"The COVID-19 global pandemic has resulted in international efforts to understand, track, and mitigate the disease, yielding a significant corpus of COVID-19 and SARS-CoV-2-related publications across scientific disciplines. As of May 2020, 128,000 coronavirus-related publications have been collected through the COVID-19 Open Research Dataset Challenge. Here we present CO-Search, a retriever-ranker semantic search engine designed to handle complex queries over the COVID-19 literature, potentially aiding overburdened health workers in finding scientific answers during a time of crisis. The retriever is built from a Siamese-BERT encoder that is linearly composed with a TF-IDF vectorizer, and reciprocal-rank fused with a BM25 vectorizer. The ranker is composed of a multi-hop question-answering module, that together with a multi-paragraph abstractive summarizer adjust retriever scores. To account for the domain-specific and relatively limited dataset, we generate a bipartite graph of document paragraphs and citations, creating 1.3 million (citation title, paragraph) tuples for training the encoder. We evaluate our system on the data of the TREC-COVID information retrieval challenge. CO-Search obtains top performance on the datasets of the first and second rounds, across several key metrics: normalized discounted cumulative gain, precision, mean average precision, and binary preference.","publish_time":1592352000000,"author_summary":" Esteva, Andre; Kale, Anuprit; Paulus, Romain;<br>Hashimoto, Kazuma; Yin, Wenpeng; Radev, Dragomir;<br>Socher, Richard","abstract_summary":" The COVID-19 global pandemic has resulted in<br>international efforts to understand, track, and mitigate the<br>disease, yielding a significant corpus of COVID-19 and<br>SARS-CoV-2-related publications across scientific disciplines.<br>As of May 2020, 128,000 coronavirus-related<br>publications have been collected through the COVID-19 Open<br>Research Dataset Challenge. Here we present CO-Search,<br>a retriever-ranker semantic search engine<br>designed to handle complex queries over the COVID-19<br>literature, potentially aiding overburdened health<br>workers in finding scientific answers during a time of<br>crisis. The retriever is built from a Siamese-BERT<br>encoder that is linearly composed with a TF-IDF<br>vectorizer, and reciprocal-rank fused with a BM25<br>vectorizer....","title_summary":" CO-Search: COVID-19 Information Retrieval<br>with Semantic Search, Question Answering, and<br>Abstractive Summarization","x":-31.5651054382,"y":31.5583305359,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.5651054382,"tsne_y":31.5583305359,"subcluster":7,"subcluster_description":"Clef Ehealth Evaluation Lab","shape":"p"},{"cord_uid":"79tbzfcc","source_x":"ArXiv","title":"Visualization of Diseases at Risk in the COVID-19 Literature","doi":null,"abstract":"This paper presents a project, named VIDAR-19, able to extract automatically diseases from the CORD-19 dataset, and also diseases which might be considered as risk factors. The project relies on the ICD-11 classification of diseases maintained by the WHO. This nomenclature is used as a data source of the extraction mechanism, and also as the repository for the results. Developed for the COVID-19, the project has the ability to extract diseases at risk and to calculate relevant indicators. The outcome of the project is presented in a dashboard which enables the user to explore graphically diseases at risk which are put back in the classification hierarchy. Beyond the COVID-19, VIDAR has much broader applications and might be directly used for any corpus dealing with other pathologies.","publish_time":1588377600000,"author_summary":" Wolinski, Francis","abstract_summary":" This paper presents a project, named VIDAR-19,<br>able to extract automatically diseases from the<br>CORD-19 dataset, and also diseases which might be<br>considered as risk factors. The project relies on the<br>ICD-11 classification of diseases maintained by the<br>WHO. This nomenclature is used as a data source of the<br>extraction mechanism, and also as the repository for the<br>results. Developed for the COVID-19, the project has the<br>ability to extract diseases at risk and to calculate<br>relevant indicators. The outcome of the project is<br>presented in a dashboard which enables the user to explore<br>graphically diseases at risk which are...","title_summary":" Visualization of Diseases at Risk in the<br>COVID-19 Literature","x":-30.6745471954,"y":31.1399841309,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6745471954,"tsne_y":31.1399841309,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"sybtcv4q","source_x":"ArXiv","title":"Facilitating Access to Multilingual COVID-19 Information via Neural Machine Translation","doi":null,"abstract":"Every day, more people are becoming infected and dying from exposure to COVID-19. Some countries in Europe like Spain, France, the UK and Italy have suffered particularly badly from the virus. Others such as Germany appear to have coped extremely well. Both health professionals and the general public are keen to receive up-to-date information on the effects of the virus, as well as treatments that have proven to be effective. In cases where language is a barrier to access of pertinent information, machine translation (MT) may help people assimilate information published in different languages. Our MT systems trained on COVID-19 data are freely available for anyone to use to help translate information published in German, French, Italian, Spanish into English, as well as the reverse direction.","publish_time":1588291200000,"author_summary":" Way, Andy; Haque, Rejwanul; Xie, Guodong;<br>Gaspari, Federico; Popovic, Maja; Poncelas, Alberto","abstract_summary":" Every day, more people are becoming infected<br>and dying from exposure to COVID-19. Some<br>countries in Europe like Spain, France, the UK and Italy<br>have suffered particularly badly from the virus.<br>Others such as Germany appear to have coped extremely<br>well. Both health professionals and the general<br>public are keen to receive up-to-date information on<br>the effects of the virus, as well as treatments that<br>have proven to be effective. In cases where language<br>is a barrier to access of pertinent information,<br>machine translation (MT) may help people assimilate<br>information published in different languages. Our MT<br>systems trained on COVID-19...","title_summary":" Facilitating Access to Multilingual COVID-19<br>Information via Neural Machine Translation","x":-31.2380867004,"y":30.1386013031,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2380867004,"tsne_y":30.1386013031,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"m6w3el71","source_x":"ArXiv","title":"A Feature Analysis for Multimodal News Retrieval","doi":null,"abstract":"Content-based information retrieval is based on the information contained in documents rather than using metadata such as keywords. Most information retrieval methods are either based on text or image. In this paper, we investigate the usefulness of multimodal features for cross-lingual news search in various domains: politics, health, environment, sport, and finance. To this end, we consider five feature types for image and text and compare the performance of the retrieval system using different combinations. Experimental results show that retrieval results can be improved when considering both visual and textual information. In addition, it is observed that among textual features entity overlap outperforms word embeddings, while geolocation embeddings achieve better performance among visual features in the retrieval task.","publish_time":1594598400000,"author_summary":" Tahmasebzadeh, Golsa; Hakimov, Sherzod;<br>Muller-Budack, Eric; Ewerth, Ralph","abstract_summary":" Content-based information retrieval is based<br>on the information contained in documents rather<br>than using metadata such as keywords. Most<br>information retrieval methods are either based on text or<br>image. In this paper, we investigate the usefulness of<br>multimodal features for cross-lingual news search in<br>various domains: politics, health, environment,<br>sport, and finance. To this end, we consider five<br>feature types for image and text and compare the<br>performance of the retrieval system using different<br>combinations. Experimental results show that retrieval<br>results can be improved when considering both visual<br>and textual information. In addition, it is<br>observed that among textual features...","title_summary":" A Feature Analysis for Multimodal News<br>Retrieval","x":-33.745513916,"y":33.8710746765,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.745513916,"tsne_y":33.8710746765,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"6up27wou","source_x":"ArXiv","title":"Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA","doi":null,"abstract":"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO_2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text and align the resulting word vectors with the wordpiece vectors of a general-domain PTLM. We evaluate on eight biomedical Named Entity Recognition (NER) tasks and compare against the recently proposed BioBERT model. We cover over 60% of the BioBERT-BERT F1 delta, at 5% of BioBERT's CO_2 footprint and 2% of its cloud compute cost. We also show how to quickly adapt an existing general-domain Question Answering (QA) model to an emerging domain: the Covid-19 pandemic.","publish_time":1586217600000,"author_summary":" Poerner, Nina; Waltinger, Ulli; Schutze,<br>Hinrich","abstract_summary":" Domain adaptation of Pretrained Language<br>Models (PTLMs) is typically achieved by unsupervised<br>pretraining on target-domain text. While successful, this<br>approach is expensive in terms of hardware, runtime and<br>CO_2 emissions. Here, we propose a cheaper<br>alternative: We train Word2Vec on target-domain text and<br>align the resulting word vectors with the wordpiece<br>vectors of a general-domain PTLM. We evaluate on eight<br>biomedical Named Entity Recognition (NER) tasks and<br>compare against the recently proposed BioBERT model.<br>We cover over 60% of the BioBERT-BERT F1 delta, at<br>5% of BioBERT's CO_2 footprint and 2% of its cloud<br>compute cost. We also show how...","title_summary":" Inexpensive Domain Adaptation of Pretrained<br>Language Models: Case Studies on Biomedical NER and<br>Covid-19 QA","x":-32.0635757446,"y":31.853307724,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.0635757446,"tsne_y":31.853307724,"subcluster":8,"subcluster_description":"Multi-Label Medical Text Classificationexperiencer","shape":"p"},{"cord_uid":"pwntqqtd","source_x":"ArXiv","title":"A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19","doi":null,"abstract":"COVID-19 has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf, BERT, BioBERT, and USE to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot, we created a user-friendly interactive web application to be hosted online.","publish_time":1592524800000,"author_summary":" Oniani, David; Wang, Yanshan","abstract_summary":" COVID-19 has resulted in an ongoing pandemic<br>and as of 12 June 2020, has caused more than 7.4<br>million cases and over 418,000 deaths. The highly<br>dynamic and rapidly evolving situation with COVID-19<br>has made it difficult to access accurate,<br>on-demand information regarding the disease. Online<br>communities, forums, and social media provide potential<br>venues to search for relevant questions and answers,<br>or post questions and seek answers from other<br>members. However, due to the nature of such sites, there<br>are always a limited number of relevant questions<br>and responses to search from, and posted questions<br>are rarely answered immediately. With...","title_summary":" A Qualitative Evaluation of Language Models on<br>Automatic Question-Answering for COVID-19","x":-31.2282390594,"y":30.6561889648,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2282390594,"tsne_y":30.6561889648,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"8aodvgww","source_x":"ArXiv","title":"Detecting fake news for the new coronavirus by reasoning on the Covid-19 ontology","doi":null,"abstract":"In the context of the Covid-19 pandemic, many were quick to spread deceptive information. I investigate here how reasoning in Description Logics (DLs) can detect inconsistencies between trusted medical sources and not trusted ones. The not-trusted information comes in natural language (e.g.\"Covid-19 affects only the elderly\"). To automatically convert into DLs, I used the FRED converter. Reasoning in Description Logics is then performed with the Racer tool.","publish_time":1587859200000,"author_summary":" Groza, Adrian","abstract_summary":" In the context of the Covid-19 pandemic, many<br>were quick to spread deceptive information. I<br>investigate here how reasoning in Description Logics (DLs)<br>can detect inconsistencies between trusted<br>medical sources and not trusted ones. The not-trusted<br>information comes in natural language (e.g.\"Covid-19<br>affects only the elderly\"). To automatically convert<br>into DLs, I used the FRED converter. Reasoning in<br>Description Logics is then performed with the Racer tool.","title_summary":" Detecting fake news for the new coronavirus by<br>reasoning on the Covid-19 ontology","x":-29.8545646667,"y":30.0841751099,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-29.8545646667,"tsne_y":30.0841751099,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ky8m39tc","source_x":"ArXiv","title":"Learning the Compositional Visual Coherence for Complementary Recommendations","doi":null,"abstract":"Complementary recommendations, which aim at providing users product suggestions that are supplementary and compatible with their obtained items, have become a hot topic in both academia and industry in recent years. %However, it is challenging due to its complexity and subjectivity. Existing work mainly focused on modeling the co-purchased relations between two items, but the compositional associations of item collections are largely unexplored. Actually, when a user chooses the complementary items for the purchased products, it is intuitive that she will consider the visual semantic coherence (such as color collocations, texture compatibilities) in addition to global impressions. Towards this end, in this paper, we propose a novel Content Attentive Neural Network (CANN) to model the comprehensive compositional coherence on both global contents and semantic contents. Specifically, we first propose a \\textit{Global Coherence Learning} (GCL) module based on multi-heads attention to model the global compositional coherence. Then, we generate the semantic-focal representations from different semantic regions and design a \\textit{Focal Coherence Learning} (FCL) module to learn the focal compositional coherence from different semantic-focal representations. Finally, we optimize the CANN in a novel compositional optimization strategy. Extensive experiments on the large-scale real-world data clearly demonstrate the effectiveness of CANN compared with several state-of-the-art methods.","publish_time":1591574400000,"author_summary":" Li, Zhi; Wu, Bo; Liu, Qi; Wu, Likang; Zhao,<br>Hongke; Mei, Tao","abstract_summary":" Complementary recommendations, which aim at<br>providing users product suggestions that are<br>supplementary and compatible with their obtained items, have<br>become a hot topic in both academia and industry in<br>recent years. %However, it is challenging due to its<br>complexity and subjectivity. Existing work mainly<br>focused on modeling the co-purchased relations<br>between two items, but the compositional associations<br>of item collections are largely unexplored.<br>Actually, when a user chooses the complementary items for<br>the purchased products, it is intuitive that she<br>will consider the visual semantic coherence (such<br>as color collocations, texture<br>compatibilities) in addition to global impressions. Towards<br>this end,...","title_summary":" Learning the Compositional Visual Coherence<br>for Complementary Recommendations","x":-32.0606575012,"y":34.1525382996,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.0606575012,"tsne_y":34.1525382996,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ewo74ui2","source_x":"ArXiv","title":"365 Dots in 2019: Quantifying Attention of News Sources","doi":null,"abstract":"We investigate the overlap of topics of online news articles from a variety of sources. To do this, we provide a platform for studying the news by measuring this overlap and scoring news stories according to the degree of attention in near-real time. This can enable multiple studies, including identifying topics that receive the most attention from news organizations and identifying slow news days versus major news days. Our application, StoryGraph, periodically (10-minute intervals) extracts the first five news articles from the RSS feeds of 17 US news media organizations across the partisanship spectrum (left, center, and right). From these articles, StoryGraph extracts named entities (PEOPLE, LOCATIONS, ORGANIZATIONS, etc.) and then represents each news article with its set of extracted named entities. Finally, StoryGraph generates a news similarity graph where the nodes represent news articles, and an edge between a pair of nodes represents a high degree of similarity between the nodes (similar news stories). Each news story within the news similarity graph is assigned an attention score which quantifies the amount of attention the topics in the news story receive collectively from the news media organizations. The StoryGraph service has been running since August 2017, and using this method, we determined that the top news story of 2018 was the\"Kavanaugh hearings\"with attention score of 25.85 on September 27, 2018. Similarly, the top news story for 2019 so far (2019-12-12) is\"AG William Barr's release of his principal conclusions of the Mueller Report,\"with an attention score of 22.93 on March 24, 2019.","publish_time":1584835200000,"author_summary":" Nwala, Alexander C.; Weigle, Michele C.;<br>Nelson, Michael L.","abstract_summary":" We investigate the overlap of topics of online<br>news articles from a variety of sources. To do this,<br>we provide a platform for studying the news by<br>measuring this overlap and scoring news stories<br>according to the degree of attention in near-real time.<br>This can enable multiple studies, including<br>identifying topics that receive the most attention from<br>news organizations and identifying slow news days<br>versus major news days. Our application, StoryGraph,<br>periodically (10-minute intervals) extracts the first five<br>news articles from the RSS feeds of 17 US news media<br>organizations across the partisanship spectrum (left,<br>center, and right). From these...","title_summary":" 365 Dots in 2019: Quantifying Attention of News<br>Sources","x":-33.3312454224,"y":31.4108886719,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.3312454224,"tsne_y":31.4108886719,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"3ps2c670","source_x":"ArXiv","title":"ASReview: Open Source Software for Efficient and Transparent Active Learning for Systematic Reviews","doi":null,"abstract":"For many tasks - including guideline development for medical doctors and systematic reviews for research fields - the scientific literature needs to be checked systematically. The current practice is that scholars and practitioners screen thousands of studies by hand to find which studies to include in their review. This is error prone and inefficient. We therefore developed an open source machine learning (ML)-aided pipeline: Active learning for Systematic Reviews (ASReview). We show that by using active learning, ASReview can lead to far more efficient reviewing than manual reviewing, while exhibiting adequate quality. Furthermore, the presented software is fully transparent and open source.","publish_time":1592784000000,"author_summary":" Schoot, Rens van de; Bruin, Jonathan de;<br>Schram, Raoul; Zahedi, Parisa; Boer, Jan de; Weijdema,<br>Felix; Kramer, Bianca; Huijts, Martijn; Hoogerwerf,<br>Maarten; Ferdinands, Gerbrich; Harkema, Albert;<br>Willemsen, Joukje; Ma, Yongchao; Fang, Qixiang; Tummers,<br>Lars; Oberski, Daniel","abstract_summary":" For many tasks - including guideline<br>development for medical doctors and systematic reviews for<br>research fields - the scientific literature needs to be<br>checked systematically. The current practice is that<br>scholars and practitioners screen thousands of studies<br>by hand to find which studies to include in their<br>review. This is error prone and inefficient. We<br>therefore developed an open source machine learning<br>(ML)-aided pipeline: Active learning for Systematic<br>Reviews (ASReview). We show that by using active<br>learning, ASReview can lead to far more efficient<br>reviewing than manual reviewing, while exhibiting<br>adequate quality. Furthermore, the presented software<br>is fully transparent and...","title_summary":" ASReview: Open Source Software for Efficient<br>and Transparent Active Learning for Systematic<br>Reviews","x":-32.2071075439,"y":32.0251121521,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.2071075439,"tsne_y":32.0251121521,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"cgj8cn2a","source_x":"ArXiv","title":"CODA-19: Reliably Annotating Research Aspects on 10,000+ CORD-19 Abstracts Using a Non-Expert Crowd","doi":null,"abstract":"This paper introduces CODA-19, a human-annotated dataset that codes the Background, Purpose, Method, Finding\/Contribution, and Other sections of 10,966 English abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk within 10 days, achieving a label quality comparable to that of experts. Each abstract was annotated by nine different workers, and the final labels were obtained by majority vote. The inter-annotator agreement (Cohen's kappa) between the crowd and the biomedical expert (0.741) is comparable to inter-expert agreement (0.788). CODA-19's labels have an accuracy of 82.2% when compared to the biomedical expert's labels, while the accuracy between experts was 85.0%. Reliable human annotations help scientists to understand the rapidly accelerating coronavirus literature and also serve as the battery of AI\/NLP research, but obtaining expert annotations can be slow. We demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against COVID-19.","publish_time":1588636800000,"author_summary":" Huang, Ting-Hao 'Kenneth'; Huang,<br>Chieh-Yang; Ding, Chien-Kuang Cornelia; Hsu, Yen-Chia;<br>Giles, C. Lee","abstract_summary":" This paper introduces CODA-19, a<br>human-annotated dataset that codes the Background, Purpose,<br>Method, Finding\/Contribution, and Other sections of<br>10,966 English abstracts in the COVID-19 Open<br>Research Dataset. CODA-19 was created by 248 crowd<br>workers from Amazon Mechanical Turk within 10 days,<br>achieving a label quality comparable to that of experts.<br>Each abstract was annotated by nine different<br>workers, and the final labels were obtained by majority<br>vote. The inter-annotator agreement (Cohen's<br>kappa) between the crowd and the biomedical expert<br>(0.741) is comparable to inter-expert agreement<br>(0.788). CODA-19's labels have an accuracy of 82.2% when<br>compared to the biomedical expert's labels,...","title_summary":" CODA-19: Reliably Annotating Research<br>Aspects on 10,000+ CORD-19 Abstracts Using a<br>Non-Expert Crowd","x":-31.9168434143,"y":31.8761196136,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.9168434143,"tsne_y":31.8761196136,"subcluster":8,"subcluster_description":"Multi-Label Medical Text Classificationexperiencer","shape":"p"},{"cord_uid":"yt8p1aq2","source_x":"ArXiv","title":"RTEX: A novel methodology for Ranking, Tagging, and Explanatory diagnostic captioning of radiography exams","doi":null,"abstract":"This paper introduces RTEx, a novel methodology for a) ranking radiography exams based on their probability to contain an abnormality, b) generating abnormality tags for abnormal exams, and c) providing a diagnostic explanation in natural language for each abnormal exam. The task of ranking radiography exams is an important first step for practitioners who want to identify and prioritize those radiography exams that are more likely to contain abnormalities, for example, to avoid mistakes due to tiredness or to manage heavy workload (e.g., during a pandemic). We used two publicly available datasets to assess our methodology and demonstrate that for the task of ranking it outperforms its competitors in terms of NDCG@k. For each abnormal radiography exam RTEx generates a set of abnormality tags alongside an explanatory diagnostic text to explain the tags and guide the medical expert. Our tagging component outperforms two strong competitor methods in terms of F1. Moreover, the diagnostic captioning component of RTEx, which exploits the already extracted tags to constrain the captioning process, outperforms all competitors with respect to clinical precision and recall.","publish_time":1591833600000,"author_summary":" Kougia, Vasiliki; Pavlopoulos, John;<br>Papapetrou, Panagiotis; Gordon, Max","abstract_summary":" This paper introduces RTEx, a novel<br>methodology for a) ranking radiography exams based on their<br>probability to contain an abnormality, b) generating<br>abnormality tags for abnormal exams, and c) providing a<br>diagnostic explanation in natural language for each<br>abnormal exam. The task of ranking radiography exams is<br>an important first step for practitioners who<br>want to identify and prioritize those radiography<br>exams that are more likely to contain abnormalities,<br>for example, to avoid mistakes due to tiredness or<br>to manage heavy workload (e.g., during a<br>pandemic). We used two publicly available datasets to<br>assess our methodology and demonstrate that for...","title_summary":" RTEX: A novel methodology for Ranking,<br>Tagging, and Explanatory diagnostic captioning of<br>radiography exams","x":-32.1876068115,"y":32.2008285522,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.1876068115,"tsne_y":32.2008285522,"subcluster":9,"subcluster_description":"Large-Scale Biomedical Semantic Indexing","shape":"p"},{"cord_uid":"map74lk0","source_x":"ArXiv","title":"Deep Graph Contrastive Representation Learning","doi":null,"abstract":"Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.","publish_time":1591488000000,"author_summary":" Zhu, Yanqiao; Xu, Yichen; Yu, Feng; Liu, Qiang;<br>Wu, Shu; Wang, Liang","abstract_summary":" Graph representation learning nowadays<br>becomes fundamental in analyzing graph-structured<br>data. Inspired by recent success of contrastive<br>methods, in this paper, we propose a novel framework for<br>unsupervised graph representation learning by leveraging a<br>contrastive objective at the node level. Specifically, we<br>generate two graph views by corruption and learn node<br>representations by maximizing the agreement of node<br>representations in these two views. To provide diverse node<br>contexts for the contrastive objective, we propose a<br>hybrid scheme for generating graph views on both<br>structure and attribute levels. Besides, we provide<br>theoretical justification behind our motivation from two<br>perspectives, mutual information and...","title_summary":" Deep Graph Contrastive Representation<br>Learning","x":-31.2486972809,"y":35.7776031494,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.2486972809,"tsne_y":35.7776031494,"subcluster":4,"subcluster_description":"Graph Neural Networksattention-Based Evolutionhyperbolic","shape":"p"},{"cord_uid":"38oeau38","source_x":"ArXiv","title":"TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection","doi":null,"abstract":"TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search.","publish_time":1588982400000,"author_summary":" Voorhees, Ellen; Alam, Tasmeer; Bedrick,<br>Steven; Demner-Fushman, Dina; Hersh, William R; Lo,<br>Kyle; Roberts, Kirk; Soboroff, Ian; Wang, Lucy Lu","abstract_summary":" TREC-COVID is a community evaluation designed<br>to build a test collection that captures the<br>information needs of biomedical researchers using the<br>scientific literature during a pandemic. One of the key<br>characteristics of pandemic search is the accelerated rate of<br>change: the topics of interest evolve as the pandemic<br>progresses and the scientific literature in the area<br>explodes. The COVID-19 pandemic provides an opportunity<br>to capture this progression as it happens.<br>TREC-COVID, in creating a test collection around COVID-19<br>literature, is building infrastructure to support new<br>research and technologies in pandemic search.","title_summary":" TREC-COVID: Constructing a Pandemic<br>Information Retrieval Test Collection","x":-30.3481464386,"y":30.2850227356,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.3481464386,"tsne_y":30.2850227356,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"o8wp12c0","source_x":"ArXiv","title":"Needles in the 'Sheet'stack: Augmented Analytics to get Insights from Spreadsheets","doi":null,"abstract":"Business intelligence (BI) tools for database analytics have come a long way and nowadays also provide ready insights or visual query explorations, e.g. QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In this demo, we focus on providing insights by examining periodic spreadsheets of different reports (aka views), without prior knowledge of the schema of the database or reports, or data information. Such a solution is targeted at users without the familiarity with the database schema or resources to conduct analytics in the contemporary way.","publish_time":1592179200000,"author_summary":" Atre, Medha; Deshpande, Anand; Godse, Reshma;<br>Deokar, Pooja; Moharir, Sandip; Ray, Dhruva;<br>Chitlangia, Akshay; Phadnis, Trupti; Goyal, Yugansh","abstract_summary":" Business intelligence (BI) tools for database<br>analytics have come a long way and nowadays also provide<br>ready insights or visual query explorations, e.g.<br>QuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot,<br>Zenvisage, etc. In this demo, we focus on providing<br>insights by examining periodic spreadsheets of<br>different reports (aka views), without prior knowledge<br>of the schema of the database or reports, or data<br>information. Such a solution is targeted at users without the<br>familiarity with the database schema or resources to<br>conduct analytics in the contemporary way.","title_summary":" Needles in the 'Sheet'stack: Augmented<br>Analytics to get Insights from Spreadsheets","x":-33.0361251831,"y":36.6411323547,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.0361251831,"tsne_y":36.6411323547,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"zxo2afjk","source_x":"ArXiv","title":"Integrating Logical Rules Into Neural Multi-Hop Reasoning for Drug Repurposing","doi":null,"abstract":"The graph structure of biomedical data differs from those in typical knowledge graph benchmark tasks. A particular property of biomedical data is the presence of long-range dependencies, which can be captured by patterns described as logical rules. We propose a novel method that combines these rules with a neural multi-hop reasoning approach that uses reinforcement learning. We conduct an empirical study based on the real-world task of drug repurposing by formulating this task as a link prediction problem. We apply our method to the biomedical knowledge graph Hetionet and show that our approach outperforms several baseline methods.","publish_time":1594339200000,"author_summary":" Liu, Yushan; Hildebrandt, Marcel; Joblin,<br>Mitchell; Ringsquandl, Martin; Tresp, Volker","abstract_summary":" The graph structure of biomedical data differs<br>from those in typical knowledge graph benchmark<br>tasks. A particular property of biomedical data is the<br>presence of long-range dependencies, which can be<br>captured by patterns described as logical rules. We<br>propose a novel method that combines these rules with a<br>neural multi-hop reasoning approach that uses<br>reinforcement learning. We conduct an empirical study based<br>on the real-world task of drug repurposing by<br>formulating this task as a link prediction problem. We apply<br>our method to the biomedical knowledge graph<br>Hetionet and show that our approach outperforms several<br>baseline methods.","title_summary":" Integrating Logical Rules Into Neural<br>Multi-Hop Reasoning for Drug Repurposing","x":-31.0105743408,"y":32.0006103516,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.0105743408,"tsne_y":32.0006103516,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"n1656hq0","source_x":"ArXiv","title":"Exploration and Discovery of the COVID-19 Literature through Semantic Visualization","doi":null,"abstract":"We are developing semantic visualization techniques in order to enhance exploration and enable discovery over large datasets of complex networks of relations. Semantic visualization is a method of enabling exploration and discovery over large datasets of complex networks by exploiting the semantics of the relations in them. This involves (i) NLP to extract named entities, relations and knowledge graphs from the original data; (ii) indexing the output and creating representations for all relevant entities and relations that can be visualized in many different ways, e.g., as tag clouds, heat maps, graphs, etc.; (iii) applying parameter reduction operations to the extracted relations, creating\"relation containers\", or functional entities that can also be visualized using the same methods, allowing the visualization of multiple relations, partial pathways, and exploration across multiple dimensions. Our hope is that this will enable the discovery of novel inferences over relations in complex data that otherwise would go unnoticed. We have applied this to analysis of the recently released CORD-19 dataset.","publish_time":1593734400000,"author_summary":" Tu, Jingxuan; Verhagen, Marc; Cochran, Brent;<br>Pustejovsky, James","abstract_summary":" We are developing semantic visualization<br>techniques in order to enhance exploration and enable<br>discovery over large datasets of complex networks of<br>relations. Semantic visualization is a method of enabling<br>exploration and discovery over large datasets of complex<br>networks by exploiting the semantics of the relations in<br>them. This involves (i) NLP to extract named<br>entities, relations and knowledge graphs from the<br>original data; (ii) indexing the output and creating<br>representations for all relevant entities and relations that<br>can be visualized in many different ways, e.g., as<br>tag clouds, heat maps, graphs, etc.; (iii)<br>applying parameter reduction operations to the<br>extracted...","title_summary":" Exploration and Discovery of the COVID-19<br>Literature through Semantic Visualization","x":-33.1028213501,"y":35.6658363342,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.1028213501,"tsne_y":35.6658363342,"subcluster":15,"subcluster_description":"Knowledge Graph","shape":"p"},{"cord_uid":"pcrqdroq","source_x":"ArXiv","title":"A Hybrid Natural Language Generation System Integrating Rules and Deep Learning Algorithms","doi":null,"abstract":"This paper proposes an enhanced natural language generation system combining the merits of both rule-based approaches and modern deep learning algorithms, boosting its performance to the extent where the generated textual content is capable of exhibiting agile human-writing styles and the content logic of which is highly controllable. We also come up with a novel approach called HMCU to measure the performance of the natural language processing comprehensively and precisely.","publish_time":1592179200000,"author_summary":" Wei, Wei; Zhou, Bei; Leontidis, Georgios","abstract_summary":" This paper proposes an enhanced natural<br>language generation system combining the merits of both<br>rule-based approaches and modern deep learning<br>algorithms, boosting its performance to the extent where<br>the generated textual content is capable of<br>exhibiting agile human-writing styles and the content<br>logic of which is highly controllable. We also come up<br>with a novel approach called HMCU to measure the<br>performance of the natural language processing<br>comprehensively and precisely.","title_summary":" A Hybrid Natural Language Generation System<br>Integrating Rules and Deep Learning Algorithms","x":-35.1821632385,"y":32.1337165833,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.1821632385,"tsne_y":32.1337165833,"subcluster":16,"subcluster_description":"Transfercase-Sensitive Neural Machine Translationinnovative","shape":"p"},{"cord_uid":"ybhwrfm7","source_x":"ArXiv","title":"BERTweet: A pre-trained language model for English Tweets","doi":null,"abstract":"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet is trained using the RoBERTa pre-training procedure (Liu et al., 2019), with the same model configuration as BERT-base (Devlin et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet to facilitate future research and downstream applications on Tweet data. Our BERTweet is available at: https:\/\/github.com\/VinAIResearch\/BERTweet","publish_time":1589932800000,"author_summary":" Nguyen, Dat Quoc; Vu, Thanh; Nguyen, Anh Tuan","abstract_summary":" We present BERTweet, the first public<br>large-scale pre-trained language model for English<br>Tweets. Our BERTweet is trained using the RoBERTa<br>pre-training procedure (Liu et al., 2019), with the same<br>model configuration as BERT-base (Devlin et al.,<br>2019). Experiments show that BERTweet outperforms<br>strong baselines RoBERTa-base and XLM-R-base<br>(Conneau et al., 2020), producing better performance<br>results than the previous state-of-the-art models on<br>three Tweet NLP tasks: Part-of-speech tagging,<br>Named-entity recognition and text classification. We<br>release BERTweet to facilitate future research and<br>downstream applications on Tweet data. Our BERTweet is<br>available at:<br>https:\/\/github.com\/VinAIResearch\/BERTweet","title_summary":" BERTweet: A pre-trained language model for<br>English Tweets","x":-34.6112747192,"y":31.4006328583,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6112747192,"tsne_y":31.4006328583,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"anw93wws","source_x":"ArXiv","title":"memeBot: Towards Automatic Image Meme Generation","doi":null,"abstract":"Image memes have become a widespread tool used by people for interacting and exchanging ideas over social media, blogs, and open messengers. This work proposes to treat automatic image meme generation as a translation process, and further present an end to end neural and probabilistic approach to generate an image-based meme for any given sentence using an encoder-decoder architecture. For a given input sentence, an image meme is generated by combining a meme template image and a text caption where the meme template image is selected from a set of popular candidates using a selection module, and the meme caption is generated by an encoder-decoder model. An encoder is used to map the selected meme template and the input sentence into a meme embedding and a decoder is used to decode the meme caption from the meme embedding. The generated natural language meme caption is conditioned on the input sentence and the selected meme template. The model learns the dependencies between the meme captions and the meme template images and generates new memes using the learned dependencies. The quality of the generated captions and the generated memes is evaluated through both automated and human evaluation. An experiment is designed to score how well the generated memes can represent the tweets from Twitter conversations. Experiments on Twitter data show the efficacy of the model in generating memes for sentences in online social interaction.","publish_time":1588204800000,"author_summary":" Sadasivam, Aadhavan; Gunasekar, Kausic;<br>Davulcu, Hasan; Yang, Yezhou","abstract_summary":" Image memes have become a widespread tool used<br>by people for interacting and exchanging ideas<br>over social media, blogs, and open messengers. This<br>work proposes to treat automatic image meme<br>generation as a translation process, and further present<br>an end to end neural and probabilistic approach to<br>generate an image-based meme for any given sentence<br>using an encoder-decoder architecture. For a given<br>input sentence, an image meme is generated by<br>combining a meme template image and a text caption where<br>the meme template image is selected from a set of<br>popular candidates using a selection module, and the<br>meme caption...","title_summary":" memeBot: Towards Automatic Image Meme<br>Generation","x":-34.6784324646,"y":31.648815155,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.6784324646,"tsne_y":31.648815155,"subcluster":11,"subcluster_description":"Automatic Arabic Diacritizationenhancing","shape":"p"},{"cord_uid":"67ximjat","source_x":"ArXiv","title":"English dictionaries, gold and silver standard corpora for biomedical natural language processing related to SARS-CoV-2 and COVID-19","doi":null,"abstract":"Here we present a toolbox for natural language processing tasks related to SARS-CoV-2. It comprises English dictionaries of synonyms for SARS-CoV-2 and COVID-19, a silver standard corpus generated with the dictionaries and a gold standard corpus of 10 Pubmed abstracts manually annotated for disease, virus, symptom and protein\/gene terms. This toolbox is freely available on github (on https:\/\/github.com\/Aitslab\/corona) and can be used for text analytics in a variety of settings related to the COVID-19 crisis. It will be expanded and applied in NLP tasks over the next weeks and the community is invited to contribute.","publish_time":1584835200000,"author_summary":" Rashed, Salma Kazemi; Frid, Johan; Aits, Sonja","abstract_summary":" Here we present a toolbox for natural language<br>processing tasks related to SARS-CoV-2. It comprises<br>English dictionaries of synonyms for SARS-CoV-2 and<br>COVID-19, a silver standard corpus generated with the<br>dictionaries and a gold standard corpus of 10 Pubmed<br>abstracts manually annotated for disease, virus,<br>symptom and protein\/gene terms. This toolbox is freely<br>available on github (on<br>https:\/\/github.com\/Aitslab\/corona) and can be used for text analytics in a variety of<br>settings related to the COVID-19 crisis. It will be<br>expanded and applied in NLP tasks over the next weeks and<br>the community is invited to contribute.","title_summary":" English dictionaries, gold and silver<br>standard corpora for biomedical natural language<br>processing related to SARS-CoV-2 and COVID-19","x":-31.126789093,"y":30.8754043579,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.126789093,"tsne_y":30.8754043579,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"wjfvr7pa","source_x":"ArXiv","title":"History-Aware Question Answering in a Blocks World Dialogue System","doi":null,"abstract":"It is essential for dialogue-based spatial reasoning systems to maintain memory of historical states of the world. In addition to conveying that the dialogue agent is mentally present and engaged with the task, referring to historical states may be crucial for enabling collaborative planning (e.g., for planning to return to a previous state, or diagnosing a past misstep). In this paper, we approach the problem of spatial memory in a multi-modal spoken dialogue system capable of answering questions about interaction history in a physical blocks world setting. This work builds upon a full spatial question-answering pipeline consisting of a vision system, speech input and output mediated by an animated avatar, a dialogue system that robustly interprets spatial queries, and a constraint solver that derives answers based on 3-D spatial modelling. The contributions of this work include a symbolic dialogue context registering knowledge about discourse history and changes in the world, as well as a natural language understanding module capable of interpreting free-form historical questions and querying the dialogue context to form an answer.","publish_time":1590451200000,"author_summary":" Kane, Benjamin; Platonov, Georgiy; Schubert,<br>Lenhart K.","abstract_summary":" It is essential for dialogue-based spatial<br>reasoning systems to maintain memory of historical<br>states of the world. In addition to conveying that the<br>dialogue agent is mentally present and engaged with the<br>task, referring to historical states may be crucial<br>for enabling collaborative planning (e.g., for<br>planning to return to a previous state, or diagnosing a<br>past misstep). In this paper, we approach the<br>problem of spatial memory in a multi-modal spoken<br>dialogue system capable of answering questions about<br>interaction history in a physical blocks world setting.<br>This work builds upon a full spatial<br>question-answering pipeline consisting of a vision...","title_summary":" History-Aware Question Answering in a Blocks<br>World Dialogue System","x":-36.4244041443,"y":31.4573497772,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-36.4244041443,"tsne_y":31.4573497772,"subcluster":0,"subcluster_description":"Blocks World Dialogue System","shape":"p"},{"cord_uid":"vaeyoxv7","source_x":"ArXiv","title":"Rapidly Bootstrapping a Question Answering Dataset for COVID-19","doi":null,"abstract":"We present CovidQA, the beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is the first publicly available resource of its type, and intended as a stopgap measure for guiding research until more substantial evaluation resources become available. While this dataset, comprising 124 question-article pairs as of the present version 0.1 release, does not have sufficient examples for supervised machine learning, we believe that it can be helpful for evaluating the zero-shot or transfer capabilities of existing models on topics specifically related to COVID-19. This paper describes our methodology for constructing the dataset and presents the effectiveness of a number of baselines, including term-based techniques and various transformer-based models. The dataset is available at http:\/\/covidqa.ai\/","publish_time":1587600000000,"author_summary":" Tang, Raphael; Nogueira, Rodrigo; Zhang,<br>Edwin; Gupta, Nikhil; Cam, Phuong; Cho, Kyunghyun;<br>Lin, Jimmy","abstract_summary":" We present CovidQA, the beginnings of a<br>question answering dataset specifically designed for<br>COVID-19, built by hand from knowledge gathered from<br>Kaggle's COVID-19 Open Research Dataset Challenge. To<br>our knowledge, this is the first publicly<br>available resource of its type, and intended as a stopgap<br>measure for guiding research until more substantial<br>evaluation resources become available. While this<br>dataset, comprising 124 question-article pairs as of<br>the present version 0.1 release, does not have<br>sufficient examples for supervised machine learning, we<br>believe that it can be helpful for evaluating the<br>zero-shot or transfer capabilities of existing models on<br>topics specifically related...","title_summary":" Rapidly Bootstrapping a Question Answering<br>Dataset for COVID-19","x":-34.4579658508,"y":32.9328613281,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.4579658508,"tsne_y":32.9328613281,"subcluster":23,"subcluster_description":"Community Question Retrieval","shape":"p"},{"cord_uid":"2l42genc","source_x":"ArXiv","title":"Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision","doi":null,"abstract":"We created this CORD-NER dataset with comprehensive named entity recognition (NER) on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus (2020-03-13). This CORD-NER dataset covers 75 fine-grained entity types: In addition to the common biomedical entity types (e.g., genes, chemicals and diseases), it covers many new entity types related explicitly to the COVID-19 studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates and immune responses), which may benefit research on COVID-19 related virus, spreading mechanisms, and potential vaccines. CORD-NER annotation is a combination of four sources with different NER methods. The quality of CORD-NER annotation surpasses SciSpacy (over 10% higher on the F1 score based on a sample set of documents), a fully supervised BioNER tool. Moreover, CORD-NER supports incrementally adding new documents as well as adding new entity types when needed by adding dozens of seeds as the input examples. We will constantly update CORD-NER based on the incremental updates of the CORD-19 corpus and the improvement of our system.","publish_time":1585267200000,"author_summary":" Wang, Xuan; Song, Xiangchen; Li, Bangzheng;<br>Guan, Yingjun; Han, Jiawei","abstract_summary":" We created this CORD-NER dataset with<br>comprehensive named entity recognition (NER) on the COVID-19<br>Open Research Dataset Challenge (CORD-19) corpus<br>(2020-03-13). This CORD-NER dataset covers 75 fine-grained<br>entity types: In addition to the common biomedical<br>entity types (e.g., genes, chemicals and diseases),<br>it covers many new entity types related<br>explicitly to the COVID-19 studies (e.g., coronaviruses,<br>viral proteins, evolution, materials, substrates<br>and immune responses), which may benefit research<br>on COVID-19 related virus, spreading<br>mechanisms, and potential vaccines. CORD-NER annotation<br>is a combination of four sources with different<br>NER methods. The quality of CORD-NER annotation<br>surpasses SciSpacy (over 10% higher on...","title_summary":" Comprehensive Named Entity Recognition on<br>CORD-19 with Distant or Weak Supervision","x":-31.501115799,"y":31.6610126495,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.501115799,"tsne_y":31.6610126495,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"bfkwy4uz","source_x":"ArXiv","title":"CompRes: A Dataset for Narrative Structure in News","doi":null,"abstract":"This paper addresses the task of automatically detecting narrative structures in raw texts. Previous works have utilized the oral narrative theory by Labov and Waletzky to identify various narrative elements in personal stories texts. Instead, we direct our focus to news articles, motivated by their growing social impact as well as their role in creating and shaping public opinion. We introduce CompRes -- the first dataset for narrative structure in news media. We describe the process in which the dataset was constructed: first, we designed a new narrative annotation scheme, better suited for news media, by adapting elements from the narrative theory of Labov and Waletzky (Complication and Resolution) and adding a new narrative element of our own (Success); then, we used that scheme to annotate a set of 29 English news articles (containing 1,099 sentences) collected from news and partisan websites. We use the annotated dataset to train several supervised models to identify the different narrative elements, achieving an $F_1$ score of up to 0.7. We conclude by suggesting several promising directions for future work.","publish_time":1594252800000,"author_summary":" Levi, Effi; Mor, Guy; Shenhav, Shaul; Sheafer,<br>Tamir","abstract_summary":" This paper addresses the task of automatically<br>detecting narrative structures in raw texts. Previous<br>works have utilized the oral narrative theory by<br>Labov and Waletzky to identify various narrative<br>elements in personal stories texts. Instead, we direct<br>our focus to news articles, motivated by their<br>growing social impact as well as their role in creating<br>and shaping public opinion. We introduce CompRes<br>-- the first dataset for narrative structure in<br>news media. We describe the process in which the<br>dataset was constructed: first, we designed a new<br>narrative annotation scheme, better suited for news<br>media, by adapting elements from the...","title_summary":" CompRes: A Dataset for Narrative Structure in<br>News","x":-33.9788398743,"y":31.3428115845,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.9788398743,"tsne_y":31.3428115845,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"cb57lo3o","source_x":"ArXiv","title":"An Evaluation of Publicly Available Deep Learning Based Commercial Information Retrieval Systems to search Biomedical Articles related to COVID-19","doi":null,"abstract":"The COVID-19 pandemic has resulted in a tremendous need for access to the latest scientific information, primarily through the use of text mining and search tools. This has led to both corpora for biomedical articles related to COVID-19 (such as the CORD-19 corpus (Wang et al., 2020)) as well as search engines to query such data. While most research in search engines is performed in the academic field of information retrieval (IR), most academic search engines--though rigorously evaluated--are sparsely utilized, while major commercial web search engines (e.g., Google, Bing) dominate. This relates to COVID-19 because it can be expected that commercial search engines deployed for the pandemic will gain much higher traction than those produced in academic labs, and thus leads to questions about the empirical performance of these search tools. This paper seeks to empirically evaluate two such commercial search engines for COVID-19, produced by Google and Amazon, in comparison to the more academic prototypes evaluated in the context of the TREC-COVID track (Roberts et al., 2020). To ensure a fair comparison, we limit the number of documents in the retrieved runs and also annotate additional documents. We find that the top-performing system from TREC-COVID on bpref metric performed the best among the different systems evaluated in this study on all the metrics.","publish_time":1593993600000,"author_summary":" Soni, Sarvesh; Roberts, Kirk","abstract_summary":" The COVID-19 pandemic has resulted in a<br>tremendous need for access to the latest scientific<br>information, primarily through the use of text mining and<br>search tools. This has led to both corpora for<br>biomedical articles related to COVID-19 (such as the<br>CORD-19 corpus (Wang et al., 2020)) as well as search<br>engines to query such data. While most research in<br>search engines is performed in the academic field of<br>information retrieval (IR), most academic search<br>engines--though rigorously evaluated--are sparsely<br>utilized, while major commercial web search engines<br>(e.g., Google, Bing) dominate. This relates to<br>COVID-19 because it can be expected that...","title_summary":" An Evaluation of Publicly Available Deep<br>Learning Based Commercial Information Retrieval<br>Systems to search Biomedical Articles related to<br>COVID-19","x":-30.9737644196,"y":31.2107887268,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.9737644196,"tsne_y":31.2107887268,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"m7w7aue8","source_x":"ArXiv","title":"Advances in Collaborative Filtering and Ranking","doi":null,"abstract":"In this dissertation, we cover some recent advances in collaborative filtering and ranking. In chapter 1, we give a brief introduction of the history and the current landscape of collaborative filtering and ranking; chapter 2 we first talk about pointwise collaborative filtering problem with graph information, and how our proposed new method can encode very deep graph information which helps four existing graph collaborative filtering algorithms; chapter 3 is on the pairwise approach for collaborative ranking and how we speed up the algorithm to near-linear time complexity; chapter 4 is on the new listwise approach for collaborative ranking and how the listwise approach is a better choice of loss for both explicit and implicit feedback over pointwise and pairwise loss; chapter 5 is about the new regularization technique Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it is both theoretically sound and empirically effectively for 6 different tasks across recommendation and natural language processing; chapter 6 is how we introduce personalization for the state-of-the-art sequential recommendation model with the help of SSE, which plays an important role in preventing our personalized model from overfitting to the training data; chapter 7, we summarize what we have achieved so far and predict what the future directions can be; chapter 8 is the appendix to all the chapters.","publish_time":1582761600000,"author_summary":" Wu, Liwei","abstract_summary":" In this dissertation, we cover some recent<br>advances in collaborative filtering and ranking. In<br>chapter 1, we give a brief introduction of the history<br>and the current landscape of collaborative<br>filtering and ranking; chapter 2 we first talk about<br>pointwise collaborative filtering problem with graph<br>information, and how our proposed new method can encode very<br>deep graph information which helps four existing<br>graph collaborative filtering algorithms; chapter<br>3 is on the pairwise approach for collaborative<br>ranking and how we speed up the algorithm to near-linear<br>time complexity; chapter 4 is on the new listwise<br>approach for collaborative ranking and how...","title_summary":" Advances in Collaborative Filtering and<br>Ranking","x":-31.5057983398,"y":34.3492927551,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.5057983398,"tsne_y":34.3492927551,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"808as0po","source_x":"ArXiv","title":"Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset: Preliminary Thoughts and Lessons Learned","doi":null,"abstract":"We present the Neural Covidex, a search engine that exploits the latest neural ranking architectures to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. This web application exists as part of a suite of tools that we have developed over the past few weeks to help domain experts tackle the ongoing global pandemic. We hope that improved information access capabilities to the scientific literature can inform evidence-based decision making and insight generation. This paper describes our initial efforts and offers a few thoughts about lessons we have learned along the way.","publish_time":1586476800000,"author_summary":" Zhang, Edwin; Gupta, Nikhil; Nogueira,<br>Rodrigo; Cho, Kyunghyun; Lin, Jimmy","abstract_summary":" We present the Neural Covidex, a search engine<br>that exploits the latest neural ranking<br>architectures to provide information access to the COVID-19<br>Open Research Dataset curated by the Allen<br>Institute for AI. This web application exists as part of a<br>suite of tools that we have developed over the past few<br>weeks to help domain experts tackle the ongoing<br>global pandemic. We hope that improved information<br>access capabilities to the scientific literature can<br>inform evidence-based decision making and insight<br>generation. This paper describes our initial efforts and<br>offers a few thoughts about lessons we have learned<br>along the way.","title_summary":" Rapidly Deploying a Neural Search Engine for<br>the COVID-19 Open Research Dataset: Preliminary<br>Thoughts and Lessons Learned","x":-30.4301548004,"y":30.4770870209,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.4301548004,"tsne_y":30.4770870209,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"hnutdxh5","source_x":"ArXiv","title":"COVID-19Base: A knowledgebase to explore biomedical entities related to COVID-19","doi":null,"abstract":"We are presenting COVID-19Base, a knowledgebase highlighting the biomedical entities related to COVID-19 disease based on literature mining. To develop COVID-19Base, we mine the information from publicly available scientific literature and related public resources. We considered seven topic-specific dictionaries, including human genes, human miRNAs, human lncRNAs, diseases, Protein Databank, drugs, and drug side effects, are integrated to mine all scientific evidence related to COVID-19. We have employed an automated literature mining and labeling system through a novel approach to measure the effectiveness of drugs against diseases based on natural language processing, sentiment analysis, and deep learning. To the best of our knowledge, this is the first knowledgebase dedicated to COVID-19, which integrates such large variety of related biomedical entities through literature mining. Proper investigation of the mined biomedical entities along with the identified interactions among those, reported in COVID-19Base, would help the research community to discover possible ways for the therapeutic treatment of COVID-19.","publish_time":1589241600000,"author_summary":" Khan, Junaed Younus; Khondaker, Md. Tawkat<br>Islam; Hoque, Iram Tazim; Al-Absi, Hamada; Rahman,<br>Mohammad Saifur; Alam, Tanvir; Rahman, M. Sohel","abstract_summary":" We are presenting COVID-19Base, a<br>knowledgebase highlighting the biomedical entities related<br>to COVID-19 disease based on literature mining.<br>To develop COVID-19Base, we mine the information<br>from publicly available scientific literature and<br>related public resources. We considered seven<br>topic-specific dictionaries, including human genes, human<br>miRNAs, human lncRNAs, diseases, Protein Databank,<br>drugs, and drug side effects, are integrated to mine<br>all scientific evidence related to COVID-19. We<br>have employed an automated literature mining and<br>labeling system through a novel approach to measure the<br>effectiveness of drugs against diseases based on natural<br>language processing, sentiment analysis, and deep<br>learning. To the best of...","title_summary":" COVID-19Base: A knowledgebase to explore<br>biomedical entities related to COVID-19","x":-30.6319694519,"y":30.9873867035,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6319694519,"tsne_y":30.9873867035,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"sot2y5y6","source_x":"ArXiv","title":"Continual BERT: Continual Learning for Adaptive Extractive Summarization of COVID-19 Literature","doi":null,"abstract":"The scientific community continues to publish an overwhelming amount of new research related to COVID-19 on a daily basis, leading to much literature without little to no attention. To aid the community in understanding the rapidly flowing array of COVID-19 literature, we propose a novel BERT architecture that provides a brief yet original summarization of lengthy papers. The model continually learns on new data in online fashion while minimizing catastrophic forgetting, thus fitting to the need of the community. Benchmark and manual examination of its performance show that the model provide a sound summary of new scientific literature.","publish_time":1594080000000,"author_summary":" Park, Jong Won","abstract_summary":" The scientific community continues to publish<br>an overwhelming amount of new research related to<br>COVID-19 on a daily basis, leading to much literature<br>without little to no attention. To aid the community in<br>understanding the rapidly flowing array of COVID-19<br>literature, we propose a novel BERT architecture that<br>provides a brief yet original summarization of lengthy<br>papers. The model continually learns on new data in<br>online fashion while minimizing catastrophic<br>forgetting, thus fitting to the need of the community.<br>Benchmark and manual examination of its performance show<br>that the model provide a sound summary of new<br>scientific literature.","title_summary":" Continual BERT: Continual Learning for<br>Adaptive Extractive Summarization of COVID-19<br>Literature","x":-34.8395118713,"y":33.3822288513,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.8395118713,"tsne_y":33.3822288513,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"dparg5wg","source_x":"ArXiv","title":"Automatic Textual Evidence Mining in COVID-19 Literature","doi":null,"abstract":"We created this EVIDENCEMINER system for automatic textual evidence mining in COVID-19 literature. EVIDENCEMINER is a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences. It is constructed in a completely automated way without any human effort for training data annotation. EVIDENCEMINER is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction. The named entities and meta-patterns are pre-computed and indexed offline to support fast online evidence retrieval. The annotation results are also highlighted in the original document for better visualization. EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization.","publish_time":1587945600000,"author_summary":" Wang, Xuan; Liu, Weili; Chauhan, Aabhas; Guan,<br>Yingjun; Han, Jiawei","abstract_summary":" We created this EVIDENCEMINER system for<br>automatic textual evidence mining in COVID-19<br>literature. EVIDENCEMINER is a web-based system that lets<br>users query a natural language statement and<br>automatically retrieves textual evidence from a background<br>corpora for life sciences. It is constructed in a<br>completely automated way without any human effort for<br>training data annotation. EVIDENCEMINER is supported<br>by novel data-driven methods for distantly<br>supervised named entity recognition and open information<br>extraction. The named entities and meta-patterns are<br>pre-computed and indexed offline to support fast online<br>evidence retrieval. The annotation results are also<br>highlighted in the original document for better<br>visualization....","title_summary":" Automatic Textual Evidence Mining in COVID-19<br>Literature","x":-34.0605354309,"y":34.121219635,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.0605354309,"tsne_y":34.121219635,"subcluster":22,"subcluster_description":"Image Query","shape":"p"},{"cord_uid":"6oiaf2cc","source_x":"ArXiv","title":"Automatic Text Summarization of COVID-19 Medical Research Articles using BERT and GPT-2","doi":null,"abstract":"With the COVID-19 pandemic, there is a growing urgency for medical community to keep up with the accelerating growth in the new coronavirus-related literature. As a result, the COVID-19 Open Research Dataset Challenge has released a corpus of scholarly articles and is calling for machine learning approaches to help bridging the gap between the researchers and the rapidly growing publications. Here, we take advantage of the recent advances in pre-trained NLP models, BERT and OpenAI GPT-2, to solve this challenge by performing text summarization on this dataset. We evaluate the results using ROUGE scores and visual inspection. Our model provides abstractive and comprehensive information based on keywords extracted from the original articles. Our work can help the the medical community, by providing succinct summaries of articles for which the abstract are not already available.","publish_time":1591142400000,"author_summary":" Kieuvongngam, Virapat; Tan, Bowen; Niu,<br>Yiming","abstract_summary":" With the COVID-19 pandemic, there is a growing<br>urgency for medical community to keep up with the<br>accelerating growth in the new coronavirus-related<br>literature. As a result, the COVID-19 Open Research Dataset<br>Challenge has released a corpus of scholarly articles and<br>is calling for machine learning approaches to<br>help bridging the gap between the researchers and<br>the rapidly growing publications. Here, we take<br>advantage of the recent advances in pre-trained NLP<br>models, BERT and OpenAI GPT-2, to solve this challenge<br>by performing text summarization on this<br>dataset. We evaluate the results using ROUGE scores and<br>visual inspection. Our model provides...","title_summary":" Automatic Text Summarization of COVID-19<br>Medical Research Articles using BERT and GPT-2","x":-30.6144866943,"y":30.7336540222,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.6144866943,"tsne_y":30.7336540222,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"wz7epr3n","source_x":"ArXiv","title":"EmotionGIF-Yankee: A Sentiment Classifier with Robust Model Based Ensemble Methods","doi":null,"abstract":"This paper provides a method to classify sentiment with robust model based ensemble methods. We preprocess tweet data to enhance coverage of tokenizer. To reduce domain bias, we first train tweet dataset for pre-trained language model. Besides, each classifier has its strengths and weakness, we leverage different types of models with ensemble methods: average and power weighted sum. From the experiments, we show that our approach has achieved positive effect for sentiment classification. Our system reached third place among 26 teams from the evaluation in SocialNLP 2020 EmotionGIF competition.","publish_time":1593907200000,"author_summary":" Wang, Wei-Yao; Chang, Kai-Shiang; Tang,<br>Yu-Chien","abstract_summary":" This paper provides a method to classify<br>sentiment with robust model based ensemble methods. We<br>preprocess tweet data to enhance coverage of tokenizer. To<br>reduce domain bias, we first train tweet dataset for<br>pre-trained language model. Besides, each classifier has<br>its strengths and weakness, we leverage different<br>types of models with ensemble methods: average and<br>power weighted sum. From the experiments, we show<br>that our approach has achieved positive effect for<br>sentiment classification. Our system reached third<br>place among 26 teams from the evaluation in SocialNLP<br>2020 EmotionGIF competition.","title_summary":" EmotionGIF-Yankee: A Sentiment Classifier<br>with Robust Model Based Ensemble Methods","x":-34.0369300842,"y":30.3684043884,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-34.0369300842,"tsne_y":30.3684043884,"subcluster":3,"subcluster_description":"Sentiment Analysis","shape":"p"},{"cord_uid":"tkpbfroi","source_x":"ArXiv","title":"COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation","doi":null,"abstract":"To combat COVID-19, clinicians and scientists all need to digest the vast amount of relevant biomedical knowledge in literature to understand the disease mechanism and the related biological functions. We have developed a novel and comprehensive knowledge discovery framework, COVID-KG, which leverages novel semantic representation and external ontologies to represent text and images in the input literature data, and then performs various extraction components to extract fine-grained multimedia knowledge elements (entities, relations and events). We then exploit the constructed multimedia KGs for question answering and report generation, using drug repurposing as a case study. Our framework also provides detailed contextual sentences, subfigures and knowledge subgraphs as evidence. All of the data, KGs, resources, and shared services are publicly available.","publish_time":1593561600000,"author_summary":" Wang, Qingyun; Li, Manling; Wang, Xuan;<br>Parulian, Nikolaus; Han, Guangxing; Ma, Jiawei; Tu,<br>Jingxuan; Lin, Ying; Zhang, Haoran; Liu, Weili; Chauhan,<br>Aabhas; Guan, Yingjun; Li, Bangzheng; Li, Ruisong;<br>Song, Xiangchen; Ji, Heng; Han, Jiawei; Chang,<br>Shih-Fu; Pustejovsky, James; Rah, Jasmine; Liem,<br>David; Elsayed, Ahmed; Palmer, Martha; Voss, Clare;<br>Schneider, Cynthia; Onyshkevych, Boyan","abstract_summary":" To combat COVID-19, clinicians and scientists<br>all need to digest the vast amount of relevant<br>biomedical knowledge in literature to understand the<br>disease mechanism and the related biological<br>functions. We have developed a novel and comprehensive<br>knowledge discovery framework, COVID-KG, which<br>leverages novel semantic representation and external<br>ontologies to represent text and images in the input<br>literature data, and then performs various extraction<br>components to extract fine-grained multimedia knowledge<br>elements (entities, relations and events). We then<br>exploit the constructed multimedia KGs for question<br>answering and report generation, using drug repurposing<br>as a case study. Our framework also provides<br>detailed contextual...","title_summary":" COVID-19 Literature Knowledge Graph<br>Construction and Drug Repurposing Report Generation","x":-30.9071311951,"y":31.7493267059,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.9071311951,"tsne_y":31.7493267059,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"dlxnckra","source_x":"ArXiv","title":"Answering Questions on COVID-19 in Real-Time","doi":null,"abstract":"The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our effort to contribute to shrinking this knowledge vacuum by creating covidAsk, a question answering (QA) system that combines biomedical text mining and QA techniques to provide answers to questions in real-time. Our system leverages both supervised and unsupervised approaches to provide informative answers using DenSPI (Seo et al., 2019) and BEST (Lee et al., 2016). Evaluation of covidAsk is carried out by using a manually created dataset called COVID-19 Questions which is based on facts about COVID-19. We hope our system will be able to aid researchers in their search for knowledge and information not only for COVID-19 but for future pandemics as well.","publish_time":1593388800000,"author_summary":" Lee, Jinhyuk; Yi, Sean S.; Jeong, Minbyul;<br>Sung, Mujeen; Yoon, Wonjin; Choi, Yonghwa; Ko,<br>Miyoung; Kang, Jaewoo","abstract_summary":" The recent outbreak of the novel coronavirus is<br>wreaking havoc on the world and researchers are<br>struggling to effectively combat it. One reason why the<br>fight is difficult is due to the lack of information<br>and knowledge. In this work, we outline our effort<br>to contribute to shrinking this knowledge vacuum<br>by creating covidAsk, a question answering (QA)<br>system that combines biomedical text mining and QA<br>techniques to provide answers to questions in real-time.<br>Our system leverages both supervised and<br>unsupervised approaches to provide informative answers<br>using DenSPI (Seo et al., 2019) and BEST (Lee et al.,<br>2016). Evaluation of...","title_summary":" Answering Questions on COVID-19 in Real-Time","x":-30.4010066986,"y":30.5987262726,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.4010066986,"tsne_y":30.5987262726,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"o366395x","source_x":"ArXiv","title":"Building a PubMed knowledge graph","doi":null,"abstract":"PubMed is an essential resource for the medical domain, but useful concepts are either difficult to extract or are ambiguated, which has significantly hindered knowledge discovery. To address this issue, we constructed a PubMed knowledge graph (PKG) by extracting bio-entities from 29 million PubMed abstracts, disambiguating author names, integrating funding data through the National Institutes of Health (NIH) ExPORTER, collecting affiliation history and educational background of authors from ORCID, and identifying fine-grained affiliation data from MapAffil. Through the integration of the credible multi-source data, we could create connections among the bio-entities, authors, articles, affiliations, and funding. Data validation revealed that the BioBERT deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the F1 score (by 0.51%), with the author name disambiguation (AND) achieving a F1 score of 98.09%. PKG can trigger broader innovations, not only enabling us to measure scholarly impact, knowledge usage, and knowledge transfer, but also assisting us in profiling authors and organizations based on their connections with bio-entities. The PKG is freely available on Figshare (https:\/\/figshare.com\/s\/6327a55355fc2c99f3a2, simplified version that exclude PubMed raw data) and TACC website (http:\/\/er.tacc.utexas.edu\/datasets\/ped, full version).","publish_time":1588896000000,"author_summary":" Xu, Jian; Kim, Sunkyu; Song, Min; Jeong,<br>Minbyul; Kim, Donghyeon; Kang, Jaewoo; Rousseau,<br>Justin F.; Li, Xin; Xu, Weijia; Torvik, Vetle I.; Bu,<br>Yi; Chen, Chongyan; Ebeid, Islam Akef; Li,<br>Daifeng; Ding, Ying","abstract_summary":" PubMed is an essential resource for the medical<br>domain, but useful concepts are either difficult to<br>extract or are ambiguated, which has significantly<br>hindered knowledge discovery. To address this issue, we<br>constructed a PubMed knowledge graph (PKG) by extracting<br>bio-entities from 29 million PubMed abstracts,<br>disambiguating author names, integrating funding data<br>through the National Institutes of Health (NIH)<br>ExPORTER, collecting affiliation history and<br>educational background of authors from ORCID, and<br>identifying fine-grained affiliation data from MapAffil.<br>Through the integration of the credible multi-source<br>data, we could create connections among the<br>bio-entities, authors, articles, affiliations, and<br>funding. Data validation revealed that...","title_summary":" Building a PubMed knowledge graph","x":-32.0780296326,"y":32.633228302,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.0780296326,"tsne_y":32.633228302,"subcluster":6,"subcluster_description":"Pubmed Knowledge Graphbuilding","shape":"p"},{"cord_uid":"t7tjvpxv","source_x":"Elsevier; PMC","title":"An intelligent Chatbot using deep learning with Bidirectional RNN and attention model","doi":"10.1016\/j.matpr.2020.05.450","abstract":"This paper shows the modeling and performance in deep learning computation for an Assistant Conversational Agent (Chatbot). The utilization of Tensorflow software library, particularly Neural Machine Translation (NMT) model. Acquiring knowledge for modeling is one of the most important task and quite difficult to preprocess it. The Bidirectional Recurrent Neural Networks (BRNN) containing attention layers is used, so that input sentence with large number of tokens (or sentences with more than 20\u201340 words) can be replied with more appropriate conversation. The dataset used in the paper for training of model is used from Reddit. The model is developed to perform English to English translation. The main purpose of this work is to increase the perplexity and learning rate of the model and find Bleu Score for translation in same language. The experiments are conducted using Tensorflow using python 3.6. The perplexity, leaning rate, Bleu score and Average time per 1000 steps are 56.10, 0.0001, 30.16 and 4.5 respectively. One epoch is completed at 23,000 steps. The paper also study MacBook Air as a system for neural network and deep learning.","publish_time":1591747200000,"author_summary":" Dhyani, Manyu; Kumar, Rajiv","abstract_summary":" This paper shows the modeling and performance<br>in deep learning computation for an Assistant<br>Conversational Agent (Chatbot). The utilization of<br>Tensorflow software library, particularly Neural<br>Machine Translation (NMT) model. Acquiring knowledge<br>for modeling is one of the most important task and<br>quite difficult to preprocess it. The Bidirectional<br>Recurrent Neural Networks (BRNN) containing attention<br>layers is used, so that input sentence with large<br>number of tokens (or sentences with more than 20\u201340<br>words) can be replied with more appropriate<br>conversation. The dataset used in the paper for training of<br>model is used from Reddit. The model is developed to<br>perform...","title_summary":" An intelligent Chatbot using deep learning<br>with Bidirectional RNN and attention model","x":-35.2048950195,"y":31.8023834229,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-35.2048950195,"tsne_y":31.8023834229,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"eaglecq7","source_x":"BioRxiv; MedRxiv","title":"TWIRLS, an automated topic-wise inference method based on massive literature, suggests a possible mechanism via ACE2 for the pathological changes in the human host after coronavirus infection","doi":"10.1101\/2020.02.27.967588","abstract":"Faced with the current large-scale public health emergency, collecting, sorting, and analyzing biomedical information related to the \u201ccoronavirus\u201d should be done as quickly as possible to gain a global perspective, which is a basic requirement for strengthening epidemic control capacity. However, for human researchers studying the viruses and the hosts, the vast amount of information available cannot be processed effectively and in a timely manner, particularly when the scientific understanding may be limited, which can further lower the information processing efficiency. We present TWIRLS, a method that can automatically acquire, organize, and classify information. Additionally, independent functional data sources can be added to build an inference system using a machine-based approach, which can provide relevant knowledge to help human researchers quickly establish subject cognition and to make more effective decisions. TWIRLS can automatically analyze more than three million words in more than 14,000 literature articles in only 4 hours. Combining with generalized gene interaction databases creates a data interface that can help researchers to further analyze the information. Using the TWIRLS system, we found that an important regulatory factor angiotensin-converting enzyme 2 (ACE2) may be involved in the host pathological changes on binding to the coronavirus after infection. After triggering functional changes in ACE2\/AT2R, an imbalance in the steady-state cytokine regulatory axis involving the Renin-Angiotensin System and IP-10 leads to a cytokine storm.","publish_time":1583107200000,"author_summary":" Ji, Xiaoyang; Zhang, Chunming; Zhai, Yubo;<br>Zhang, Zhonghai; Zhang, Chunli; Xue, Yiqing; Tan,<br>Guangming; Niu, Gang","abstract_summary":" Faced with the current large-scale public<br>health emergency, collecting, sorting, and<br>analyzing biomedical information related to the<br>\u201ccoronavirus\u201d should be done as quickly as possible to gain a<br>global perspective, which is a basic requirement for<br>strengthening epidemic control capacity. However, for human<br>researchers studying the viruses and the hosts, the vast<br>amount of information available cannot be processed<br>effectively and in a timely manner, particularly when the<br>scientific understanding may be limited, which can<br>further lower the information processing efficiency.<br>We present TWIRLS, a method that can<br>automatically acquire, organize, and classify information.<br>Additionally, independent functional data sources can...","title_summary":" TWIRLS, an automated topic-wise inference<br>method based on massive literature, suggests a<br>possible mechanism via ACE2 for the pathological<br>changes in the human host after coronavirus infection","x":-29.9640636444,"y":31.3831863403,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-29.9640636444,"tsne_y":31.3831863403,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"kl9oi7q5","source_x":"Medline; PMC","title":"Constructing Co-occurrence Network Embeddings to Assist Association Extraction for COVID-19 and Other Coronavirus Infectious Diseases","doi":"10.1093\/jamia\/ocaa117","abstract":"OBJECTIVE: As COVID-19 started its rapid emergence and gradually transformed into an unprecedented pandemic, the need for having a knowledge repository for the disease became crucial. To address this issue, a new COVID-19 machine readable dataset known as COVID-19 Open Research Dataset (CORD-19) has been released. Based on this, our objective was to build a computable co-occurrence network embeddings to assist association detection amongst COVID-19 related biomedical entities. MATERIALS AND METHODS: Leveraging a Linked Data version of CORD-19 (i.e., CORD-19-on-FHIR), we first utilized SPARQL to extract co-occurrences among chemicals, diseases, genes, and mutations and build a co-occurrence network. We then trained the representation of the derived co-occurrence network using node2vec with four edge embeddings operations (L1, L2, Average, and Hadamard). Six algorithms (Decision Tree, Linear Regression, Support Vector Machine, Random Forest, Naive Bayes, and Multi-layer Perceptron) were applied to evaluate performance on link prediction. An unsupervised learning strategy was also developed incorporating the t-SNE and DBSCAN algorithms for case studies. RESULTS: Random Forest classifier showed the best performance on link prediction across different network embeddings. For edge embeddings generated using the Average operation, Random Forest achieved the optimal average precision of 0.97 and F1 score of 0.90. For unsupervised learning, 63 clusters were formed with silhouette score of 0.128. Significant associations were detected for five coronavirus infectious diseases in their corresponding subgroups. CONCLUSION: In this study, we constructed COVID-19-centered co-occurrence network embeddings. Results indicated that the generated embeddings were able to extract significant associations for COVID-19 and coronavirus infectious diseases.","publish_time":1590537600000,"author_summary":" Oniani, David; Jiang, Guoqian; Liu, Hongfang;<br>Shen, Feichen","abstract_summary":" OBJECTIVE: As COVID-19 started its rapid<br>emergence and gradually transformed into an<br>unprecedented pandemic, the need for having a knowledge<br>repository for the disease became crucial. To address this<br>issue, a new COVID-19 machine readable dataset known<br>as COVID-19 Open Research Dataset (CORD-19) has<br>been released. Based on this, our objective was to<br>build a computable co-occurrence network<br>embeddings to assist association detection amongst<br>COVID-19 related biomedical entities. MATERIALS AND<br>METHODS: Leveraging a Linked Data version of CORD-19<br>(i.e., CORD-19-on-FHIR), we first utilized SPARQL to<br>extract co-occurrences among chemicals, diseases,<br>genes, and mutations and build a co-occurrence<br>network. We then...","title_summary":" Constructing Co-occurrence Network<br>Embeddings to Assist Association Extraction for COVID-19<br>and Other Coronavirus Infectious Diseases","x":-30.2644042969,"y":30.9399452209,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.2644042969,"tsne_y":30.9399452209,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"},{"cord_uid":"7ypg25l0","source_x":"Elsevier; PMC","title":"Entity Perception of Two-Step-Matching Framework for Public Opinions","doi":"10.1016\/j.jnlssr.2020.06.005","abstract":"Abstract Entity perception of ambiguous user comments is a critical problem of target identification for huge amount of public opinions. In this paper, a Two-Step-Matching method is proposed to identify the precise target entity from multiple entities mentioned. Firstly, potential entities are extracted by BiLSTM-CRF model and characteristic words by TF-IDF model from public comments. Secondly, the first matching is implemented between potential entities and an official business directory by Jaro-Winkler distance algorithm. Then, in order to find the precise one, an industry-characteristic dictionary is developed into the second matching process. The precise entity is identified according to the count of characteristic words matching to industry-characteristic dictionary. In addition, associated rate (global indicator) and accuracy rate (sample indicator) are defined for evaluation of matching accuracy. The results for three data sets of public opinions about major public health events show that the highest associated rate and accuracy rate arrive at 0.93 and 0.95, averagely enhanced by 32% and 30% above the case of using the first matching process alone. This framework provides the method to find the true target entity of really wanted expression from public opinions.","publish_time":1593475200000,"author_summary":" Li, Ren-De; Ma, Hao-Tian; Wang, Zi-Yi; Guo,<br>Qiang; Liu, Jian-Guo","abstract_summary":" Abstract Entity perception of ambiguous user<br>comments is a critical problem of target identification<br>for huge amount of public opinions. In this paper, a<br>Two-Step-Matching method is proposed to identify the precise<br>target entity from multiple entities mentioned.<br>Firstly, potential entities are extracted by<br>BiLSTM-CRF model and characteristic words by TF-IDF model<br>from public comments. Secondly, the first matching<br>is implemented between potential entities and an<br>official business directory by Jaro-Winkler distance<br>algorithm. Then, in order to find the precise one, an<br>industry-characteristic dictionary is developed into the second<br>matching process. The precise entity is identified<br>according to the count...","title_summary":" Entity Perception of Two-Step-Matching<br>Framework for Public Opinions","x":-33.1432418823,"y":32.7734375,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.1432418823,"tsne_y":32.7734375,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"h17sz6x1","source_x":"Medline; PMC","title":"Building a PubMed knowledge graph","doi":"10.1038\/s41597-020-0543-2","abstract":"PubMed(\u00ae) is an essential resource for the medical domain, but useful concepts are either difficult to extract or are ambiguous, which has significantly hindered knowledge discovery. To address this issue, we constructed a PubMed knowledge graph (PKG) by extracting bio-entities from 29 million PubMed abstracts, disambiguating author names, integrating funding data through the National Institutes of Health (NIH) ExPORTER, collecting affiliation history and educational background of authors from ORCID(\u00ae), and identifying fine-grained affiliation data from MapAffil. Through the integration of these credible multi-source data, we could create connections among the bio-entities, authors, articles, affiliations, and funding. Data validation revealed that the BioBERT deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the F1 score (by 0.51%), with the author name disambiguation (AND) achieving an F1 score of 98.09%. PKG can trigger broader innovations, not only enabling us to measure scholarly impact, knowledge usage, and knowledge transfer, but also assisting us in profiling authors and organizations based on their connections with bio-entities.","publish_time":1593129600000,"author_summary":" Xu, Jian; Kim, Sunkyu; Song, Min; Jeong,<br>Minbyul; Kim, Donghyeon; Kang, Jaewoo; Rousseau,<br>Justin F.; Li, Xin; Xu, Weijia; Torvik, Vetle I.; Bu,<br>Yi; Chen, Chongyan; Ebeid, Islam Akef; Li,<br>Daifeng; Ding, Ying","abstract_summary":" PubMed(\u00ae) is an essential resource for the<br>medical domain, but useful concepts are either<br>difficult to extract or are ambiguous, which has<br>significantly hindered knowledge discovery. To address this<br>issue, we constructed a PubMed knowledge graph (PKG)<br>by extracting bio-entities from 29 million<br>PubMed abstracts, disambiguating author names,<br>integrating funding data through the National Institutes<br>of Health (NIH) ExPORTER, collecting<br>affiliation history and educational background of authors<br>from ORCID(\u00ae), and identifying fine-grained<br>affiliation data from MapAffil. Through the integration of<br>these credible multi-source data, we could create<br>connections among the bio-entities, authors, articles,<br>affiliations, and funding. Data validation revealed that...","title_summary":" Building a PubMed knowledge graph","x":-32.0738868713,"y":32.6267280579,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-32.0738868713,"tsne_y":32.6267280579,"subcluster":6,"subcluster_description":"Pubmed Knowledge Graphbuilding","shape":"p"},{"cord_uid":"mw7g32u1","source_x":"Elsevier; PMC","title":"Harvesting Patterns from Textual Web Sources with Tolerance Rough Sets","doi":"10.1016\/j.patter.2020.100053","abstract":"Construction of knowledge repositories from web corpora by harvesting linguistic patterns is of benefit for many natural language-processing applications that rely on question-answering schemes. These methods require minimal or no human intervention and can recursively learn new relational facts-instances in a fully automated and scalable manner. This paper explores the performance of tolerance rough set-based learner with respect to two important issues: scalability and its effect on concept drift, by (1) designing a new version of the semi-supervised tolerance rough set-based pattern learner (TPL 2.0), (2) adapting a tolerance form of rough set methodology to categorize linguistic patterns, and (3) extracting categorical information from a large noisy dataset of crawled web pages. This work demonstrates that the TPL 2.0 learner is promising in terms of precision@30 metric when compared with three benchmark algorithms: Tolerant Pattern Learner 1.0, Fuzzy-Rough Set Pattern Learner, and Coupled Bayesian Sets-based learner.","publish_time":1593129600000,"author_summary":" Moghaddam, Hoora Rezaei; Ramanna, Sheela","abstract_summary":" Construction of knowledge repositories from<br>web corpora by harvesting linguistic patterns is<br>of benefit for many natural language-processing<br>applications that rely on question-answering schemes.<br>These methods require minimal or no human<br>intervention and can recursively learn new relational<br>facts-instances in a fully automated and scalable manner. This<br>paper explores the performance of tolerance rough<br>set-based learner with respect to two important issues:<br>scalability and its effect on concept drift, by (1)<br>designing a new version of the semi-supervised tolerance<br>rough set-based pattern learner (TPL 2.0), (2)<br>adapting a tolerance form of rough set methodology to<br>categorize linguistic patterns, and (3)...","title_summary":" Harvesting Patterns from Textual Web Sources<br>with Tolerance Rough Sets","x":-33.76171875,"y":34.7758522034,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-33.76171875,"tsne_y":34.7758522034,"subcluster":14,"subcluster_description":"Few-Shot Large Document Classificationkeyword","shape":"p"},{"cord_uid":"wr64fzjf","source_x":"Elsevier; Medline; PMC","title":"A relevance and quality-based ranking algorithm applied to evidence-based medicine","doi":"10.1016\/j.cmpb.2020.105415","abstract":"Abstract Background The amount of information available about millions of different subjects is growing every day. This has led to the birth of new search tools specialized in different domains, because classical information retrieval models have trouble dealing with the special characteristics of some of these domains. Evidence-based Medicine is a case of a complex domain where classical information retrieval models can help search engines retrieve documents by considering the presence or absence of terms, but these must be complemented with other specific strategies which allow retrieving and ranking documents including the best current evidence and methodological quality. Objective The goal is to present a ranking algorithm able to select the best documents for clinicians considering aspects related to the relevance and the quality of said documents. Methods In order to assess the effectiveness of this proposal, an experimental methodology has been followed by using Medline as a data set and the Cochrane Library as a gold standard. Results Applying the evaluation methodology proposed, and after submitting 40 queries on the platform developed, the MAP (Mean Average Precision) obtained was 20.26%. Conclusions Successful results have been achieved with the experiments, improving on other studies, but under different and even more complex circumstances.","publish_time":1596153600000,"author_summary":" Serrano-Guerrero, Jesus; Romero, Francisco<br>P.; Olivas, Jose A.","abstract_summary":" Abstract Background The amount of information<br>available about millions of different subjects is<br>growing every day. This has led to the birth of new search<br>tools specialized in different domains, because<br>classical information retrieval models have trouble<br>dealing with the special characteristics of some of<br>these domains. Evidence-based Medicine is a case of a<br>complex domain where classical information retrieval<br>models can help search engines retrieve documents by<br>considering the presence or absence of terms, but these must<br>be complemented with other specific strategies<br>which allow retrieving and ranking documents<br>including the best current evidence and methodological<br>quality. Objective The...","title_summary":" A relevance and quality-based ranking<br>algorithm applied to evidence-based medicine","x":-31.9276695251,"y":32.5763435364,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-31.9276695251,"tsne_y":32.5763435364,"subcluster":6,"subcluster_description":"Pubmed Knowledge Graphbuilding","shape":"p"},{"cord_uid":"p4gr0xcb","source_x":"Elsevier; Medline; PMC","title":"A hierarchical temporal attention-based LSTM encoder-decoder model for individual mobility prediction","doi":"10.1016\/j.neucom.2020.03.080","abstract":"Prediction of individual mobility is crucial in human mobility related applications. Whereas, existing research on individual mobility prediction mainly focuses on next location prediction and short-term dependencies between traveling locations. Long-term location sequence prediction is of great importance for long-time traffic planning and location advertising, and long-term dependencies exist as individual mobility regularity typically occurs daily and weekly. This paper proposes a novel hierarchical temporal attention-based LSTM encoder-decoder model for individual location sequence prediction. The proposed hierarchical attention mechanism captures both long-term and short-term dependencies underlying in individual longitudinal trajectories, and uncovers frequential and periodical mobility patterns in an interpretable manner by incorporating the calendar cycle of individual travel regularities into location prediction. More specifically, the hierarchical attention consists of local temporal attention to identify highly related locations in each day, and global temporal attention to discern important travel regularities over a week. Experiments on individual trajectory datasets with varying degree of traveling uncertainty demonstrate that our method outperforms four baseline methods on three evaluation metrics. In addition, we explore the interpretability of the proposed model in understanding individual daily, and weekly mobility patterns by visualizing the temporal attention weights and frequent traveling patterns associated with locations.","publish_time":1598313600000,"author_summary":" Li, Fa; Gui, Zhipeng; Zhang, Zhaoyu; Peng,<br>Dehua; Tian, Siyu; Yuan, Kunxiaojia; Sun, Yunzeng;<br>Wu, Huayi; Gong, Jianya; Lei, Yichen","abstract_summary":" Prediction of individual mobility is crucial<br>in human mobility related applications.<br>Whereas, existing research on individual mobility<br>prediction mainly focuses on next location prediction and<br>short-term dependencies between traveling locations.<br>Long-term location sequence prediction is of great<br>importance for long-time traffic planning and location<br>advertising, and long-term dependencies exist as<br>individual mobility regularity typically occurs daily<br>and weekly. This paper proposes a novel<br>hierarchical temporal attention-based LSTM<br>encoder-decoder model for individual location sequence<br>prediction. The proposed hierarchical attention<br>mechanism captures both long-term and short-term<br>dependencies underlying in individual longitudinal<br>trajectories, and uncovers frequential and periodical<br>mobility patterns in an interpretable...","title_summary":" A hierarchical temporal attention-based LSTM<br>encoder-decoder model for individual mobility prediction","x":-30.0723381042,"y":34.3338775635,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.0723381042,"tsne_y":34.3338775635,"subcluster":2,"subcluster_description":"Content Based Recommendation","shape":"p"},{"cord_uid":"fdrdopxx","source_x":"Medline; PMC","title":"TREC-COVID: Rationale and Structure of an Information Retrieval Shared Task for COVID-19","doi":"10.1093\/jamia\/ocaa091","abstract":"TREC-COVID is an information retrieval (IR) shared task initiated to support clinicians and clinical research during the COVID-19 pandemic. IR for pandemics breaks many normal assumptions, which can be seen by examining nine important basic IR research questions related to pandemic situations. TREC-COVID differs from traditional IR shared task evaluations with special considerations for the expected users, IR modality considerations, topic development, participant requirements, assessment process, relevance criteria, evaluation metrics, iteration process, projected timeline, and the implications of data use as a post-task test collection. This article describes how all these were addressed for the particular requirements of developing IR systems under a pandemic situation. Finally, initial participation numbers are also provided, which demonstrate the tremendous interest the IR community has in this effort.","publish_time":1588550400000,"author_summary":" Roberts, Kirk; Alam, Tasmeer; Bedrick,<br>Steven; Demner-Fushman, Dina; Lo, Kyle; Soboroff,<br>Ian; Voorhees, Ellen; Wang, Lucy Lu; Hersh, William<br>R","abstract_summary":" TREC-COVID is an information retrieval (IR)<br>shared task initiated to support clinicians and<br>clinical research during the COVID-19 pandemic. IR for<br>pandemics breaks many normal assumptions, which can be<br>seen by examining nine important basic IR research<br>questions related to pandemic situations. TREC-COVID<br>differs from traditional IR shared task evaluations<br>with special considerations for the expected<br>users, IR modality considerations, topic<br>development, participant requirements, assessment<br>process, relevance criteria, evaluation metrics,<br>iteration process, projected timeline, and the<br>implications of data use as a post-task test collection. This<br>article describes how all these were addressed for the<br>particular requirements of developing IR...","title_summary":" TREC-COVID: Rationale and Structure of an<br>Information Retrieval Shared Task for COVID-19","x":-30.3530769348,"y":30.252035141,"cluster":3,"cluster_name":"c4","cluster_description":"Knowledge Graph Information","tsne_x":-30.3530769348,"tsne_y":30.252035141,"subcluster":1,"subcluster_description":"Covid-19 Medical Research","shape":"p"}]