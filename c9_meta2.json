[{"cord_uid":"0jh0945y","source_x":"PMC","title":"Research on covert communication channel based on modulation of common compressed speech codec","doi":"10.1007\/s00521-020-04882-y","abstract":"As is well known, multimedia has been widely used in VoIP and mobile communications. Research on how to establish covert communication channel over the above popular public applications has been flourishing in recent years. This paper tries to present a novel and effective method to construct a covert channel over common compressed speech stream by embedding sense information into it. In our method, after analysing the characteristic features of the excitation pulse positions of the ITU-T G.723.1 and G.729A speech codec, we design a novel and effective covert communication channel by finely modulating the codes of excitation pulse positions of the above two codecs in line with the secret information to be hidden. To improve the embedding capacity of the proposed method, we also use all the odd\/even characteristics of pulse code positions to conduct information hiding. To test and verify the proposed approach, experiments are conducted on several different scenarios. Experimental results show that our methods and algorithms perform a higher degree of secrecy and sound information embedding efficacy compared with exiting similar methods.","publish_time":1586736000000,"author_summary":" Li, Fufang; Li, Binbin; Huang, Yongfeng; Feng,<br>Yuanyong; Peng, Lingxi; Zhou, Naqin","abstract_summary":" As is well known, multimedia has been widely<br>used in VoIP and mobile communications. Research on<br>how to establish covert communication channel<br>over the above popular public applications has been<br>flourishing in recent years. This paper tries to present a<br>novel and effective method to construct a covert<br>channel over common compressed speech stream by<br>embedding sense information into it. In our method, after<br>analysing the characteristic features of the excitation<br>pulse positions of the ITU-T G.723.1 and G.729A<br>speech codec, we design a novel and effective covert<br>communication channel by finely modulating the codes of<br>excitation pulse positions of...","title_summary":" Research on covert communication channel<br>based on modulation of common compressed speech<br>codec","x":-11.988360405,"y":41.1564292908,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.988360405,"tsne_y":41.1564292908,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"nypu4w9s","source_x":"PMC","title":"SlideImages: A Dataset for Educational Image Classification","doi":"10.1007\/978-3-030-45442-5_36","abstract":"In the past few years, convolutional neural networks (CNNs) have achieved impressive results in computer vision tasks, which however mainly focus on photos with natural scene content. Besides, non-sensor derived images such as illustrations, data visualizations, figures, etc. are typically used to convey complex information or to explore large datasets. However, this kind of images has received little attention in computer vision. CNNs and similar techniques use large volumes of training data. Currently, many document analysis systems are trained in part on scene images due to the lack of large datasets of educational image data. In this paper, we address this issue and present SlideImages, a dataset for the task of classifying educational illustrations. SlideImages contains training data collected from various sources, e.g., Wikimedia Commons and the AI2D dataset, and test data collected from educational slides. We have reserved all the actual educational images as a test dataset in order to ensure that the approaches using this dataset generalize well to new educational images, and potentially other domains. Furthermore, we present a baseline system using a standard deep neural architecture and discuss dealing with the challenge of limited training data.","publish_time":1585008000000,"author_summary":" Morris, David; M\u00fcller-Budack, Eric; Ewerth,<br>Ralph","abstract_summary":" In the past few years, convolutional neural<br>networks (CNNs) have achieved impressive results in<br>computer vision tasks, which however mainly focus on<br>photos with natural scene content. Besides,<br>non-sensor derived images such as illustrations, data<br>visualizations, figures, etc. are typically used to convey<br>complex information or to explore large datasets.<br>However, this kind of images has received little<br>attention in computer vision. CNNs and similar<br>techniques use large volumes of training data. Currently,<br>many document analysis systems are trained in part<br>on scene images due to the lack of large datasets of<br>educational image data. In this paper, we address...","title_summary":" SlideImages: A Dataset for Educational Image<br>Classification","x":-9.1914844513,"y":41.4188499451,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.1914844513,"tsne_y":41.4188499451,"subcluster":6,"subcluster_description":"Educational Image Classificationrobust 3D","shape":"p"},{"cord_uid":"apf2g736","source_x":"PMC","title":"Motion Words: A Text-Like Representation of 3D Skeleton Sequences","doi":"10.1007\/978-3-030-45439-5_35","abstract":"There is a growing amount of human motion data captured as a continuous 3D skeleton sequence without any information about its semantic partitioning. To make such unsegmented and unlabeled data efficiently accessible, we propose to transform them into a text-like representation and employ well-known text retrieval models. Specifically, we partition each motion synthetically into a sequence of short segments and quantize the segments into motion words, i.e. compact features with similar characteristics as words in text documents. We introduce several quantization techniques for building motion-word vocabularies and propose application-independent criteria for assessing the vocabulary quality. We verify these criteria on two real-life application scenarios.","publish_time":1584403200000,"author_summary":" Sedmidubsky, Jan; Budikova, Petra; Dohnal,<br>Vlastislav; Zezula, Pavel","abstract_summary":" There is a growing amount of human motion data<br>captured as a continuous 3D skeleton sequence without<br>any information about its semantic partitioning.<br>To make such unsegmented and unlabeled data<br>efficiently accessible, we propose to transform them into a<br>text-like representation and employ well-known text<br>retrieval models. Specifically, we partition each<br>motion synthetically into a sequence of short<br>segments and quantize the segments into motion words,<br>i.e. compact features with similar<br>characteristics as words in text documents. We introduce<br>several quantization techniques for building<br>motion-word vocabularies and propose<br>application-independent criteria for assessing the vocabulary<br>quality. We verify these criteria on...","title_summary":" Motion Words: A Text-Like Representation of 3D<br>Skeleton Sequences","x":-10.3001756668,"y":42.1723327637,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.3001756668,"tsne_y":42.1723327637,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"d6r4fr9r","source_x":"PMC","title":"Biconditional Generative Adversarial Networks for Multiview Learning with Missing Views","doi":"10.1007\/978-3-030-45439-5_53","abstract":"In this paper, we present a conditional GAN with two generators and a common discriminator for multiview learning problems where observations have two views, but one of them may be missing for some of the training samples. This is for example the case for multilingual collections where documents are not available in all languages. Some studies tackled this problem by assuming the existence of view generation functions to approximately complete the missing views; for example Machine Translation to translate documents into the missing languages. These functions generally require an external resource to be set and their quality has a direct impact on the performance of the learned multiview classifier over the completed training set. Our proposed approach addresses this problem by jointly learning the missing views and the multiview classifier using a tripartite game with two generators and a discriminator. Each of the generators is associated to one of the views and tries to fool the discriminator by generating the other missing view conditionally on the corresponding observed view. The discriminator then tries to identify if for an observation, one of its views is completed by one of the generators or if both views are completed along with its class. Our results on a subset of Reuters RCV1\/RCV2 collections show that the discriminator achieves significant classification performance; and that the generators learn the missing views with high quality without the need of any consequent external resource.","publish_time":1584403200000,"author_summary":" Doinychko, Anastasiia; Amini, Massih-Reza","abstract_summary":" In this paper, we present a conditional GAN with<br>two generators and a common discriminator for<br>multiview learning problems where observations have two<br>views, but one of them may be missing for some of the<br>training samples. This is for example the case for<br>multilingual collections where documents are not available<br>in all languages. Some studies tackled this<br>problem by assuming the existence of view generation<br>functions to approximately complete the missing views;<br>for example Machine Translation to translate<br>documents into the missing languages. These functions<br>generally require an external resource to be set and their<br>quality has a direct...","title_summary":" Biconditional Generative Adversarial<br>Networks for Multiview Learning with Missing Views","x":-11.2994241714,"y":40.2467727661,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.2994241714,"tsne_y":40.2467727661,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"shauvo3j","source_x":"PMC","title":"Using Open Source Libraries in the Development of Control Systems Based on Machine Vision","doi":"10.1007\/978-3-030-47240-5_7","abstract":"The possibility of the boundaries detection in the images of crushed ore particles using a convolutional neural network is analyzed. The structure of the neural network is given. The construction of training and test datasets of ore particle images is described. Various modifications of the underlying neural network have been investigated. Experimental results are presented.","publish_time":1588636800000,"author_summary":" Kruglov, Vasiliy N.","abstract_summary":" The possibility of the boundaries detection in<br>the images of crushed ore particles using a<br>convolutional neural network is analyzed. The structure of<br>the neural network is given. The construction of<br>training and test datasets of ore particle images is<br>described. Various modifications of the underlying<br>neural network have been investigated. Experimental<br>results are presented.","title_summary":" Using Open Source Libraries in the Development<br>of Control Systems Based on Machine Vision","x":-9.049826622,"y":41.040512085,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.049826622,"tsne_y":41.040512085,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"30cx62vb","source_x":"PMC","title":"Curiosity-Driven Variational Autoencoder for Deep Q Network","doi":"10.1007\/978-3-030-47426-3_59","abstract":"In recent years, deep reinforcement learning (DRL) has achieved tremendous success in high-dimensional and large-scale space control and sequential decision-making tasks. However, the current model-free DRL methods suffer from low sample efficiency, which is a bottleneck that limits their performance. To alleviate this problem, some researchers used the generative model for modeling the environment. But the generative model may become inaccurate or even collapse if the state has not been sufficiently explored. In this paper, we introduce a model called Curiosity-driven Variational Autoencoder (CVAE), which combines variational autoencoder and curiosity-driven exploration. During the training process, the CVAE model can improve sample efficiency while curiosity-driven exploration can make sufficient exploration in a complex environment. Then, a CVAE-based algorithm is proposed, namely DQN-CVAE, that scales CVAE to higher dimensional environments. Finally, the performance of our algorithm is evaluated through several Atari 2600 games, and the experimental results show that the DQN-CVAE achieves better performance in terms of average reward per episode on these games.","publish_time":1587081600000,"author_summary":" Han, Gao-Jie; Zhang, Xiao-Fang; Wang, Hao;<br>Mao, Chen-Guang","abstract_summary":" In recent years, deep reinforcement learning<br>(DRL) has achieved tremendous success in<br>high-dimensional and large-scale space control and sequential<br>decision-making tasks. However, the current model-free DRL<br>methods suffer from low sample efficiency, which is a<br>bottleneck that limits their performance. To alleviate<br>this problem, some researchers used the generative<br>model for modeling the environment. But the<br>generative model may become inaccurate or even collapse if<br>the state has not been sufficiently explored. In<br>this paper, we introduce a model called<br>Curiosity-driven Variational Autoencoder (CVAE), which<br>combines variational autoencoder and<br>curiosity-driven exploration. During the training process, the<br>CVAE model can improve...","title_summary":" Curiosity-Driven Variational Autoencoder<br>for Deep Q Network","x":-12.4691257477,"y":40.1399688721,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.4691257477,"tsne_y":40.1399688721,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"xm4vyv3r","source_x":"PMC","title":"Accelerating Hyperparameter Optimization of Deep Neural Network via Progressive Multi-Fidelity Evaluation","doi":"10.1007\/978-3-030-47426-3_58","abstract":"Deep neural networks usually require careful tuning of hyperparameters to show their best performance. However, with the size of state-of-the-art neural networks growing larger, the evaluation cost of the traditional Bayesian optimization has become unacceptable in most cases. Moreover, most practical problems usually require good hyperparameter configurations within a limited time budget. To speed up the hyperparameter optimization, the successive halving technique is used to stop poorly-performed configurations as early as possible. In this paper, we propose a novel hyperparameter optimization method FastHO, which combines the progressive multi-fidelity technique with successive halving under a multi-armed bandit framework. Furthermore, we employ Bayesian optimization to guide the selection of initial configurations and an efficient data subsampling based method to warm start the surrogate model of Bayesian optimization. Extensive empirical evaluation on a broad range of neural networks and datasets shows that FastHO is not only effective to speed up hyperparameter optimization but also can achieve better anytime performance and final performance than the state-of-the-art hyperparameter optimization methods.","publish_time":1587081600000,"author_summary":" Zhu, Guanghui; Zhu, Ruancheng","abstract_summary":" Deep neural networks usually require careful<br>tuning of hyperparameters to show their best<br>performance. However, with the size of state-of-the-art<br>neural networks growing larger, the evaluation cost<br>of the traditional Bayesian optimization has<br>become unacceptable in most cases. Moreover, most<br>practical problems usually require good hyperparameter<br>configurations within a limited time budget. To speed up the<br>hyperparameter optimization, the successive halving<br>technique is used to stop poorly-performed<br>configurations as early as possible. In this paper, we propose a<br>novel hyperparameter optimization method FastHO,<br>which combines the progressive multi-fidelity<br>technique with successive halving under a multi-armed<br>bandit framework. Furthermore, we employ...","title_summary":" Accelerating Hyperparameter Optimization of<br>Deep Neural Network via Progressive<br>Multi-Fidelity Evaluation","x":-12.632478714,"y":40.2711715698,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.632478714,"tsne_y":40.2711715698,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"5cn42qq5","source_x":"PMC","title":"6GCVAE: Gated Convolutional Variational Autoencoder for IPv6 Target Generation","doi":"10.1007\/978-3-030-47426-3_47","abstract":"IPv6 scanning has always been a challenge for researchers in the field of network measurement. Due to the considerable IPv6 address space, while recent network speed and computational power have been improved, using a brute-force approach to probe the entire network space of IPv6 is almost impossible. Systems are required an algorithmic approach to generate more possible active target candidate sets to probe. In this paper, we first try to use deep learning to design such IPv6 target generation algorithms. The model effectively learns the address structure by stacking the gated convolutional layer to construct Variational Autoencoder (VAE). We also introduce two address classification methods to improve the model effect of the target generation. Experiments indicate that our approach 6GCVAE outperformed the conventional VAE models and the state of the art target generation algorithm in two active address datasets.","publish_time":1587081600000,"author_summary":" Cui, Tianyu; Gou, Gaopeng; Xiong, Gang","abstract_summary":" IPv6 scanning has always been a challenge for<br>researchers in the field of network measurement. Due to the<br>considerable IPv6 address space, while recent network speed<br>and computational power have been improved, using<br>a brute-force approach to probe the entire<br>network space of IPv6 is almost impossible. Systems are<br>required an algorithmic approach to generate more<br>possible active target candidate sets to probe. In this<br>paper, we first try to use deep learning to design such<br>IPv6 target generation algorithms. The model<br>effectively learns the address structure by stacking the<br>gated convolutional layer to construct Variational<br>Autoencoder (VAE). We also...","title_summary":" 6GCVAE: Gated Convolutional Variational<br>Autoencoder for IPv6 Target Generation","x":-10.8828315735,"y":40.4412117004,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.8828315735,"tsne_y":40.4412117004,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"hl7u2fub","source_x":"PMC","title":"Prototype Similarity Learning for Activity Recognition","doi":"10.1007\/978-3-030-47426-3_50","abstract":"Human Activity Recognition (HAR) plays an irreplaceable role in various applications such as security, gaming, and assisted living. Recent studies introduce deep learning to mitigate the manual feature extraction (i.e., data representation) efforts and achieve high accuracy. However, there are still challenges in learning accurate representations for sensory data due to the weakness of representation modules and the subject variances. We propose a scheme called Distance-based HAR from Ensembled spatial-temporal Representations (DHARER) to address above challenges. The idea behind DHARER is straightforward\u2014the same activities should have similar representations. We first learn representations of the input sensory segments and latent prototype representations of each class, using a Convolution Neural Network (CNN)-based dual-stream representation module; then the learned representations are projected to activity types by measuring their similarity to the learned prototypes. We have conducted extensive experiments under a strict subject-independent setting on three large-scale datasets to evaluate the proposed scheme, and our experimental results demonstrate superior performance of DHARER to several state-of-the-art methods.","publish_time":1587081600000,"author_summary":" Bai, Lei; Yao, Lina; Wang, Xianzhi; Kanhere,<br>Salil S.; Xiao, Yang","abstract_summary":" Human Activity Recognition (HAR) plays an<br>irreplaceable role in various applications such as security,<br>gaming, and assisted living. Recent studies introduce<br>deep learning to mitigate the manual feature<br>extraction (i.e., data representation) efforts and<br>achieve high accuracy. However, there are still<br>challenges in learning accurate representations for<br>sensory data due to the weakness of representation<br>modules and the subject variances. We propose a scheme<br>called Distance-based HAR from Ensembled<br>spatial-temporal Representations (DHARER) to address above<br>challenges. The idea behind DHARER is straightforward\u2014the<br>same activities should have similar<br>representations. We first learn representations of the input<br>sensory segments and latent prototype...","title_summary":" Prototype Similarity Learning for Activity<br>Recognition","x":-9.8090963364,"y":40.5190544128,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.8090963364,"tsne_y":40.5190544128,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"r253ygx0","source_x":"PMC","title":"Cross-data Automatic Feature Engineering via Meta-learning and Reinforcement Learning","doi":"10.1007\/978-3-030-47426-3_63","abstract":"Feature Engineering (FE) is one of the most beneficial, yet most difficult and time-consuming tasks of machine learning projects, and requires strong expert knowledge. It is thus significant to design generalized ways to perform FE. The primary difficulties arise from the multiform information to consider, the potentially infinite number of possible features and the high computational cost of feature generation and evaluation. We present a framework called Cross-data Automatic Feature Engineering Machine (CAFEM), which formalizes the FE problem as an optimization problem over a Feature Transformation Graph (FTG). CAFEM contains two components: a FE learner (FeL) that learns fine-grained FE strategies on one single dataset by Double Deep Q-learning (DDQN) and a Cross-data Component (CdC) that speeds up FE learning on an unseen dataset by the generalized FE policies learned by Meta-Learning on a collection of datasets. We compare the performance of FeL with several existing state-of-the-art automatic FE techniques on a large collection of datasets. It shows that FeL outperforms existing approaches and is robust on the selection of learning algorithms. Further experiments also show that CdC can not only speed up FE learning but also increase learning performance.","publish_time":1587081600000,"author_summary":" Zhang, Jianyu; Hao, Jianye; Fogelman-Souli\u00e9,<br>Fran\u00e7oise","abstract_summary":" Feature Engineering (FE) is one of the most<br>beneficial, yet most difficult and time-consuming tasks of<br>machine learning projects, and requires strong expert<br>knowledge. It is thus significant to design generalized<br>ways to perform FE. The primary difficulties arise<br>from the multiform information to consider, the<br>potentially infinite number of possible features and the<br>high computational cost of feature generation and<br>evaluation. We present a framework called Cross-data<br>Automatic Feature Engineering Machine (CAFEM), which<br>formalizes the FE problem as an optimization problem over a<br>Feature Transformation Graph (FTG). CAFEM contains<br>two components: a FE learner (FeL) that learns<br>fine-grained FE...","title_summary":" Cross-data Automatic Feature Engineering via<br>Meta-learning and Reinforcement Learning","x":-12.6556634903,"y":39.8897094727,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.6556634903,"tsne_y":39.8897094727,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"p77j6nhr","source_x":"PMC","title":"Human Activity Recognition Using Semi-supervised Multi-modal DEC for Instagram Data","doi":"10.1007\/978-3-030-47426-3_67","abstract":"Human Activity Recognition (HAR) using social media provides a solid basis for a variety of context-aware applications. Existing HAR approaches have adopted supervised machine learning algorithms using texts and their meta-data such as time, venue, and keywords. However, their recognition accuracy may decrease when applied to image-sharing social media where users mostly describe their daily activities and thoughts using both texts and images. In this paper, we propose a semi-supervised multi-modal deep embedding clustering method to recognize human activities on Instagram. Our proposed method learns multi-modal feature representations by alternating a supervised learning phase and an unsupervised learning phase. By utilizing a large number of unlabeled data, it learns a more generalized feature distribution for each HAR class and avoids overfitting to limited labeled data. Evaluation results show that leveraging multi-modality and unlabeled data is effective for HAR and our method outperforms existing approaches.","publish_time":1587081600000,"author_summary":" Kim, Dongmin; Han, Sumin; Son, Heesuk; Lee,<br>Dongman","abstract_summary":" Human Activity Recognition (HAR) using social<br>media provides a solid basis for a variety of<br>context-aware applications. Existing HAR approaches have<br>adopted supervised machine learning algorithms using<br>texts and their meta-data such as time, venue, and<br>keywords. However, their recognition accuracy may<br>decrease when applied to image-sharing social media<br>where users mostly describe their daily activities<br>and thoughts using both texts and images. In this<br>paper, we propose a semi-supervised multi-modal deep<br>embedding clustering method to recognize human<br>activities on Instagram. Our proposed method learns<br>multi-modal feature representations by alternating a<br>supervised learning phase and an unsupervised learning<br>phase. By...","title_summary":" Human Activity Recognition Using<br>Semi-supervised Multi-modal DEC for Instagram Data","x":-10.0587320328,"y":40.2482261658,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.0587320328,"tsne_y":40.2482261658,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"24w3n4jj","source_x":"PMC","title":"Mask-Guided Region Attention Network for Person Re-Identification","doi":"10.1007\/978-3-030-47436-2_22","abstract":"Person re-identification (ReID) is an important and practical task which identifies pedestrians across non-overlapping surveillance cameras based on their visual features. In general, ReID is an extremely challenging task due to complex background clutters, large pose variations and severe occlusions. To improve its performance, a robust and discriminative feature extraction methodology is particularly crucial. Recently, the feature alignment technique driven by human pose estimation, that is, matching two person images with their corresponding parts, increases the effectiveness of ReID to a certain extent. However, we argue that there are still a few problems among these methods such as imprecise handcrafted segmentation of body parts, and some improvements can be further achieved. In this paper, we present a novel framework called Mask-Guided Region Attention Network (MGRAN) for person ReID. MGRAN consists of two major components: Mask-guided Region Attention (MRA) and Multi-feature Alignment (MA). MRA aims to generate spatial attention masks and meanwhile mask out the background clutters and occlusions. Moreover, the generated masks are utilized for region-level feature alignment in the MA module. We then evaluate the proposed method on three public datasets, including Market-1501, DukeMTMC-reID and CUHK03. Extensive experiments with ablation analysis show the effectiveness of this method.","publish_time":1587081600000,"author_summary":" Zhou, Cong; Yu, Han","abstract_summary":" Person re-identification (ReID) is an<br>important and practical task which identifies<br>pedestrians across non-overlapping surveillance cameras<br>based on their visual features. In general, ReID is an<br>extremely challenging task due to complex background<br>clutters, large pose variations and severe occlusions.<br>To improve its performance, a robust and<br>discriminative feature extraction methodology is<br>particularly crucial. Recently, the feature alignment<br>technique driven by human pose estimation, that is,<br>matching two person images with their corresponding<br>parts, increases the effectiveness of ReID to a<br>certain extent. However, we argue that there are still a<br>few problems among these methods such as imprecise<br>handcrafted...","title_summary":" Mask-Guided Region Attention Network for<br>Person Re-Identification","x":-9.2177085876,"y":41.8422584534,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2177085876,"tsne_y":41.8422584534,"subcluster":7,"subcluster_description":"Cnnan Efficient Image Descriptor","shape":"p"},{"cord_uid":"odz5vts8","source_x":"PMC","title":"Deep Multimodal Clustering with Cross Reconstruction","doi":"10.1007\/978-3-030-47426-3_24","abstract":"Recently, there has been surging interests in multimodal clustering. And extracting common features plays a critical role in these methods. However, since the ignorance of the fact that data in different modalities shares similar distributions in feature space, most works did not mining the inter-modal distribution relationships completely, which eventually leads to unacceptable common features. To address this issue, we propose the deep multimodal clustering with cross reconstruction method, which firstly focuses on multimodal feature extraction in an unsupervised way and then clusters these extracted features. The proposed cross reconstruction aims to build latent connections among different modalities, which effectively reduces the distribution differences in feature space. The theoretical analysis shows that the cross reconstruction reduces the Wasserstein distance of multimodal feature distributions. Experimental results on six benchmark datasets demonstrate that our method achieves obviously improvement over several state-of-arts.","publish_time":1587081600000,"author_summary":" Zhang, Xianchao; Tang, Xiaorui; Zong, Linlin;<br>Liu, Xinyue; Mu, Jie","abstract_summary":" Recently, there has been surging interests in<br>multimodal clustering. And extracting common features<br>plays a critical role in these methods. However,<br>since the ignorance of the fact that data in different<br>modalities shares similar distributions in feature<br>space, most works did not mining the inter-modal<br>distribution relationships completely, which eventually<br>leads to unacceptable common features. To address<br>this issue, we propose the deep multimodal<br>clustering with cross reconstruction method, which<br>firstly focuses on multimodal feature extraction in an<br>unsupervised way and then clusters these extracted<br>features. The proposed cross reconstruction aims to<br>build latent connections among different<br>modalities, which effectively...","title_summary":" Deep Multimodal Clustering with Cross<br>Reconstruction","x":-11.7816896439,"y":40.2803955078,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.7816896439,"tsne_y":40.2803955078,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"ewql8io7","source_x":"PMC","title":"Multi-Layer Cross Loss Model for Zero-Shot Human Activity Recognition","doi":"10.1007\/978-3-030-47426-3_17","abstract":"Most existing methods of human activity recognition are based on supervised learning. These methods can only recognize classes which appear in the training dataset, but are out of work when the classes are not in the training dataset. Zero-shot learning aims at solving this problem. In this paper, we propose a novel model termed Multi-Layer Cross Loss Model (MLCLM). Our model has two novel ideas: (1) In the model, we design a multi-nonlinear layers model to project features to semantic space for that the deeper the network is, the better the network can fit the data\u2019s distribution. (2) A novel objective function combining mean square loss and cross entropy loss is designed for the zero-shot learning task. We have conduct sufficient experiments to evaluate the proposed model on three benchmark datasets. Experiments show that our model outperforms other state-of-the-art methods significantly in zero-shot human activity recognition.","publish_time":1587081600000,"author_summary":" Wu, Tong; Chen, Yiqiang; Gu, Yang; Wang, Jiwei;<br>Zhang, Siyu; Zhechen, Zhanghu","abstract_summary":" Most existing methods of human activity<br>recognition are based on supervised learning. These<br>methods can only recognize classes which appear in the<br>training dataset, but are out of work when the classes are<br>not in the training dataset. Zero-shot learning<br>aims at solving this problem. In this paper, we<br>propose a novel model termed Multi-Layer Cross Loss<br>Model (MLCLM). Our model has two novel ideas: (1) In<br>the model, we design a multi-nonlinear layers<br>model to project features to semantic space for that<br>the deeper the network is, the better the network<br>can fit the data\u2019s distribution. (2) A novel...","title_summary":" Multi-Layer Cross Loss Model for Zero-Shot<br>Human Activity Recognition","x":-9.9437150955,"y":40.5969276428,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.9437150955,"tsne_y":40.5969276428,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"i6puqauk","source_x":"PMC","title":"Deep Multivariate Time Series Embedding Clustering via Attentive-Gated Autoencoder","doi":"10.1007\/978-3-030-47426-3_25","abstract":"Nowadays, great quantities of data are produced by a large and diverse family of sensors (e.g., remote sensors, biochemical sensors, wearable devices), which typically measure multiple variables over time, resulting in data streams that can be profitably organized as multivariate time-series. In practical scenarios, the speed at which such information is collected often makes the data labeling task uneasy and too expensive, so that limit the use of supervised approaches. For this reason, unsupervised and exploratory methods represent a fundamental tool to deal with the analysis of multivariate time series. In this paper we propose a deep-learning based framework for clustering multivariate time series data with varying lengths. Our framework, namely DeTSEC (Deep Time Series Embedding Clustering), includes two stages: firstly a recurrent autoencoder exploits attention and gating mechanisms to produce a preliminary embedding representation; then, a clustering refinement stage is introduced to stretch the embedding manifold towards the corresponding clusters. Experimental assessment on six real-world benchmarks coming from different domains has highlighted the effectiveness of our proposal.","publish_time":1587081600000,"author_summary":" Ienco, Dino; Interdonato, Roberto","abstract_summary":" Nowadays, great quantities of data are<br>produced by a large and diverse family of sensors (e.g.,<br>remote sensors, biochemical sensors, wearable<br>devices), which typically measure multiple variables<br>over time, resulting in data streams that can be<br>profitably organized as multivariate time-series. In<br>practical scenarios, the speed at which such information<br>is collected often makes the data labeling task<br>uneasy and too expensive, so that limit the use of<br>supervised approaches. For this reason, unsupervised and<br>exploratory methods represent a fundamental tool to deal<br>with the analysis of multivariate time series. In<br>this paper we propose a deep-learning based<br>framework for...","title_summary":" Deep Multivariate Time Series Embedding<br>Clustering via Attentive-Gated Autoencoder","x":-12.6021127701,"y":39.6952819824,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.6021127701,"tsne_y":39.6952819824,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"f4ibgn9i","source_x":"PMC","title":"Self-supervised Learning for Semi-supervised Time Series Classification","doi":"10.1007\/978-3-030-47426-3_39","abstract":"Self-supervised learning is a promising new technique for learning representative features in the absence of manual annotations. It is particularly efficient in cases where labeling the training data is expensive and tedious, naturally linking it to the semi-supervised learning paradigm. In this work, we propose a new semi-supervised time series classification model that leverages features learned from the self-supervised task on unlabeled data. The idea is to exploit the unlabeled training data with a forecasting task which provides a strong surrogate supervision signal for feature learning. We draw from established multi-task learning approaches and model forecasting as an auxiliary task to be optimized jointly with the main task of classification. We evaluate our proposed method on benchmark time series classification datasets in semi-supervised setting and are able to show that it significantly outperforms the state-of-the-art baselines.","publish_time":1587081600000,"author_summary":" Jawed, Shayan; Grabocka, Josif;<br>Schmidt-Thieme, Lars","abstract_summary":" Self-supervised learning is a promising new<br>technique for learning representative features in the<br>absence of manual annotations. It is particularly<br>efficient in cases where labeling the training data is<br>expensive and tedious, naturally linking it to the<br>semi-supervised learning paradigm. In this work, we propose a<br>new semi-supervised time series classification<br>model that leverages features learned from the<br>self-supervised task on unlabeled data. The idea is to exploit<br>the unlabeled training data with a forecasting<br>task which provides a strong surrogate supervision<br>signal for feature learning. We draw from established<br>multi-task learning approaches and model forecasting as<br>an auxiliary task...","title_summary":" Self-supervised Learning for<br>Semi-supervised Time Series Classification","x":-12.8721408844,"y":39.6762275696,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.8721408844,"tsne_y":39.6762275696,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"fd293rwg","source_x":"PMC","title":"L0-norm Constrained Autoencoders for Unsupervised Outlier Detection","doi":"10.1007\/978-3-030-47436-2_51","abstract":"Unsupervised outlier detection is commonly performed using reconstruction-based methods such as Principal Component Analysis. A recent problem in this field is the learning of low-dimensional nonlinear manifolds under L0-norm constraints for error terms. Despite significant efforts, no method that consistently treats such features exists. We propose a novel unsupervised outlier detection method, L0-norm Constrained Autoencoders (L0-AE), based on an autoencoder-based detector with L0-norm constraints for error terms. Unlike existing methods, the proposed optimization procedure of L0-AE provably guarantees the convergence of the objective function under a mild condition, while neither the relaxation of the L0-norm constraint nor the linearity of the latent manifold is enforced. Experimental results show that the proposed L0-AE is more robust and accurate than other reconstruction-based methods, as well as conventional methods such as Isolation Forest.","publish_time":1587081600000,"author_summary":" Ishii, Yoshinao; Koide, Satoshi; Hayakawa,<br>Keiichiro","abstract_summary":" Unsupervised outlier detection is commonly<br>performed using reconstruction-based methods such as<br>Principal Component Analysis. A recent problem in this<br>field is the learning of low-dimensional nonlinear<br>manifolds under L0-norm constraints for error terms.<br>Despite significant efforts, no method that<br>consistently treats such features exists. We propose a novel<br>unsupervised outlier detection method, L0-norm<br>Constrained Autoencoders (L0-AE), based on an<br>autoencoder-based detector with L0-norm constraints for error<br>terms. Unlike existing methods, the proposed<br>optimization procedure of L0-AE provably guarantees the<br>convergence of the objective function under a mild<br>condition, while neither the relaxation of the L0-norm<br>constraint nor the linearity of...","title_summary":" L0-norm Constrained Autoencoders for<br>Unsupervised Outlier Detection","x":-12.6161966324,"y":40.3549232483,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.6161966324,"tsne_y":40.3549232483,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"r5wpmz0w","source_x":"PMC","title":"Adversarial Autoencoder and Multi-Task Semi-Supervised Learning for Multi-stage Process","doi":"10.1007\/978-3-030-47436-2_1","abstract":"In selection processes, decisions follow a sequence of stages. Early stages have more applicants and general information, while later stages have fewer applicants but specific data. This is represented by a dual funnel structure, in which the sample size decreases from one stage to the other while the information increases. Training classifiers for this case is challenging. In the early stages, the information may not contain distinct patterns to learn, causing underfitting. In later stages, applicants have been filtered out and the small sample can cause overfitting. We redesign the multi-stage problem to address both cases by combining adversarial autoencoders (AAE) and multi-task semi-supervised learning (MTSSL) to train an end-to-end neural network for all stages together. The AAE learns the representation of the data and performs data imputation in missing values. The generated dataset is fed to an MTSSL mechanism that trains all stages together, encouraging related tasks to contribute to each other using a temporal regularization structure. Using real-world data, we show that our approach outperforms other state-of-the-art methods with a gain of 4x over the standard case and a 12% improvement over the second-best method.","publish_time":1587081600000,"author_summary":" Mendes, Andre; Togelius, Julian; dos Santos<br>Coelho, Leandro","abstract_summary":" In selection processes, decisions follow a<br>sequence of stages. Early stages have more applicants<br>and general information, while later stages have<br>fewer applicants but specific data. This is<br>represented by a dual funnel structure, in which the sample<br>size decreases from one stage to the other while the<br>information increases. Training classifiers for this case<br>is challenging. In the early stages, the<br>information may not contain distinct patterns to learn,<br>causing underfitting. In later stages, applicants<br>have been filtered out and the small sample can cause<br>overfitting. We redesign the multi-stage problem to address<br>both cases by combining adversarial autoencoders...","title_summary":" Adversarial Autoencoder and Multi-Task<br>Semi-Supervised Learning for Multi-stage Process","x":-11.9571170807,"y":40.0025062561,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.9571170807,"tsne_y":40.0025062561,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"umxw9qlb","source_x":"PMC","title":"Identifying Near-Miss Traffic Incidents in Event Recorder Data","doi":"10.1007\/978-3-030-47436-2_54","abstract":"Front video and sensor data captured by vehicle-mounted event recorders are used for not only traffic accident evidence but also safe-driving education as near-miss traffic incident data. However, most event recorder (ER) data shows only regular driving events. To utilize near-miss data for safe-driving education, we need to be able to easily and rapidly locate the appropriate data from large amounts of ER data through labels attached to the scenes\/events of interest. This paper proposes a method that can automatically identify near-misses with objects such as pedestrians and bicycles by processing the ER data. The proposed method extracts two deep feature representations that consider car status and the environment surrounding the car. The first feature representation is generated by considering the temporal transitions of car status. The second one can extract the positional relationship between the car and surrounding objects by processing object detection results. Experiments on actual ER data demonstrate that the proposed method can accurately identify and tag near-miss events.","publish_time":1587081600000,"author_summary":" Yamamoto, Shuhei; Kurashima, Takeshi; Toda,<br>Hiroyuki","abstract_summary":" Front video and sensor data captured by<br>vehicle-mounted event recorders are used for not only traffic<br>accident evidence but also safe-driving education as<br>near-miss traffic incident data. However, most event<br>recorder (ER) data shows only regular driving events. To<br>utilize near-miss data for safe-driving education, we<br>need to be able to easily and rapidly locate the<br>appropriate data from large amounts of ER data through<br>labels attached to the scenes\/events of interest.<br>This paper proposes a method that can automatically<br>identify near-misses with objects such as pedestrians<br>and bicycles by processing the ER data. The<br>proposed method extracts two deep...","title_summary":" Identifying Near-Miss Traffic Incidents in<br>Event Recorder Data","x":-9.197974205,"y":40.5520896912,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.197974205,"tsne_y":40.5520896912,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"duie9zp9","source_x":"PMC","title":"Optimal Topology Search for Fast Model Averaging in Decentralized Parallel SGD","doi":"10.1007\/978-3-030-47436-2_67","abstract":"Distributed training of deep learning models on high-latency systems necessitates decentralized parallel SGD solutions. However, existing solutions suffer from slow convergence because of hand-crafted topologies. The question arises, \u201cfor decentralized parallel SGD, is it possible to learn a topology that provides faster model averaging compared to the hand-crafted counterparts?\u201d. By leveraging spectral properties of the graph, we formulate the objective function for finding the topology that provides fast model averaging. Since direct optimization of the objective function is infeasible, we employ a local search algorithm guided by the objective function. We show through extensive empirical evaluation on image classification tasks that the model averaging based on learned topologies leads to fast convergence. An equally important aspect of the decentralized parallel SGD is the link weights for sparse model averaging. In contrast to setting weights via Metropolis-Hastings, we propose to use Laplacian link weights on the learned topologies, which provide a significant lift in performance.","publish_time":1587081600000,"author_summary":" Jameel, Mohsan; Jawed, Shayan;<br>Schmidt-Thieme, Lars","abstract_summary":" Distributed training of deep learning models<br>on high-latency systems necessitates<br>decentralized parallel SGD solutions. However, existing<br>solutions suffer from slow convergence because of<br>hand-crafted topologies. The question arises, \u201cfor<br>decentralized parallel SGD, is it possible to learn a topology<br>that provides faster model averaging compared to<br>the hand-crafted counterparts?\u201d. By leveraging<br>spectral properties of the graph, we formulate the<br>objective function for finding the topology that<br>provides fast model averaging. Since direct<br>optimization of the objective function is infeasible, we<br>employ a local search algorithm guided by the<br>objective function. We show through extensive empirical<br>evaluation on image classification tasks...","title_summary":" Optimal Topology Search for Fast Model<br>Averaging in Decentralized Parallel SGD","x":-12.7708444595,"y":40.3099021912,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.7708444595,"tsne_y":40.3099021912,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"lt2nxbzm","source_x":"PMC","title":"MsFcNET: Multi-scale Feature-Crossing Attention Network for Multi-field Sparse Data","doi":"10.1007\/978-3-030-47426-3_12","abstract":"Feature engineering usually needs to excavate dense-and-implicit cross features from multi-filed sparse data. Recently, many state-of-the-art models have been proposed to achieve low-order and high-order feature interactions. However, most of them ignore the importance of cross features and fail to suppress the negative impact of useless features. In this paper, a novel multi-scale feature-crossing attention network (MsFcNET) is proposed to extract dense-and-implicit cross features and learn their importance in the different scales. The model adopts the DIA-LSTM units to construct a new attention calibration architecture, which can adaptively adjust the weights of features in the process of feature interactions. On the other hand, it also integrates a multi-scale feature-crossing module to strengthen the representation ability of cross features from multi-field sparse data. The extensive experimental results on three real-world prediction datasets demonstrate that our proposed model yields superior performance compared with the other state-of-the-art models.","publish_time":1587081600000,"author_summary":" Xie, Zhifeng; Zhang, Wenling; Ding, Huiming;<br>Ma, Lizhuang","abstract_summary":" Feature engineering usually needs to excavate<br>dense-and-implicit cross features from multi-filed sparse data.<br>Recently, many state-of-the-art models have been<br>proposed to achieve low-order and high-order feature<br>interactions. However, most of them ignore the importance of<br>cross features and fail to suppress the negative<br>impact of useless features. In this paper, a novel<br>multi-scale feature-crossing attention network<br>(MsFcNET) is proposed to extract dense-and-implicit<br>cross features and learn their importance in the<br>different scales. The model adopts the DIA-LSTM units to<br>construct a new attention calibration architecture,<br>which can adaptively adjust the weights of features<br>in the process of feature interactions. On...","title_summary":" MsFcNET: Multi-scale Feature-Crossing<br>Attention Network for Multi-field Sparse Data","x":-12.3736925125,"y":39.9118766785,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.3736925125,"tsne_y":39.9118766785,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"7tsp5hf7","source_x":"PMC","title":"Multi-view Deep Gaussian Process with a Pre-training Acceleration Technique","doi":"10.1007\/978-3-030-47436-2_23","abstract":"Deep Gaussian process (DGP) is one of the popular probabilistic modeling methods, which is powerful and widely used for function approximation and uncertainty estimation. However, the traditional DGP lacks consideration for multi-view cases in which data may come from different sources or be constructed by different types of features. In this paper, we propose a generalized multi-view DGP (MvDGP) to capture the characteristics of different views and model data in different views discriminately. In order to make the proposed model more efficient in training, we introduce a pre-training network in MvDGP and incorporate stochastic variational inference for fine-tuning. Experimental results on real-world data sets demonstrate that pre-trained MvDGP outperforms the state-of-the-art DGP models and deep neural networks, achieving higher computational efficiency than other DGP models.","publish_time":1587081600000,"author_summary":" Zhu, Han; Zhao, Jing; Sun, Shiliang","abstract_summary":" Deep Gaussian process (DGP) is one of the<br>popular probabilistic modeling methods, which is<br>powerful and widely used for function approximation and<br>uncertainty estimation. However, the traditional DGP<br>lacks consideration for multi-view cases in which<br>data may come from different sources or be<br>constructed by different types of features. In this paper,<br>we propose a generalized multi-view DGP (MvDGP)<br>to capture the characteristics of different<br>views and model data in different views<br>discriminately. In order to make the proposed model more<br>efficient in training, we introduce a pre-training<br>network in MvDGP and incorporate stochastic<br>variational inference for fine-tuning. Experimental<br>results...","title_summary":" Multi-view Deep Gaussian Process with a<br>Pre-training Acceleration Technique","x":-12.1570711136,"y":40.2652168274,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.1570711136,"tsne_y":40.2652168274,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"jat7pttf","source_x":"PMC","title":"Correlation-Aware Deep Generative Model for Unsupervised Anomaly Detection","doi":"10.1007\/978-3-030-47436-2_52","abstract":"Unsupervised anomaly detection aims to identify anomalous samples from highly complex and unstructured data, which is pervasive in both fundamental research and industrial applications. However, most existing methods neglect the complex correlation among data samples, which is important for capturing normal patterns from which the abnormal ones deviate. In this paper, we propose a method of Correlation aware unsupervised Anomaly detection via Deep Gaussian Mixture Model (CADGMM), which captures the complex correlation among data points for high-quality low-dimensional representation learning. More specifically, the relations among data samples are correlated firstly in forms of a graph structure, in which, the node denotes the sample and the edge denotes the correlation between two samples from the feature space. Then, a dual-encoder that consists of a graph encoder and a feature encoder, is employed to encode both the feature and correlation information of samples into the low-dimensional latent space jointly, followed by a decoder for data reconstruction. Finally, a separate estimation network as a Gaussian Mixture Model is utilized to estimate the density of the learned latent vector, and the anomalies can be detected by measuring the energy of the samples. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method.","publish_time":1587081600000,"author_summary":" Fan, Haoyi; Zhang, Fengbin; Wang, Ruidong; Xi,<br>Liang; Li, Zuoyong","abstract_summary":" Unsupervised anomaly detection aims to<br>identify anomalous samples from highly complex and<br>unstructured data, which is pervasive in both fundamental<br>research and industrial applications. However, most<br>existing methods neglect the complex correlation among<br>data samples, which is important for capturing<br>normal patterns from which the abnormal ones deviate.<br>In this paper, we propose a method of Correlation<br>aware unsupervised Anomaly detection via Deep<br>Gaussian Mixture Model (CADGMM), which captures the<br>complex correlation among data points for<br>high-quality low-dimensional representation learning.<br>More specifically, the relations among data<br>samples are correlated firstly in forms of a graph<br>structure, in which, the node...","title_summary":" Correlation-Aware Deep Generative Model for<br>Unsupervised Anomaly Detection","x":-12.7891321182,"y":39.503364563,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.7891321182,"tsne_y":39.503364563,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"sh1kehrh","source_x":"PMC","title":"Deep Learning-Based Computer Vision Application with Multiple Built-In Data Science-Oriented Capabilities","doi":"10.1007\/978-3-030-48791-1_4","abstract":"This paper presents a Data Science-oriented application for image classification tasks that is able to automatically: a) gather images needed for training Deep Learning (DL) models with a built-in search engine crawler; b) remove duplicate images; c) sort images using built-in pre-trained DL models or user\u2019s own trained DL model; d) apply data augmentation; e) train a DL classification model; f) evaluate the performance of a DL model and system by using an accuracy calculator as well as the Accuracy Per Consumption (APC), Accuracy Per Energy Cost (APEC), Time to closest APC (TTCAPC) and Time to closest APEC (TTCAPEC) metrics calculators. Experimental results show that the proposed Computer Vision application has several unique features and advantages, proving to be efficient regarding execution time and much easier to use when compared to similar applications.","publish_time":1588377600000,"author_summary":" Jurj, Sorin Liviu; Opritoiu, Flavius;<br>Vladutiu, Mircea","abstract_summary":" This paper presents a Data Science-oriented<br>application for image classification tasks that is able to<br>automatically: a) gather images needed for training Deep<br>Learning (DL) models with a built-in search engine<br>crawler; b) remove duplicate images; c) sort images<br>using built-in pre-trained DL models or user\u2019s own<br>trained DL model; d) apply data augmentation; e) train a<br>DL classification model; f) evaluate the<br>performance of a DL model and system by using an accuracy<br>calculator as well as the Accuracy Per Consumption (APC),<br>Accuracy Per Energy Cost (APEC), Time to closest APC<br>(TTCAPC) and Time to closest APEC (TTCAPEC) metrics...","title_summary":" Deep Learning-Based Computer Vision<br>Application with Multiple Built-In Data Science-Oriented<br>Capabilities","x":-9.3811807632,"y":41.1787414551,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3811807632,"tsne_y":41.1787414551,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"q613giht","source_x":"PMC","title":"Fingerprints Recognition System-Based on Mobile Device Identification Using Circular String Pattern Matching Techniques","doi":"10.1007\/978-3-030-49190-1_20","abstract":"As fingerprint recognition systems have become increasingly adopted within a range of technology applications over the last decade, so too has their attention within emerging research. However, although this increased attention has led to an enhancement of the software and algorithms behind this recognition process, the majority of research has still not addressed the issues of incorrect rotation or proximity between the finger and the device. Current systems assume that the direction of the imprinted finger will align with that of the target fingerprint image; this decreases the accuracy of fingerprint recognition across a variety of finger orientations and scenarios. In response to this use-case dilemma, this paper proposes a new technique of pattern matching that can account for this natural range of fingerprint orientations. This is achieved first through a preliminary stage of orientation identification, whereby the fingerprint image can be stored under multiple permutations by using approximate circular string-matching algorithms. This enables the database of images for each approximate permutation of orientation to be stored in advance. It can then be matched against the strong information of the fingerprint at its exact relative rotation of input. The improved accuracy of recognition demonstrated through the results of this study may enable the functionality of fingerprint recognition to adapt to challenging device form-factors and provide the accuracy needed for military and medical applications.","publish_time":1588550400000,"author_summary":" Alshammary, Miznah H.; Iliopoulos, Costas S.;<br>Khan, Mujibur R.","abstract_summary":" As fingerprint recognition systems have<br>become increasingly adopted within a range of<br>technology applications over the last decade, so too has<br>their attention within emerging research. However,<br>although this increased attention has led to an<br>enhancement of the software and algorithms behind this<br>recognition process, the majority of research has still not<br>addressed the issues of incorrect rotation or proximity<br>between the finger and the device. Current systems<br>assume that the direction of the imprinted finger will<br>align with that of the target fingerprint image; this<br>decreases the accuracy of fingerprint recognition<br>across a variety of finger orientations and<br>scenarios....","title_summary":" Fingerprints Recognition System-Based on<br>Mobile Device Identification Using Circular String<br>Pattern Matching Techniques","x":-10.3557329178,"y":42.8316993713,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.3557329178,"tsne_y":42.8316993713,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"xfbjmmsh","source_x":"PMC","title":"Task-Projected Hyperdimensional Computing for Multi-task Learning","doi":"10.1007\/978-3-030-49161-1_21","abstract":"Brain-inspired Hyperdimensional (HD) computing is an emerging technique for cognitive tasks in the field of low-power design. As an energy-efficient and fast learning computational paradigm, HD computing has shown great success in many real-world applications. However, an HD model incrementally trained on multiple tasks suffers from the negative impacts of catastrophic forgetting. The model forgets the knowledge learned from previous tasks and only focuses on the current one. To the best of our knowledge, no study has been conducted to investigate the feasibility of applying multi-task learning to HD computing. In this paper, we propose Task-Projected Hyperdimensional Computing (TP-HDC) to make the HD model simultaneously support multiple tasks by exploiting the redundant dimensionality in the hyperspace. To mitigate the interferences between different tasks, we project each task into a separate subspace for learning. Compared with the baseline method, our approach efficiently utilizes the unused capacity in the hyperspace and shows a 12.8% improvement in averaged accuracy with negligible memory overhead.","publish_time":1588723200000,"author_summary":" Chang, Cheng-Yang; Chuang, Yu-Chuan; Wu,<br>An-Yeu (Andy)","abstract_summary":" Brain-inspired Hyperdimensional (HD)<br>computing is an emerging technique for cognitive tasks in<br>the field of low-power design. As an<br>energy-efficient and fast learning computational paradigm, HD<br>computing has shown great success in many real-world<br>applications. However, an HD model incrementally trained on<br>multiple tasks suffers from the negative impacts of<br>catastrophic forgetting. The model forgets the knowledge<br>learned from previous tasks and only focuses on the<br>current one. To the best of our knowledge, no study has<br>been conducted to investigate the feasibility of<br>applying multi-task learning to HD computing. In this<br>paper, we propose Task-Projected Hyperdimensional<br>Computing (TP-HDC) to...","title_summary":" Task-Projected Hyperdimensional Computing<br>for Multi-task Learning","x":-12.5661058426,"y":39.9430656433,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.5661058426,"tsne_y":39.9430656433,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"w7t1xtgq","source_x":"PMC","title":"Robust 3D Detection in Traffic Scenario with Tracking-Based Coupling System","doi":"10.1007\/978-3-030-49161-1_28","abstract":"Autonomous driving is conducted in complex scenarios, which requires to detect 3D objects in real time scenarios as well as accurately track these 3D objects in order to get such information as location, size, trajectory, velocity. MOT (Multi-Object Tracking) performance is heavily dependent on object detection. Once object detection gives false alarms or missing alarms, the multi-object tracking would be automatically influenced. In this paper, we propose a coupling system which combines 3D object detection and multi-object tracking into one framework. We use the tracked objects as a reference in 3D object detection, in order to locate objects, reduce false or missing alarms in a single frame, and weaken the impact of false and missing alarms on the tracking quality. Our method is evaluated on kitti dataset and is proved effective.","publish_time":1588723200000,"author_summary":" Zhou, Zhuoli; Chen, Shitao; Huang, Rongyao;<br>Zheng, Nanning","abstract_summary":" Autonomous driving is conducted in complex<br>scenarios, which requires to detect 3D objects in real time<br>scenarios as well as accurately track these 3D objects in<br>order to get such information as location, size,<br>trajectory, velocity. MOT (Multi-Object Tracking)<br>performance is heavily dependent on object detection. Once<br>object detection gives false alarms or missing<br>alarms, the multi-object tracking would be<br>automatically influenced. In this paper, we propose a<br>coupling system which combines 3D object detection and<br>multi-object tracking into one framework. We use the tracked<br>objects as a reference in 3D object detection, in order<br>to locate objects, reduce false...","title_summary":" Robust 3D Detection in Traffic Scenario with<br>Tracking-Based Coupling System","x":-9.2472438812,"y":41.5427513123,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2472438812,"tsne_y":41.5427513123,"subcluster":6,"subcluster_description":"Educational Image Classificationrobust 3D","shape":"p"},{"cord_uid":"p0wpq3mm","source_x":"PMC","title":"Joint Multi-object Detection and Segmentation from an Untrimmed Video","doi":"10.1007\/978-3-030-49161-1_27","abstract":"In this paper, we present a novel method for jointly detecting and segmenting multiple objects from an untrimmed video. Unlike most existing video object segmentation methods that can only handle a trimmed video in which all video frames contain the target objects, we address a more practical and difficult problem, i.e., joint multi-object detection and segmentation from an untrimmed video where the target objects do not always appear per frame. In particular, our method consists of two modules, i.e., object decision module and object segmentation module. The object decision module is used to detect the objects and decide which target objects need to be separated out from video. As there are usually two or more target objects and they do not always appear in the whole video, we introduce the data association into object decision module to identify their correspondences among frames. The object segmentation module aims to separate the target objects identified by object decision module. In order to extensively evaluate the proposed method, we introduce a new dataset named UNVOSeg dataset, in which [Formula: see text] of the video frames do not contain objects. Experimental results on four datasets demonstrate that our method outperforms most of the state-of-the-art approaches.","publish_time":1588723200000,"author_summary":" Liu, Xinling; Wang, Le; Zhang, Qilin; Zheng,<br>Nanning; Hua, Gang","abstract_summary":" In this paper, we present a novel method for<br>jointly detecting and segmenting multiple objects<br>from an untrimmed video. Unlike most existing video<br>object segmentation methods that can only handle a<br>trimmed video in which all video frames contain the<br>target objects, we address a more practical and<br>difficult problem, i.e., joint multi-object detection<br>and segmentation from an untrimmed video where the<br>target objects do not always appear per frame. In<br>particular, our method consists of two modules, i.e.,<br>object decision module and object segmentation<br>module. The object decision module is used to detect the<br>objects and decide which target...","title_summary":" Joint Multi-object Detection and<br>Segmentation from an Untrimmed Video","x":-9.2981443405,"y":41.8025932312,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2981443405,"tsne_y":41.8025932312,"subcluster":7,"subcluster_description":"Cnnan Efficient Image Descriptor","shape":"p"},{"cord_uid":"i74hjnor","source_x":"PMC","title":"Knowledge-Based Fusion for Image Tampering Localization","doi":"10.1007\/978-3-030-49161-1_16","abstract":"In this paper we introduce a fusion framework for image tampering localization, that moves towards overcoming the limitation of available tools by allowing a synergistic analysis and multiperspective refinement of the final forensic report. The framework is designed to combine multiple state-of-the-art techniques by exploiting their complementarities so as to produce a single refined tampering localization output map. Extensive evaluation experiments of state-of-the-art methods on diverse datasets have resulted in a modular framework design where candidate methods go through a multi-criterion selection process to become part of the framework. Currently, this includes a set of five passive tampering localization methods for splicing localization on JPEG images. Our experimental findings on two different benchmark datasets showcase that the fused output achieves high performance and advanced interpretability by managing to leverage the correctly localized outputs of individual methods, and even detecting cases that were missed by all individual methods.","publish_time":1588723200000,"author_summary":" Iakovidou, Chryssanthi; Papadopoulos,<br>Symeon; Kompatsiaris, Yiannis","abstract_summary":" In this paper we introduce a fusion framework<br>for image tampering localization, that moves<br>towards overcoming the limitation of available tools<br>by allowing a synergistic analysis and<br>multiperspective refinement of the final forensic report. The<br>framework is designed to combine multiple<br>state-of-the-art techniques by exploiting their<br>complementarities so as to produce a single refined tampering<br>localization output map. Extensive evaluation experiments<br>of state-of-the-art methods on diverse datasets<br>have resulted in a modular framework design where<br>candidate methods go through a multi-criterion<br>selection process to become part of the framework.<br>Currently, this includes a set of five passive tampering<br>localization methods...","title_summary":" Knowledge-Based Fusion for Image Tampering<br>Localization","x":-9.5225458145,"y":42.5166625977,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.5225458145,"tsne_y":42.5166625977,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"a1d0hjtc","source_x":"PMC","title":"Regularized Evolution for Macro Neural Architecture Search","doi":"10.1007\/978-3-030-49186-4_10","abstract":"Neural Architecture Search is becoming an increasingly popular research field and method to design deep learning architectures. Most research focuses on searching for small blocks of deep learning operations, or micro-search. This method yields satisfactory results but demands prior knowledge of the macro architecture\u2019s structure. Generally, methods that do not utilize macro structure knowledge perform worse but are able to be applied to datasets of completely new domains. In this paper, we propose a macro NAS methodology which utilizes concepts of Regularized Evolution and Macro Neural Architecture Search (DeepNEAT), and apply it to the Fashion-MNIST dataset. By utilizing our method, we are able to produce networks that outperform other macro NAS methods on the dataset, when the same post-search inference methods are used. Furthermore, we are able to achieve 94.46% test accuracy, while requiring considerably less epochs to fully train our network.","publish_time":1588723200000,"author_summary":" Kyriakides, George; Margaritis,<br>Konstantinos","abstract_summary":" Neural Architecture Search is becoming an<br>increasingly popular research field and method to design<br>deep learning architectures. Most research<br>focuses on searching for small blocks of deep learning<br>operations, or micro-search. This method yields<br>satisfactory results but demands prior knowledge of the<br>macro architecture\u2019s structure. Generally,<br>methods that do not utilize macro structure knowledge<br>perform worse but are able to be applied to datasets of<br>completely new domains. In this paper, we propose a macro<br>NAS methodology which utilizes concepts of<br>Regularized Evolution and Macro Neural Architecture<br>Search (DeepNEAT), and apply it to the Fashion-MNIST<br>dataset. By utilizing our method,...","title_summary":" Regularized Evolution for Macro Neural<br>Architecture Search","x":-12.7810649872,"y":40.2429771423,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.7810649872,"tsne_y":40.2429771423,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"slkuwbmw","source_x":"PMC","title":"Dempster-Shafer Parzen-Rosenblatt Hidden Markov Fields for Multichannel Image Segmentation","doi":"10.1007\/978-3-030-50146-4_45","abstract":"Theory of evidence has been successfully used in many areas covering pattern recognition and image processing due to its effectiveness in both information fusion and reasoning under uncertainty. Such notoriety led to extension of many existing Bayesian tools such as hidden Markov models, extensively used for image segmentation. This paper falls under this category of frameworks and aims to propose a new hidden Markov field that better handles nonGaussian forms of noise, designed for multichannel image segmentation. To this end, we use a recent kernel smoothing- based noise density estimation combined with a genuine approach of mass determination from data. The proposed model is validated on sampled and real remote sensing images and the results obtained outperform those produced by conventional hidden Markov fields.","publish_time":1589760000000,"author_summary":" Boudaren, Mohamed El Yazid; Hamache, Ali;<br>Debicha, Islam; Sadouk, Hamza Tarik","abstract_summary":" Theory of evidence has been successfully used<br>in many areas covering pattern recognition and<br>image processing due to its effectiveness in both<br>information fusion and reasoning under uncertainty. Such<br>notoriety led to extension of many existing Bayesian<br>tools such as hidden Markov models, extensively used<br>for image segmentation. This paper falls under<br>this category of frameworks and aims to propose a new<br>hidden Markov field that better handles nonGaussian<br>forms of noise, designed for multichannel image<br>segmentation. To this end, we use a recent kernel smoothing-<br>based noise density estimation combined with a<br>genuine approach of mass determination from data....","title_summary":" Dempster-Shafer Parzen-Rosenblatt Hidden<br>Markov Fields for Multichannel Image Segmentation","x":-10.0124740601,"y":42.7571029663,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.0124740601,"tsne_y":42.7571029663,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"8d9ig9y4","source_x":"PMC","title":"Thin Structures Segmentation Using Anisotropic Neighborhoods","doi":"10.1007\/978-3-030-50146-4_44","abstract":"Bayesian and probabilistic models are widely used in image processing to handle noise due to various alteration phenomena. To benefit from the spatial information in a tractable way, Markov Random Fields (MRF) are often assumed with isotropic neighborhoods, that is however at the detriment of the preservation of thin structures. In this study, we aim at relaxing this assumption on stationarity and isotropy of the neighborhood shape in order to get a prior probability term that is relevant not only within the homogeneous areas but also close to object borders and within thin structures. To tackle the issue of neighborhood shape estimation, we propose to use tensor voting, that allows for the estimation of structure direction and saliency at various scales. We propose three main ways to derive anisotropic neighborhoods, namely shape-based, target-based and cardinal-based neighborhood. Then, having defined the neighborhood field, we introduce an energy that will be minimized using graph cuts, and illustrate the benefits of our approach against the use of isotropic neighborhoods in the applicative context of crack detection. First results on such a challenging problem are very encouraging.","publish_time":1589760000000,"author_summary":" Ribal, Christophe; Lerm\u00e9, Nicolas; Le<br>H\u00e9garat-Mascle, Sylvie","abstract_summary":" Bayesian and probabilistic models are widely<br>used in image processing to handle noise due to<br>various alteration phenomena. To benefit from the<br>spatial information in a tractable way, Markov Random<br>Fields (MRF) are often assumed with isotropic<br>neighborhoods, that is however at the detriment of the<br>preservation of thin structures. In this study, we aim at<br>relaxing this assumption on stationarity and isotropy<br>of the neighborhood shape in order to get a prior<br>probability term that is relevant not only within the<br>homogeneous areas but also close to object borders and<br>within thin structures. To tackle the issue of<br>neighborhood...","title_summary":" Thin Structures Segmentation Using<br>Anisotropic Neighborhoods","x":-9.451128006,"y":43.0475311279,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.451128006,"tsne_y":43.0475311279,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"hde9waz3","source_x":"PMC","title":"Bayesian Image Analysis in Fourier Space Using Data-Driven Priors (DD-BIFS)","doi":"10.1007\/978-3-030-50153-2_29","abstract":"Statistical image analysis is an extensive field that includes problems such as noise-reduction, de-blurring, feature enhancement, and object detection\/identification, to name a few. Bayesian image analysis can improve image quality, by balancing a priori expectations of image characteristics, with a model for the noise process via Bayes Theorem. We have previously given a reformulation of the conventional Bayesian image analysis paradigm in Fourier space, i.e. the prior distribution (the prior) and likelihood are given in terms of spatial frequency signals. By specifying the Bayesian model in Fourier space, spatially correlated priors, that are relatively difficult to model and compute in conventional image space, can be efficiently modeled as a set of independent processes across Fourier space. The originally inter-correlated and high-dimensional problem in image space is thereby broken down into a series of (trivially parallelizable) independent one-dimensional problems. In this paper we adapt this Fourier space process into a data-driven framework in which the Fourier space priors are built empirically from a database of images and then used to enhance future images. We will describe the data-driven Bayesian image analysis in Fourier space (DD-BIFS) modeling approach, illustrate it\u2019s computational efficiency and speed. Finally, we give specific applications of DD-BIFS to improve the quality of arterial-spin-labeling (ASL) perfusion images via a database of human brain positron emission tomography (PET) images.","publish_time":1589587200000,"author_summary":" Kornak, John; Boylan, Ross; Young, Karl; Wolf,<br>Amy; Cobigo, Yann; Rosen, Howard","abstract_summary":" Statistical image analysis is an extensive<br>field that includes problems such as<br>noise-reduction, de-blurring, feature enhancement, and object<br>detection\/identification, to name a few. Bayesian image analysis can<br>improve image quality, by balancing a priori<br>expectations of image characteristics, with a model for the<br>noise process via Bayes Theorem. We have previously<br>given a reformulation of the conventional Bayesian<br>image analysis paradigm in Fourier space, i.e. the<br>prior distribution (the prior) and likelihood are<br>given in terms of spatial frequency signals. By<br>specifying the Bayesian model in Fourier space, spatially<br>correlated priors, that are relatively difficult to model<br>and compute in...","title_summary":" Bayesian Image Analysis in Fourier Space Using<br>Data-Driven Priors (DD-BIFS)","x":-9.8182096481,"y":43.1431312561,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.8182096481,"tsne_y":43.1431312561,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"t4abfe61","source_x":"PMC","title":"Orthogonal Local Image Descriptors with Convolutional Autoencoders","doi":"10.1007\/978-3-030-49076-8_15","abstract":"This work proposes the use of deep learning architectures, and in particular Convolutional Autencoders (CAE\u2019s), to incorporate an explicit component of orthogonality to the computation of local image descriptors. For this purpose we present a methodology based on the computation of dot products among the hidden outputs of the center-most layer of a convolutional autoencoder. This is, the dot product between the responses of the different kernels of the central layer (sections of a latent representation). We compare this dot product against an indicator of orthogonality, which in the presence of non-orthogonal hidden representations, back-propagates a gradient through the network, adjusting its parameters to produce new representations which will be closer to have orthogonality among them in future iterations. Our results show that the proposed methodology is suitable for the estimation of local image descriptors that are orthogonal to one another, which is often a desirable feature in many patter recognition tasks.","publish_time":1588118400000,"author_summary":" Roman-Rangel, Edgar; Marchand-Maillet,<br>Stephane","abstract_summary":" This work proposes the use of deep learning<br>architectures, and in particular Convolutional Autencoders<br>(CAE\u2019s), to incorporate an explicit component of<br>orthogonality to the computation of local image descriptors.<br>For this purpose we present a methodology based on<br>the computation of dot products among the hidden<br>outputs of the center-most layer of a convolutional<br>autoencoder. This is, the dot product between the responses<br>of the different kernels of the central layer<br>(sections of a latent representation). We compare this<br>dot product against an indicator of<br>orthogonality, which in the presence of non-orthogonal hidden<br>representations, back-propagates a gradient through the<br>network,...","title_summary":" Orthogonal Local Image Descriptors with<br>Convolutional Autoencoders","x":-10.492972374,"y":41.231136322,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.492972374,"tsne_y":41.231136322,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"as1q6ps7","source_x":"PMC","title":"New Method for Extreme Color Detection in Images","doi":"10.1007\/978-3-030-49076-8_9","abstract":"In image processing and computer vision, it is common to find applications, in which it is necessary to detect reference points characterized by extreme color, i.e., a primary color RGB or complementary CMY with very high saturation. Thus, there are cases in which a certain class of objects can be distinguished according to their characteristic extreme color, which can be used as landmarks or to identify objects. Therefore, there is an interest in identifying landmarks characterized by extreme colors. In this paper, a new method for detecting objects with an extreme color is introduced and compared with other approaches found in the literature. The methods are analyzed and compared using a color palette in which a transition between R, G, B, C, M and Y colors is generated. The results obtained show that the methods studied allow the specific colors to be adequately discriminated, while the proposed method is the only one that allows the full range of extreme colors R, G, B, C, M and Y to be detected, being more selective than the others, by taking practically the areas corresponding to each color separately .","publish_time":1588118400000,"author_summary":" Forero, Manuel G.; \u00c1vila-Navarro, Juli\u00e1n;<br>Herrera-Rivera, Sergio","abstract_summary":" In image processing and computer vision, it is<br>common to find applications, in which it is necessary<br>to detect reference points characterized by<br>extreme color, i.e., a primary color RGB or<br>complementary CMY with very high saturation. Thus, there are<br>cases in which a certain class of objects can be<br>distinguished according to their characteristic extreme<br>color, which can be used as landmarks or to identify<br>objects. Therefore, there is an interest in identifying<br>landmarks characterized by extreme colors. In this<br>paper, a new method for detecting objects with an<br>extreme color is introduced and compared with other<br>approaches found...","title_summary":" New Method for Extreme Color Detection in<br>Images","x":-9.3672103882,"y":42.6528320312,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3672103882,"tsne_y":42.6528320312,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"w3honzzc","source_x":"PMC","title":"A Simple Methodology for 2D Reconstruction Using a CNN Model","doi":"10.1007\/978-3-030-49076-8_10","abstract":"In recent years, Deep Learning research have demonstrated their effectiveness in digital image processing, mainly in areas with heavy computational load. Such is the case of aerial photogrammetry, where the principal objective is to generate a 2D map or a 3D model from a specific terrain. In these topics, high-efficiency in visual information processing is demanded. In this work we present a simple methodology to build an orthomosaic, our proposal is focused in replacing traditional digital imagen processing using instead a Convolutional Neuronal Network (CNN) model. The dataset of aerial images is generated from drone photographs of our university campus. The method described in this article uses a CNN model to detect matching points and RANSAC algorithm to correct feature\u2019s correlation. Experimental results show that feature maps and matching points obtained between pair of images through a CNN are comparable with those obtained in traditional artificial vision algorithms.","publish_time":1588118400000,"author_summary":" Rodr\u00edguez-Santiago, Armando Levid;<br>Arias-Aguilar, Jos\u00e9 Anibal; Petrilli-Barcel\u00f3, Alberto<br>El\u00edas; Miranda-Luna, Rosebet","abstract_summary":" In recent years, Deep Learning research have<br>demonstrated their effectiveness in digital image<br>processing, mainly in areas with heavy computational load.<br>Such is the case of aerial photogrammetry, where the<br>principal objective is to generate a 2D map or a 3D model<br>from a specific terrain. In these topics,<br>high-efficiency in visual information processing is demanded.<br>In this work we present a simple methodology to<br>build an orthomosaic, our proposal is focused in<br>replacing traditional digital imagen processing using<br>instead a Convolutional Neuronal Network (CNN) model.<br>The dataset of aerial images is generated from<br>drone photographs of our university campus....","title_summary":" A Simple Methodology for 2D Reconstruction<br>Using a CNN Model","x":-9.0352840424,"y":42.0789260864,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.0352840424,"tsne_y":42.0789260864,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"1dvlvy6c","source_x":"PMC","title":"ModuleNet: A Convolutional Neural Network for Stereo Vision","doi":"10.1007\/978-3-030-49076-8_21","abstract":"Convolutional Neural Networks (CNN) has gained much attention for the solution of numerous vision problems including disparities calculation in stereo vision systems. In this paper, we present a CNN based solution for disparities estimation that builds upon a basic module (BM) with limited range of disparities that can be extended using various BM in parallel. Our BM can be understood as a segmentation by disparity and produces an output channel with the memberships for each disparity candidate, additionally the BM computes a channel with the out\u2013of\u2013range disparity regions. This extra channel allows us to parallelize several BM and dealing with their respective responsibilities. We train our model with the MPI Sintel dataset. The results show that ModuleNet, our modular CNN model, outperforms the baseline algorithm Efficient Large-scale Stereo Matching (ELAS) and FlowNetC achieving about a 80% of improvement.","publish_time":1588118400000,"author_summary":" Renteria-Vidales, O. I.; Cuevas-Tello, J. C.;<br>Reyes-Figueroa, A.; Rivera, M.","abstract_summary":" Convolutional Neural Networks (CNN) has<br>gained much attention for the solution of numerous<br>vision problems including disparities calculation<br>in stereo vision systems. In this paper, we<br>present a CNN based solution for disparities<br>estimation that builds upon a basic module (BM) with<br>limited range of disparities that can be extended using<br>various BM in parallel. Our BM can be understood as a<br>segmentation by disparity and produces an output channel<br>with the memberships for each disparity candidate,<br>additionally the BM computes a channel with the out\u2013of\u2013range<br>disparity regions. This extra channel allows us to<br>parallelize several BM and dealing with...","title_summary":" ModuleNet: A Convolutional Neural Network for<br>Stereo Vision","x":-9.8431138992,"y":41.3624382019,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.8431138992,"tsne_y":41.3624382019,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"xycwdtig","source_x":"PMC","title":"Towards Inpainting and Denoising Latent Fingerprints: A Study on the Impact in Latent Fingerprint Identification","doi":"10.1007\/978-3-030-49076-8_8","abstract":"In this paper, we provide a study about the impact of the most prominent inpainting and denoising solutions on the latent fingerprint identification. From an in-depth analysis, we show how some of the analyzed inpainting and denoising solutions can improve up 63% for Rank-1 and 26% for Rank-20 the fingerprint identification rates when state-of-the-art minutiae extractors are used. Nevertheless, it is necessary to create new denoising and inpainting solutions that are specifically built to deal with latent fingerprints and their associated issues.","publish_time":1588118400000,"author_summary":" Ram\u00edrez-S\u00e1yago, Ernesto; Loyola-Gonz\u00e1lez,<br>Octavio; Medina-P\u00e9rez, Miguel Angel","abstract_summary":" In this paper, we provide a study about the<br>impact of the most prominent inpainting and denoising<br>solutions on the latent fingerprint identification.<br>From an in-depth analysis, we show how some of the<br>analyzed inpainting and denoising solutions can<br>improve up 63% for Rank-1 and 26% for Rank-20 the<br>fingerprint identification rates when state-of-the-art<br>minutiae extractors are used. Nevertheless, it is<br>necessary to create new denoising and inpainting<br>solutions that are specifically built to deal with latent<br>fingerprints and their associated issues.","title_summary":" Towards Inpainting and Denoising Latent<br>Fingerprints: A Study on the Impact in Latent Fingerprint<br>Identification","x":-9.819519043,"y":42.3799133301,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.819519043,"tsne_y":42.3799133301,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"fx3rynwt","source_x":"PMC","title":"Experimental Study on Transfer Learning in Denoising Autoencoders for Speech Enhancement","doi":"10.1007\/978-3-030-49076-8_29","abstract":"The quality of speech signals is affected by a combination of background noise, reverberation, and other distortions in real-life environments. The processing of such signals presents important challenges for tasks such as voice or speaker recognition. To enhance signals in such challenging conditions several deep learning-based methods have been proposed. Those new methods have proven to be effective, in comparison to classical algorithms based on statistical analysis and signal processing. In particular, recurrent neural networks, especially those with long short-term memory (LSTM and BLSTM), have presented surprising results in tasks related to enhancing speech. One of the most challenging aspects of artificial neural networks is to reduce the high computational cost of the training procedure. In this work, we present a comparative study on transfer learning to accelerate and improve traditional training based on random initialization of the internal weights of the networks. The results show the advantage of the proposal in terms of less training time and better results for the task of denoising speech signals at several signal-to-noise ratio levels of white noise.","publish_time":1588118400000,"author_summary":" Coto-Jim\u00e9nez, Marvin","abstract_summary":" The quality of speech signals is affected by a<br>combination of background noise, reverberation, and other<br>distortions in real-life environments. The processing of<br>such signals presents important challenges for<br>tasks such as voice or speaker recognition. To<br>enhance signals in such challenging conditions<br>several deep learning-based methods have been<br>proposed. Those new methods have proven to be effective,<br>in comparison to classical algorithms based on<br>statistical analysis and signal processing. In<br>particular, recurrent neural networks, especially those<br>with long short-term memory (LSTM and BLSTM), have<br>presented surprising results in tasks related to<br>enhancing speech. One of the most challenging aspects...","title_summary":" Experimental Study on Transfer Learning in<br>Denoising Autoencoders for Speech Enhancement","x":-11.4028310776,"y":40.5708122253,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.4028310776,"tsne_y":40.5708122253,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"kn2lxz5p","source_x":"PMC","title":"Improvement of the Turajli\u0107 Method for the Estimation of Gaussian Noise in Images","doi":"10.1007\/978-3-030-49076-8_11","abstract":"Gaussian noise estimation is an important step in some of the more recently developed noise removal methods. This is a difficult task and although several estimation techniques have been proposed recently, they generally do not produce good results. In a previous comparative study, among several noise estimation techniques, a method proposed in 2017 by Turajli\u0107 was found to give the best results. Although acceptable, they are still far from ideal. Therefore, several changes to this method are introduced in this paper to improve the estimation. Tests on monochromatic images contaminated with different levels of Gaussian noise showed that the modified method produces a significant improvement in the estimation of Gaussian noise, over 35%, at a slightly higher computational cost.","publish_time":1588118400000,"author_summary":" Forero, Manuel G.; Miranda, Sergio L.;<br>Jacanamejoy-Jamioy, Carlos","abstract_summary":" Gaussian noise estimation is an important step<br>in some of the more recently developed noise<br>removal methods. This is a difficult task and although<br>several estimation techniques have been proposed<br>recently, they generally do not produce good results. In a<br>previous comparative study, among several noise<br>estimation techniques, a method proposed in 2017 by<br>Turajli\u0107 was found to give the best results. Although<br>acceptable, they are still far from ideal. Therefore,<br>several changes to this method are introduced in this<br>paper to improve the estimation. Tests on<br>monochromatic images contaminated with different levels of<br>Gaussian noise showed that the modified...","title_summary":" Improvement of the Turajli\u0107 Method for the<br>Estimation of Gaussian Noise in Images","x":-9.8299407959,"y":42.7310028076,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.8299407959,"tsne_y":42.7310028076,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"2fholnwk","source_x":"PMC","title":"Fruit Classification for Retail Stores Using Deep Learning","doi":"10.1007\/978-3-030-49076-8_1","abstract":"Payment of fruits or vegetables in retail stores normally require them to be manually identified. This paper presents an image classification method, based on lightweight Convolutional Neural Networks (CNN), with the goal of speeding up the checkout process in stores. A new dataset of images is introduced that considers three classes of fruits, inside or without plastic bags. In order to increase the classification accuracy, different input features are added into the CNN architecture. Such inputs are, a single RGB color, the RGB histogram, and the RGB centroid obtained from K-means clustering. The results show an overall 95% classification accuracy for fruits with no plastic bag, and 93% for fruits in a plastic bag .","publish_time":1588118400000,"author_summary":" Rojas-Aranda, Jose Luis; Nunez-Varela, Jose<br>Ignacio; Cuevas-Tello, J. C.; Rangel-Ramirez,<br>Gabriela","abstract_summary":" Payment of fruits or vegetables in retail<br>stores normally require them to be manually<br>identified. This paper presents an image classification<br>method, based on lightweight Convolutional Neural<br>Networks (CNN), with the goal of speeding up the checkout<br>process in stores. A new dataset of images is introduced<br>that considers three classes of fruits, inside or<br>without plastic bags. In order to increase the<br>classification accuracy, different input features are added<br>into the CNN architecture. Such inputs are, a single<br>RGB color, the RGB histogram, and the RGB centroid<br>obtained from K-means clustering. The results show an<br>overall 95% classification accuracy...","title_summary":" Fruit Classification for Retail Stores Using<br>Deep Learning","x":-8.4190149307,"y":41.1319580078,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.4190149307,"tsne_y":41.1319580078,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"nxj0lfo5","source_x":"PMC","title":"What the Appearance Channel from Two-Stream Architectures for Activity Recognition Is Learning?","doi":"10.1007\/978-3-030-49076-8_24","abstract":"The automatic recognition of human activities from video data is being led by spatio-temporal Convolutional Neural Networks (3D CNNs), in particular two-stream architectures such as I3D that reports the best accuracy so far. Despite the high performance in accuracy of this kind of architectures, very little is known about what they are really learning from data, resulting therefore in a lack of robustness and explainability. In this work we select the appearance channel from the I3D architecture and create a set of experiments aimed at explaining what this model is learning. Throughout the proposed experiments we provide evidence that this particular model is learning the texture of the largest area (which can be the activity or the background, depending on the distance from the camera to the action performed). In addition, we state several considerations to take into account when selecting the training data to achieve a better generalization of the model for human activity recognition.","publish_time":1588118400000,"author_summary":" Oves Garc\u00eda, Reinier; Sucar, L. Enrique","abstract_summary":" The automatic recognition of human activities<br>from video data is being led by spatio-temporal<br>Convolutional Neural Networks (3D CNNs), in particular<br>two-stream architectures such as I3D that reports the best<br>accuracy so far. Despite the high performance in<br>accuracy of this kind of architectures, very little is<br>known about what they are really learning from data,<br>resulting therefore in a lack of robustness and<br>explainability. In this work we select the appearance channel<br>from the I3D architecture and create a set of<br>experiments aimed at explaining what this model is<br>learning. Throughout the proposed experiments we<br>provide evidence that this...","title_summary":" What the Appearance Channel from Two-Stream<br>Architectures for Activity Recognition Is Learning?","x":-9.593296051,"y":40.8942642212,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.593296051,"tsne_y":40.8942642212,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"bxjw84ee","source_x":"PMC","title":"COUPLED: Calibration of a LiDAR and Camera Rig Using Automatic Plane Detection","doi":"10.1007\/978-3-030-49076-8_20","abstract":"LiDARs and cameras are two widely used sensors in robotics and computer vision, particularly for navigation and recognition in 3D scenarios. Systems combining both may benefit from the precise depth of the former and the high-density information of the latter, but a calibration process is necessary to relate them spatially. In this paper, we introduce COUPLED, a method that finds the extrinsic parameters to relate information between them. The method implies the use of a setup consisting of three planes with charuco patterns to find the planes in both systems. We obtain corresponding points in both systems through geometric relations between the planes. Afterward, we use these points and the Kabsch algorithm to compute the transformation that merges the planes between both systems. Compared to recent single plane algorithms, we obtain more accurate parameters, and only one pose is required. In the process, we develop a method to automatically find the calibration target using a plane detector instead of manually selecting the target in the LiDAR frame.","publish_time":1588118400000,"author_summary":" Montoya, Omar; Icasio, Octavio; Salas,<br>Joaqu\u00edn","abstract_summary":" LiDARs and cameras are two widely used sensors<br>in robotics and computer vision, particularly<br>for navigation and recognition in 3D scenarios.<br>Systems combining both may benefit from the precise<br>depth of the former and the high-density information<br>of the latter, but a calibration process is<br>necessary to relate them spatially. In this paper, we<br>introduce COUPLED, a method that finds the extrinsic<br>parameters to relate information between them. The method<br>implies the use of a setup consisting of three planes<br>with charuco patterns to find the planes in both<br>systems. We obtain corresponding points in both systems<br>through geometric relations...","title_summary":" COUPLED: Calibration of a LiDAR and Camera Rig<br>Using Automatic Plane Detection","x":-9.2782669067,"y":42.6221885681,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2782669067,"tsne_y":42.6221885681,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"xxwad2i1","source_x":"PMC","title":"Spatial [Formula: see text]-Trimmed Fuzzy C-Means Algorithm to Image Segmentation","doi":"10.1007\/978-3-030-49076-8_12","abstract":"An important aspect should be taken into account, when an image is segmented, the presence of atypical information. In this investigation an algorithm is proposed that is noise tolerant in the segmentation process. A method to image segmentation that combines Fuzzy C-Means (FCM) algorithm and Trimmed Means filter, called Spatial [Formula: see text] Trimmed Fuzzy C-means, using local information to achieve better segmentation. The FCM is very sensitive to noise, and the Trimmed Means filter is used to eliminate outliers with a lower computational cost. Compared to some state-of-the-art algorithms, the proposed is faster and noise tolerant, demonstrating better performance in the metrics considered.","publish_time":1588118400000,"author_summary":" Vela-Rinc\u00f3n, Virna V.; M\u00fajica-Vargas, Dante;<br>Mej\u00eda Lavalle, Manuel; Magad\u00e1n Salazar, Andrea","abstract_summary":" An important aspect should be taken into<br>account, when an image is segmented, the presence of<br>atypical information. In this investigation an<br>algorithm is proposed that is noise tolerant in the<br>segmentation process. A method to image segmentation that<br>combines Fuzzy C-Means (FCM) algorithm and Trimmed<br>Means filter, called Spatial [Formula: see text]<br>Trimmed Fuzzy C-means, using local information to<br>achieve better segmentation. The FCM is very sensitive<br>to noise, and the Trimmed Means filter is used to<br>eliminate outliers with a lower computational cost.<br>Compared to some state-of-the-art algorithms, the<br>proposed is faster and noise tolerant, demonstrating<br>better performance...","title_summary":" Spatial [Formula: see text]-Trimmed Fuzzy<br>C-Means Algorithm to Image Segmentation","x":-9.6800327301,"y":42.7175102234,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.6800327301,"tsne_y":42.7175102234,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"wicdmxet","source_x":"PMC","title":"Onboard CNN-Based Processing for Target Detection and Autonomous Landing for MAVs","doi":"10.1007\/978-3-030-49076-8_19","abstract":"In this work, we address the problem of target detection involved in an autonomous landing task for a Micro Aerial Vehicle (MAV). The challenge is to detect a flag located somewhere in the environment. The flag is posed on a pole, and to its right, a landing platform is located. Thus, the MAV has to detect the flag, fly towards it and once it is close enough, locate the landing platform nearby, aiming at centring over it to perform landing; all of this has to be carried out autonomously. In this context, the main problem is the detection of both the flag and the landing platform, whose shapes are known in advanced. Traditional computer vision algorithms could be used; however, the main challenges in this task are the changes in illumination, rotation and scale, and the fact that the flight controller uses the detection to perform the autonomous flight; hence the detection has to be stable and continuous on every camera frame. Motivated by this, we propose to use a Convolutional Neural Network optimised to be run on a small computer with limited computer processing budget. The MAV carries this computer, and it is used to process everything on board. To validate our system, we tested with rotated images, changes in scale and the presence of low illumination. Our method is compared against two conventional computer vision methods, namely, template and feature matching. In addition, we tested our system performance in a wide corridor, executing everything on board the MAV. We achieved a successful detection of the flag with a confidence metric of 0.9386 and 0.9826 for the Landing platform. In total, all the onboard computations ran at an average of 13.01 fps.","publish_time":1588118400000,"author_summary":" Cabrera-Ponce, A. A.; Martinez-Carranza, J.","abstract_summary":" In this work, we address the problem of target<br>detection involved in an autonomous landing task for a<br>Micro Aerial Vehicle (MAV). The challenge is to<br>detect a flag located somewhere in the environment.<br>The flag is posed on a pole, and to its right, a<br>landing platform is located. Thus, the MAV has to detect<br>the flag, fly towards it and once it is close enough,<br>locate the landing platform nearby, aiming at<br>centring over it to perform landing; all of this has to be<br>carried out autonomously. In this context, the main<br>problem is the detection of both...","title_summary":" Onboard CNN-Based Processing for Target<br>Detection and Autonomous Landing for MAVs","x":-9.1308908463,"y":41.3912887573,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.1308908463,"tsne_y":41.3912887573,"subcluster":6,"subcluster_description":"Educational Image Classificationrobust 3D","shape":"p"},{"cord_uid":"6zgepp9b","source_x":"PMC","title":"Vision-Based Blind Spot Warning System by Deep Neural Networks","doi":"10.1007\/978-3-030-49076-8_18","abstract":"Traffic accidents represent one of the most serious problems around the world. Many efforts have been concentrated on implementing Advanced Driver Assistance Systems (ADAS) to increase safety by reducing critical tasks faced by the driver. In this paper, a Blind Spot Warning (BSW) system capable of virtualizing cars around the driver\u2019s vehicle is presented. The system is based on deep neural models for car detection and depth estimation using images captured with a camera located on top of the main vehicle, then transformations are applied to the image and to generate the appropriate information format. Finally the cars in the environment are represented in a 3D graphical interface. We present a comparison between car detectors and another one between depth estimators from which we choose the best performance ones to be implemented in the BSW system. In particular, our system offers a more intuitive assistance interface for the driver allowing a better and quicker understanding of the environment from monocular cameras.","publish_time":1588118400000,"author_summary":" Virgilio G., V\u00edctor R.; Sossa, Humberto;<br>Zamora, Erik","abstract_summary":" Traffic accidents represent one of the most<br>serious problems around the world. Many efforts have<br>been concentrated on implementing Advanced Driver<br>Assistance Systems (ADAS) to increase safety by reducing<br>critical tasks faced by the driver. In this paper, a Blind<br>Spot Warning (BSW) system capable of virtualizing<br>cars around the driver\u2019s vehicle is presented. The<br>system is based on deep neural models for car detection<br>and depth estimation using images captured with a<br>camera located on top of the main vehicle, then<br>transformations are applied to the image and to generate the<br>appropriate information format. Finally the cars in the...","title_summary":" Vision-Based Blind Spot Warning System by Deep<br>Neural Networks","x":-9.1125154495,"y":41.1204719543,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.1125154495,"tsne_y":41.1204719543,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"payo1zgt","source_x":"PMC","title":"Structured Pointcloud Segmentation for Individual Mangrove Tree Modeling","doi":"10.1007\/978-3-030-49076-8_17","abstract":"Tree structure parameters of mangrove forests are hard to measure in the field and therefore inventories of this type of forests are impossible to keep up to date. In this article, we tested a structured pointcloud segmentation method for extracting individual mangrove trees. Structure parameters of individual trees were estimated from the segmented pointcloud and its 3d geometry was generated using revolution surfaces. Estimated parameters were then assessed at both plot and tree levels using field data. It was observed that the number of segments in each test plot agreed well with the number of trees observed in the field. Nonetheless, the estimated parameters exhibited mixed accuracy with top height being the most accurate.","publish_time":1588118400000,"author_summary":" Silv\u00e1n-C\u00e1rdenas, Jos\u00e9 L.; Gallardo-Cruz,<br>Jos\u00e9 A.; Hern\u00e1ndez-Huerta, Laura M.","abstract_summary":" Tree structure parameters of mangrove forests<br>are hard to measure in the field and therefore<br>inventories of this type of forests are impossible to keep up<br>to date. In this article, we tested a structured<br>pointcloud segmentation method for extracting<br>individual mangrove trees. Structure parameters of<br>individual trees were estimated from the segmented<br>pointcloud and its 3d geometry was generated using<br>revolution surfaces. Estimated parameters were then<br>assessed at both plot and tree levels using field data. It<br>was observed that the number of segments in each<br>test plot agreed well with the number of trees<br>observed in the field....","title_summary":" Structured Pointcloud Segmentation for<br>Individual Mangrove Tree Modeling","x":-9.7177248001,"y":44.0971755981,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.7177248001,"tsne_y":44.0971755981,"subcluster":0,"subcluster_description":"Individual Mangrove Tree Modelingcombined","shape":"p"},{"cord_uid":"007qk8nv","source_x":"PMC","title":"A Novel Set of Moment Invariants for Pattern Recognition Applications Based on Jacobi Polynomials","doi":"10.1007\/978-3-030-49076-8_14","abstract":"A novel set of moment invariants for pattern recognition applications, which are based on Jacobi polynomials, are presented. These moment invariants are constructed for digital images by means of a combination with geometric moments, and are invariant in the face of affine geometric transformations such as rotation, translation and scaling, on the image plane. This invariance is tested on a sample of the MPEG-7 CE-Shape-1 dataset. The results presented show that the low-order moment invariants indeed possess low variance between images that are affected by the mentioned geometric transformations.","publish_time":1588118400000,"author_summary":" Rocha Angulo, Rafael Augusto; Carpio, Juan<br>Mart\u00edn; Rojas-Dom\u00ednguez, Alfonso;<br>Ornelas-Rodr\u00edguez, Manuel; Puga, H\u00e9ctor","abstract_summary":" A novel set of moment invariants for pattern<br>recognition applications, which are based on Jacobi<br>polynomials, are presented. These moment invariants are<br>constructed for digital images by means of a combination<br>with geometric moments, and are invariant in the<br>face of affine geometric transformations such as<br>rotation, translation and scaling, on the image plane.<br>This invariance is tested on a sample of the MPEG-7<br>CE-Shape-1 dataset. The results presented show that the<br>low-order moment invariants indeed possess low variance<br>between images that are affected by the mentioned<br>geometric transformations.","title_summary":" A Novel Set of Moment Invariants for Pattern<br>Recognition Applications Based on Jacobi Polynomials","x":-9.9776115417,"y":42.2999076843,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.9776115417,"tsne_y":42.2999076843,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"9nx1k9dy","source_x":"PMC","title":"Restoration of Range Images by the Gaussian Pyramid Method, Testing Different Interpolation Techniques to Select the Best Performance","doi":"10.1007\/978-3-030-49076-8_13","abstract":"The inpainting method implemented in this work was used to estimate the missing information in a range image from a single image, achieved independence of the RGB image of the scene or multiple range images to perform the restoration. The proposal is based on improving the results of restoring range images using the Gaussian Pyramid method. This, finding the best interpolation technique to use in this method to estimate the missing information. Different interpolation techniques were computed and applied in order to know the best option to implement. This is carried out considering the amount of information that can be estimated, processing time and the total of information missing in the image to be restored. The method was tested with five different databases, one of which was created specifically for this work. These databases include different interior scenarios with several objects. A qualitative and quantitative comparative analysis of the obtained results was performed.","publish_time":1588118400000,"author_summary":" Chavira Calder\u00f3n, Enrique; Cruz-Bernal,<br>Alejandra","abstract_summary":" The inpainting method implemented in this work<br>was used to estimate the missing information in a<br>range image from a single image, achieved<br>independence of the RGB image of the scene or multiple range<br>images to perform the restoration. The proposal is<br>based on improving the results of restoring range<br>images using the Gaussian Pyramid method. This,<br>finding the best interpolation technique to use in this<br>method to estimate the missing information.<br>Different interpolation techniques were computed and<br>applied in order to know the best option to implement.<br>This is carried out considering the amount of<br>information that can be...","title_summary":" Restoration of Range Images by the Gaussian<br>Pyramid Method, Testing Different Interpolation<br>Techniques to Select the Best Performance","x":-9.3432416916,"y":42.8211517334,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3432416916,"tsne_y":42.8211517334,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"zazr8uj5","source_x":"PMC","title":"Cast Shadow Generation Using Generative Adversarial Networks","doi":"10.1007\/978-3-030-50426-7_36","abstract":"We propose a computer graphics pipeline for 3D rendered cast shadow generation using generative adversarial networks (GANs). This work is inspired by the existing regression models as well as other convolutional neural networks such as the U-Net architectures which can be geared to produce believable global illumination effects. Here, we use a semi-supervised GANs model comprising of a PatchGAN and a conditional GAN which is then complemented by a U-Net structure. We have adopted this structure because of its training ability and the quality of the results that come forth. Unlike other forms of GANs, the chosen implementation utilises colour labels to generate believable visual coherence. We carried forth a series of experiments, through laboratory generated image sets, to explore the extent at which colour can create the correct shadows for a variety of 3D shadowed and un-shadowed images. Once an optimised model is achieved, we then apply high resolution image mappings to enhance the quality of the final render. As a result, we have established that the chosen GANs model can produce believable outputs with the correct cast shadows with plausible scores on PSNR and SSIM similarity index metrices.","publish_time":1590364800000,"author_summary":" Taif, Khasrouf; Ugail, Hassan; Mehmood, Irfan","abstract_summary":" We propose a computer graphics pipeline for 3D<br>rendered cast shadow generation using generative<br>adversarial networks (GANs). This work is inspired by the<br>existing regression models as well as other<br>convolutional neural networks such as the U-Net<br>architectures which can be geared to produce believable<br>global illumination effects. Here, we use a<br>semi-supervised GANs model comprising of a PatchGAN and a<br>conditional GAN which is then complemented by a U-Net<br>structure. We have adopted this structure because of its<br>training ability and the quality of the results that come<br>forth. Unlike other forms of GANs, the chosen<br>implementation utilises colour...","title_summary":" Cast Shadow Generation Using Generative<br>Adversarial Networks","x":-10.5282440186,"y":41.410358429,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.5282440186,"tsne_y":41.410358429,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"h01j6t6e","source_x":"PMC","title":"Improved Two-Step Binarization of Degraded Document Images Based on Gaussian Mixture Model","doi":"10.1007\/978-3-030-50426-7_35","abstract":"Image binarization is one of the most relevant preprocessing operations influencing the results of further image analysis conducted for many purposes. During this step a significant loss of information occurs and the use of inappropriate thresholding methods may cause difficulties in further shape analysis or even make it impossible to recognize different shapes of objects or characters. Some of the most typical applications utilizing the analysis of binary images are Optical Character Recognition (OCR) and Optical Mark Recognition (OMR), which may also be applied for unevenly illuminated natural images, as well as for challenging degraded historical document images, considered as typical benchmarking tools for image binarization algorithms. To face the still valid challenge of relatively fast and simple, but robust binarization of degraded document images, a novel two-step algorithm utilizing initial thresholding, based on the modelling of the simplified image histogram using Gaussian Mixture Model (GMM) and the Monte Carlo method, is proposed in the paper. This approach can be considered as the extension of recently developed image preprocessing method utilizing Generalized Gaussian Distribution (GGD), based on the assumption of its similarity to the histograms of ground truth binary images distorted by Gaussian noise. The processing time of the first step, producing the intermediate images with partially removed background information, may be significantly reduced due to the use of the Monte Carlo method. The proposed improved approach leads to even better results, not only for well-known DIBCO benchmarking databases, but also for more demanding Bickley Diary dataset, allowing the use of some well-known classical binarization methods, including the global ones, in the second step of the algorithm.","publish_time":1590364800000,"author_summary":" Krupi\u0144ski, Robert; Lech, Piotr; Okarma,<br>Krzysztof","abstract_summary":" Image binarization is one of the most relevant<br>preprocessing operations influencing the results of further<br>image analysis conducted for many purposes. During<br>this step a significant loss of information occurs<br>and the use of inappropriate thresholding methods<br>may cause difficulties in further shape analysis<br>or even make it impossible to recognize different<br>shapes of objects or characters. Some of the most<br>typical applications utilizing the analysis of binary<br>images are Optical Character Recognition (OCR) and<br>Optical Mark Recognition (OMR), which may also be<br>applied for unevenly illuminated natural images, as<br>well as for challenging degraded historical<br>document images, considered as...","title_summary":" Improved Two-Step Binarization of Degraded<br>Document Images Based on Gaussian Mixture Model","x":-9.5636329651,"y":42.4324531555,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.5636329651,"tsne_y":42.4324531555,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"mikuptli","source_x":"PMC","title":"Depth Map Estimation with Consistent Normals from Stereo Images","doi":"10.1007\/978-3-030-50426-7_41","abstract":"The total variation regularization of non-convex data terms in continuous variational models can be convexified by the so called functional lifting, which may be considered as a continuous counterpart of Ishikawa\u2019s method for multi-label discrete variational problems. We solve the resulting convex continuous variational problem by the augmented Lagrangian method. Application of this method to the dense depth map estimation allows us to obtain a consistent normal field to the depth surface as a byproduct. We illustrate the method with numerical examples of the depth map estimation for rectified stereo image pairs.","publish_time":1590364800000,"author_summary":" Malyshev, Alexander","abstract_summary":" The total variation regularization of<br>non-convex data terms in continuous variational models<br>can be convexified by the so called functional<br>lifting, which may be considered as a continuous<br>counterpart of Ishikawa\u2019s method for multi-label discrete<br>variational problems. We solve the resulting convex<br>continuous variational problem by the augmented<br>Lagrangian method. Application of this method to the dense<br>depth map estimation allows us to obtain a consistent<br>normal field to the depth surface as a byproduct. We<br>illustrate the method with numerical examples of the depth<br>map estimation for rectified stereo image pairs.","title_summary":" Depth Map Estimation with Consistent Normals<br>from Stereo Images","x":-10.0387182236,"y":43.4363479614,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.0387182236,"tsne_y":43.4363479614,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"11mt1pz5","source_x":"PMC","title":"Plane Space Representation in Context of Mode-Based Symmetry Plane Detection","doi":"10.1007\/978-3-030-50426-7_38","abstract":"This paper describes various representations of the space of planes. The main focus is on the plane space representation in the symmetry plane detection in [Formula: see text] where many candidate planes for many pairs of points of the given object are created and then the most often candidate is found as a mode in the candidate space, so-called Mode-based approach. The result depends on the representation used in the mode-seeking process. The most important aspect is how well distances in the space correspond to similarities of the actual planes with respect to the input object. So, we describe various usable distance functions and compare them both theoretically and practically. The results suggest that, when using the Mode-based approach, representing planes by reflection transformations is the best way but other simpler representations are applicable as well. On the other hand, representations using 3D dual spaces are not very appropriate. Furthermore, we introduce a novel way of representing the reflection transformations using dual quaternions.","publish_time":1590364800000,"author_summary":" Hruda, Luk\u00e1\u0161; Kolingerov\u00e1, Ivana; L\u00e1vi\u010dka,<br>Miroslav","abstract_summary":" This paper describes various representations<br>of the space of planes. The main focus is on the<br>plane space representation in the symmetry plane<br>detection in [Formula: see text] where many candidate<br>planes for many pairs of points of the given object are<br>created and then the most often candidate is found as a<br>mode in the candidate space, so-called Mode-based<br>approach. The result depends on the representation used<br>in the mode-seeking process. The most important<br>aspect is how well distances in the space correspond to<br>similarities of the actual planes with respect to the input<br>object. So, we describe various...","title_summary":" Plane Space Representation in Context of<br>Mode-Based Symmetry Plane Detection","x":-9.8429670334,"y":42.4305801392,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.8429670334,"tsne_y":42.4305801392,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"pk4rhjlj","source_x":"PMC","title":"A Combination of Moment Descriptors, Fourier Transform and Matching Measures for Action Recognition Based on Shape","doi":"10.1007\/978-3-030-50417-5_28","abstract":"This paper presents an approach for human action recognition based on shape analysis. The purpose of the approach is to classify simple actions by applying shape descriptors to sequences of binary silhouettes. The recognition process consists of several main stages: shape representation, action sequence representation and action sequence classification. Firstly, each shape is represented using a selected shape descriptor. Secondly, shape descriptors of each sequence are matched, matching values are put into a vector and transformed into final action representation\u2014we employ Fourier transform-based methods to obtain action representations equal in size. A classification into eight classes is performed using leave-one-out cross-validation and template matching approaches. We present results of the experiments on classification accuracy using moment-based shape descriptors (Zernike Moments, Moment Invariants and Contour Sequence Moments) and three matching measures (Euclidean distance, correlation coefficient and C1 correlation). Different combinations of the above-mentioned algorithms are examined in order to indicate the most effective one. The experiments show that satisfactory results are obtained when low-order Zernike Moments are used for shape representation and absolute values of Fourier transform are applied to represent action sequences. Moreover, the selection of matching technique strongly influences final classification results.","publish_time":1592179200000,"author_summary":" Go\u015bciewska, Katarzyna; Frejlichowski,<br>Dariusz","abstract_summary":" This paper presents an approach for human<br>action recognition based on shape analysis. The<br>purpose of the approach is to classify simple actions by<br>applying shape descriptors to sequences of binary<br>silhouettes. The recognition process consists of several<br>main stages: shape representation, action<br>sequence representation and action sequence<br>classification. Firstly, each shape is represented using a<br>selected shape descriptor. Secondly, shape<br>descriptors of each sequence are matched, matching values<br>are put into a vector and transformed into final<br>action representation\u2014we employ Fourier<br>transform-based methods to obtain action representations<br>equal in size. A classification into eight classes is<br>performed using leave-one-out...","title_summary":" A Combination of Moment Descriptors, Fourier<br>Transform and Matching Measures for Action Recognition<br>Based on Shape","x":-9.9589090347,"y":42.1578369141,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.9589090347,"tsne_y":42.1578369141,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"dtb959ei","source_x":"PMC","title":"Evolving Long Short-Term Memory Networks","doi":"10.1007\/978-3-030-50417-5_25","abstract":"Machine learning techniques have been massively employed in the last years over a wide variety of applications, especially those based on deep learning, which obtained state-of-the-art results in several research fields. Despite the success, such techniques still suffer from some shortcomings, such as the sensitivity to their hyperparameters, whose proper selection is context-dependent, i.e., the model may perform better over each dataset when using a specific set of hyperparameters. Therefore, we propose an approach based on evolutionary optimization techniques for fine-tuning Long Short-Term Memory networks. Experiments were conducted over three public word-processing datasets for part-of-speech tagging. The results showed the robustness of the proposed approach for the aforementioned task.","publish_time":1592179200000,"author_summary":" Lobo Neto, Vicente Coelho; Passos, Leandro<br>Aparecido; Papa, Jo\u00e3o Paulo","abstract_summary":" Machine learning techniques have been<br>massively employed in the last years over a wide variety of<br>applications, especially those based on deep learning, which<br>obtained state-of-the-art results in several research<br>fields. Despite the success, such techniques still<br>suffer from some shortcomings, such as the<br>sensitivity to their hyperparameters, whose proper<br>selection is context-dependent, i.e., the model may<br>perform better over each dataset when using a specific<br>set of hyperparameters. Therefore, we propose an<br>approach based on evolutionary optimization<br>techniques for fine-tuning Long Short-Term Memory<br>networks. Experiments were conducted over three public<br>word-processing datasets for part-of-speech tagging. The<br>results showed the...","title_summary":" Evolving Long Short-Term Memory Networks","x":-12.3686437607,"y":40.1711921692,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.3686437607,"tsne_y":40.1711921692,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"3a6kdh4t","source_x":"PMC","title":"Improving Accuracy and Speeding Up Document Image Classification Through Parallel Systems","doi":"10.1007\/978-3-030-50417-5_29","abstract":"This paper presents a study showing the benefits of the EfficientNet models compared with heavier Convolutional Neural Networks (CNNs) in the Document Classification task, essential problem in the digitalization process of institutions. We show in the RVL-CDIP dataset that we can improve previous results with a much lighter model and present its transfer learning capabilities on a smaller in-domain dataset such as Tobacco3482. Moreover, we present an ensemble pipeline which is able to boost solely image input by combining image model predictions with the ones generated by BERT model on extracted text by OCR. We also show that the batch size can be effectively increased without hindering its accuracy so that the training process can be sped up by parallelizing throughout multiple GPUs, decreasing the computational time needed. Lastly, we expose the training performance differences between PyTorch and Tensorflow Deep Learning frameworks.","publish_time":1592179200000,"author_summary":" Ferrando, Javier; Dom\u00ednguez, Juan Luis;<br>Torres, Jordi; Garc\u00eda, Ra\u00fal; Garc\u00eda, David; Garrido,<br>Daniel; Cortada, Jordi; Valero, Mateo","abstract_summary":" This paper presents a study showing the<br>benefits of the EfficientNet models compared with<br>heavier Convolutional Neural Networks (CNNs) in the<br>Document Classification task, essential problem in the<br>digitalization process of institutions. We show in the<br>RVL-CDIP dataset that we can improve previous results<br>with a much lighter model and present its transfer<br>learning capabilities on a smaller in-domain dataset<br>such as Tobacco3482. Moreover, we present an<br>ensemble pipeline which is able to boost solely image<br>input by combining image model predictions with the<br>ones generated by BERT model on extracted text by<br>OCR. We also show that the batch...","title_summary":" Improving Accuracy and Speeding Up Document<br>Image Classification Through Parallel Systems","x":-10.3704652786,"y":40.9163246155,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.3704652786,"tsne_y":40.9163246155,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"y3taa8ie","source_x":"PMC","title":"Missing Features Reconstruction Using a Wasserstein Generative Adversarial Imputation Network","doi":"10.1007\/978-3-030-50423-6_17","abstract":"Missing data is one of the most common preprocessing problems. In this paper, we experimentally research the use of generative and non-generative models for feature reconstruction. Variational Autoencoder with Arbitrary Conditioning (VAEAC) and Generative Adversarial Imputation Network (GAIN) were researched as representatives of generative models, while the denoising autoencoder (DAE) represented non-generative models. Performance of the models is compared to traditional methods k-nearest neighbors (k-NN) and Multiple Imputation by Chained Equations (MICE). Moreover, we introduce WGAIN as the Wasserstein modification of GAIN, which turns out to be the best imputation model when the degree of missingness is less than or equal to [Formula: see text]. Experiments were performed on real-world and artificial datasets with continuous features where different percentages of features, varying from [Formula: see text] to [Formula: see text], were missing. Evaluation of algorithms was done by measuring the accuracy of the classification model previously trained on the uncorrupted dataset. The results show that GAIN and especially WGAIN are the best imputers regardless of the conditions. In general, they outperform or are comparative to MICE, k-NN, DAE, and VAEAC.","publish_time":1590192000000,"author_summary":" Friedjungov\u00e1, Magda; Va\u0161ata, Daniel;<br>Balatsko, Maksym; Ji\u0159ina, Marcel","abstract_summary":" Missing data is one of the most common<br>preprocessing problems. In this paper, we experimentally<br>research the use of generative and non-generative<br>models for feature reconstruction. Variational<br>Autoencoder with Arbitrary Conditioning (VAEAC) and<br>Generative Adversarial Imputation Network (GAIN) were<br>researched as representatives of generative models,<br>while the denoising autoencoder (DAE) represented<br>non-generative models. Performance of the models is compared<br>to traditional methods k-nearest neighbors<br>(k-NN) and Multiple Imputation by Chained Equations<br>(MICE). Moreover, we introduce WGAIN as the<br>Wasserstein modification of GAIN, which turns out to be the<br>best imputation model when the degree of<br>missingness is less than or...","title_summary":" Missing Features Reconstruction Using a<br>Wasserstein Generative Adversarial Imputation Network","x":-11.5238981247,"y":40.2441101074,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.5238981247,"tsne_y":40.2441101074,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"wad23nsb","source_x":"PMC","title":"Design of Loss Functions for Solving Inverse Problems Using Deep Learning","doi":"10.1007\/978-3-030-50420-5_12","abstract":"Solving inverse problems is a crucial task in several applications that strongly affect our daily lives, including multiple engineering fields, military operations, and\/or energy production. There exist different methods for solving inverse problems, including gradient based methods, statistics based methods, and Deep Learning (DL) methods. In this work, we focus on the latest. Specifically, we study the design of proper loss functions for dealing with inverse problems using DL. To do this, we introduce a simple benchmark problem with known analytical solution. Then, we propose multiple loss functions and compare their performance when applied to our benchmark example problem. In addition, we analyze how to improve the approximation of the forward function by: (a) considering a Hermite-type interpolation loss function, and (b) reducing the number of samples for the forward training in the Encoder-Decoder method. Results indicate that a correct design of the loss function is crucial to obtain accurate inversion results.","publish_time":1590105600000,"author_summary":" Rivera, Jon Ander; Pardo, David; Alberdi,<br>Elisabete","abstract_summary":" Solving inverse problems is a crucial task in<br>several applications that strongly affect our daily<br>lives, including multiple engineering fields,<br>military operations, and\/or energy production. There<br>exist different methods for solving inverse<br>problems, including gradient based methods, statistics<br>based methods, and Deep Learning (DL) methods. In<br>this work, we focus on the latest. Specifically, we<br>study the design of proper loss functions for dealing<br>with inverse problems using DL. To do this, we<br>introduce a simple benchmark problem with known<br>analytical solution. Then, we propose multiple loss<br>functions and compare their performance when applied to<br>our benchmark example problem. In...","title_summary":" Design of Loss Functions for Solving Inverse<br>Problems Using Deep Learning","x":-12.2732496262,"y":40.3865089417,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.2732496262,"tsne_y":40.3865089417,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"2itbuydk","source_x":"PMC","title":"Granulation-Based Reverse Image Retrieval for Microscopic Rock Images","doi":"10.1007\/978-3-030-50420-5_6","abstract":"The paper presents a method of object detection on microscopic images of rocks, which makes it possible to identify images with similar structural features of the rock. These features are understood as the sizes and shapes of its components and the mutual relationships between them. The proposed detection methodology is an adaptive and unsupervised method that analyzes characteristic color clusters in the image. It achieves good detection results for rocks with clear clusters of colored objects. For the analyzed data set, the method finds in the rock image sets with high visual similarity, which translates into the geological classification of rocks at a level of above 78%. Considering the fact that the proposed method is based on segmentation that does not require any input parameters, this result should be considered satisfactory. In the authors\u2019 opinion, this method can be used in issues of rock image search, sorting, or e.g. automatic selection of optimal segmentation techniques.","publish_time":1590105600000,"author_summary":" Habrat, Magdalena; M\u0142ynarczuk, Mariusz","abstract_summary":" The paper presents a method of object detection<br>on microscopic images of rocks, which makes it<br>possible to identify images with similar structural<br>features of the rock. These features are understood as<br>the sizes and shapes of its components and the<br>mutual relationships between them. The proposed<br>detection methodology is an adaptive and unsupervised<br>method that analyzes characteristic color clusters<br>in the image. It achieves good detection results<br>for rocks with clear clusters of colored objects.<br>For the analyzed data set, the method finds in the<br>rock image sets with high visual similarity, which<br>translates into the geological classification of...","title_summary":" Granulation-Based Reverse Image Retrieval<br>for Microscopic Rock Images","x":-9.3657417297,"y":42.7788314819,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3657417297,"tsne_y":42.7788314819,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"edj6hm89","source_x":"PMC","title":"Retrain or Not Retrain? - Efficient Pruning Methods of Deep CNN Networks","doi":"10.1007\/978-3-030-50420-5_34","abstract":"Nowadays, convolutional neural networks (CNN) play a major role in image processing tasks like image classification, object detection, semantic segmentation. Very often CNN networks have from several to hundred stacked layers with several megabytes of weights. One of the possible techniques to reduce complexity and memory footprint is pruning. Pruning is a process of removing weights which connect neurons from two adjacent layers in the network. The process of finding near optimal solution with specified and acceptable drop in accuracy can be more sophisticated when DL model has higher number of convolutional layers. In the paper few approaches based on retraining and no retraining are described and compared together.","publish_time":1590105600000,"author_summary":" Pietron, Marcin; Wielgosz, Maciej","abstract_summary":" Nowadays, convolutional neural networks<br>(CNN) play a major role in image processing tasks like<br>image classification, object detection, semantic<br>segmentation. Very often CNN networks have from several to<br>hundred stacked layers with several megabytes of<br>weights. One of the possible techniques to reduce<br>complexity and memory footprint is pruning. Pruning is a<br>process of removing weights which connect neurons from<br>two adjacent layers in the network. The process of<br>finding near optimal solution with specified and<br>acceptable drop in accuracy can be more sophisticated when<br>DL model has higher number of convolutional<br>layers. In the paper few approaches based on...","title_summary":" Retrain or Not Retrain? - Efficient Pruning<br>Methods of Deep CNN Networks","x":-10.6924333572,"y":40.9473342896,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.6924333572,"tsne_y":40.9473342896,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"5e93h3ip","source_x":"PMC","title":"Deep Low-Density Separation for Semi-supervised Classification","doi":"10.1007\/978-3-030-50420-5_22","abstract":"Given a small set of labeled data and a large set of unlabeled data, semi-supervised learning (ssl) attempts to leverage the location of the unlabeled datapoints in order to create a better classifier than could be obtained from supervised methods applied to the labeled training set alone. Effective ssl imposes structural assumptions on the data, e.g. that neighbors are more likely to share a classification or that the decision boundary lies in an area of low density. For complex and high-dimensional data, neural networks can learn feature embeddings to which traditional ssl methods can then be applied in what we call hybrid methods. Previously-developed hybrid methods iterate between refining a latent representation and performing graph-based ssl on this representation. In this paper, we introduce a novel hybrid method that instead applies low-density separation to the embedded features. We describe it in detail and discuss why low-density separation may better suited for ssl on neural network-based embeddings than graph-based algorithms. We validate our method using in-house customer survey data and compare it to other state-of-the-art learning methods. Our approach effectively classifies thousands of unlabeled users from a relatively small number of hand-classified examples.","publish_time":1590105600000,"author_summary":" Burkhart, Michael C.; Shan, Kyle","abstract_summary":" Given a small set of labeled data and a large set<br>of unlabeled data, semi-supervised learning<br>(ssl) attempts to leverage the location of the<br>unlabeled datapoints in order to create a better<br>classifier than could be obtained from supervised methods<br>applied to the labeled training set alone. Effective<br>ssl imposes structural assumptions on the data,<br>e.g. that neighbors are more likely to share a<br>classification or that the decision boundary lies in an area of<br>low density. For complex and high-dimensional<br>data, neural networks can learn feature embeddings<br>to which traditional ssl methods can then be<br>applied in what we...","title_summary":" Deep Low-Density Separation for<br>Semi-supervised Classification","x":-12.9177217484,"y":39.7657089233,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.9177217484,"tsne_y":39.7657089233,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"bvfsznob","source_x":"PMC","title":"ArtPDGAN: Creating Artistic Pencil Drawing with Key Map Using Generative Adversarial Networks","doi":"10.1007\/978-3-030-50436-6_21","abstract":"A lot of researches focus on image transfer using deep learning, especially with generative adversarial networks (GANs). However, no existing methods can produce high quality artistic pencil drawings. First, artists do not convert all the details of the photos into the drawings. Instead, artists tend to use strategies to magnify some special parts of the items and cut others down. Second, the elements in artistic drawings may not be located precisely. What\u2019s more, the lines may not relate to the features of the items strictly. To address above challenges, we propose ArtPDGAN, a novel GAN based framework that combines an image-to-image network to generate key map. And then, we use the key map as an important part of input to generate artistic pencil drawings. The key map can show the key parts of the items to guide the generator. We use a paired and unaligned artistic drawing dataset containing high-resolution photos of items and corresponding professional artistic pencil drawings to train ArtPDGAN. Results of our experiments show that the proposed framework performs excellently against existing methods in terms of similarity to artist\u2019s work and user evaluations.","publish_time":1590364800000,"author_summary":" Li, SuChang; Li, Kan; Kacher, Ilyes; Taira,<br>Yuichiro; Yanatori, Bungo; Sato, Imari","abstract_summary":" A lot of researches focus on image transfer<br>using deep learning, especially with generative<br>adversarial networks (GANs). However, no existing methods<br>can produce high quality artistic pencil<br>drawings. First, artists do not convert all the details of<br>the photos into the drawings. Instead, artists<br>tend to use strategies to magnify some special parts<br>of the items and cut others down. Second, the<br>elements in artistic drawings may not be located<br>precisely. What\u2019s more, the lines may not relate to the<br>features of the items strictly. To address above<br>challenges, we propose ArtPDGAN, a novel GAN based<br>framework that combines an...","title_summary":" ArtPDGAN: Creating Artistic Pencil Drawing<br>with Key Map Using Generative Adversarial Networks","x":-10.5744972229,"y":41.3485107422,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.5744972229,"tsne_y":41.3485107422,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"upbb9to1","source_x":"PMC","title":"High Accuracy Terrain Reconstruction from Point Clouds Using Implicit Deformable Model","doi":"10.1007\/978-3-030-50433-5_20","abstract":"Few previous works have studied the modeling of forest ground surfaces from LiDAR point clouds using implicit functions. [10] is a pioneer in this area. However, by design this approach proposes over-smoothed surfaces, in particular in highly occluded areas, limiting its ability to reconstruct fine-grained terrain surfaces. This paper presents a method designed to finely approximate ground surfaces by relying on deep learning to separate vegetation from potential ground points, filling holes by blending multiple local approximations through the partition of unity principle, then improving the accuracy of the reconstructed surfaces by pushing the surface towards the data points through an iterative convection model.","publish_time":1590364800000,"author_summary":" Morel, Jules; Bac, Alexandra; Kanai, Takashi","abstract_summary":" Few previous works have studied the modeling of<br>forest ground surfaces from LiDAR point clouds using<br>implicit functions. [10] is a pioneer in this area.<br>However, by design this approach proposes<br>over-smoothed surfaces, in particular in highly occluded<br>areas, limiting its ability to reconstruct<br>fine-grained terrain surfaces. This paper presents a method<br>designed to finely approximate ground surfaces by<br>relying on deep learning to separate vegetation from<br>potential ground points, filling holes by blending<br>multiple local approximations through the partition of<br>unity principle, then improving the accuracy of the<br>reconstructed surfaces by pushing the surface towards the<br>data points through...","title_summary":" High Accuracy Terrain Reconstruction from<br>Point Clouds Using Implicit Deformable Model","x":-9.8187875748,"y":44.046962738,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.8187875748,"tsne_y":44.046962738,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"05ab85tt","source_x":"PMC","title":"Normal Grouping Density Separation (NGDS): A Novel Object-Driven Indoor Point Cloud Partition Method","doi":"10.1007\/978-3-030-50433-5_8","abstract":"Precise segmentation\/partition is an essential part of many point cloud processing strategies. In the state-of-the-art methods, either the number of clusters or expected supervoxel resolution needs to be carefully selected before segmentation. This makes these processes semi-supervised. The proposed Normal Grouping- Density Separation (NGDS) strategy, relying on both grouping normal vectors into cardinal directions and density-based separation, produces clusters of better (according to use quality measures) quality than current state-of-the-art methods for widely applied object-annotated indoor benchmark dataset. The method reaches, on average, lower under-segmentation error than VCCS (by 45.9pp), Lin et al. (by 14.8pp), and SSP (by 26.2pp). Another metric - achievable segmentation accuracy - yields 92.1% across the tested dataset what is higher value than VCCS (by 14pp), Lin et al. (by 3.8pp), and SSP (by 10.3pp). The experiment carried out indicates superiority of the proposed method as a partition\/segmentation algorithm - a process being usually a preprocessing stage of many object detection workflows.","publish_time":1590364800000,"author_summary":" Walczak, Jakub; Andrzejczak, Grzegorz;<br>Scherer, Rafa\u0142; Wojciechowski, Adam","abstract_summary":" Precise segmentation\/partition is an<br>essential part of many point cloud processing<br>strategies. In the state-of-the-art methods, either the<br>number of clusters or expected supervoxel resolution<br>needs to be carefully selected before segmentation.<br>This makes these processes semi-supervised. The<br>proposed Normal Grouping- Density Separation (NGDS)<br>strategy, relying on both grouping normal vectors into<br>cardinal directions and density-based separation,<br>produces clusters of better (according to use quality<br>measures) quality than current state-of-the-art<br>methods for widely applied object-annotated indoor<br>benchmark dataset. The method reaches, on average, lower<br>under-segmentation error than VCCS (by 45.9pp), Lin et al. (by<br>14.8pp), and SSP (by 26.2pp). Another...","title_summary":" Normal Grouping Density Separation (NGDS): A<br>Novel Object-Driven Indoor Point Cloud Partition<br>Method","x":-9.2805213928,"y":41.9261894226,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2805213928,"tsne_y":41.9261894226,"subcluster":7,"subcluster_description":"Cnnan Efficient Image Descriptor","shape":"p"},{"cord_uid":"rfw7iutn","source_x":"PMC","title":"Combined Metrics for Quality Assessment of 3D Printed Surfaces for Aesthetic Purposes: Towards Higher Accordance with Subjective Evaluations","doi":"10.1007\/978-3-030-50436-6_24","abstract":"Objective quality assessment for 3D printing purposes may be considered as one of the most useful applications of machine vision in smart monitoring related to the development of the Industry 4.0 solutions. During recent years several approaches have been proposed, assuming observing the side surfaces, mainly based on the analysis of the regularity of visible patterns, which represent the consecutive printed layers. These methods, based on the use of general purpose image quality assessment (IQA) metrics, Hough transform, entropy and texture analysis, make it possible to classify the printed samples, independently of the filament\u2019s colour, into low and high quality classes, with the use of photos or 3D scans of the side surfaces. The next step of research, investigated in this paper, is the combination of various proposed approaches to develop a combined metric, possibly highly correlated with subjective opinions. Since the correlation of single metrics developed mainly for classification is relatively low, their combination makes it possible to achieve much better results, verified using an original, newly developed database containing 107 captured images and 3D scans of the 3D printed surfaces with various colours and local distortions caused by external factors, together with Mean Opinion Scores (MOS) gathered from independent observers. Obtained results are promising and may be a starting point for further research towards the optimisation of the newly developed metrics for the automatic assessment of the 3D printed surfaces, mainly for aesthetic purposes.","publish_time":1590364800000,"author_summary":" Fastowicz, Jaros\u0142aw; Lech, Piotr; Okarma,<br>Krzysztof","abstract_summary":" Objective quality assessment for 3D printing<br>purposes may be considered as one of the most useful<br>applications of machine vision in smart monitoring related<br>to the development of the Industry 4.0 solutions.<br>During recent years several approaches have been<br>proposed, assuming observing the side surfaces, mainly<br>based on the analysis of the regularity of visible<br>patterns, which represent the consecutive printed<br>layers. These methods, based on the use of general<br>purpose image quality assessment (IQA) metrics, Hough<br>transform, entropy and texture analysis, make it possible<br>to classify the printed samples, independently<br>of the filament\u2019s colour, into low and high<br>quality...","title_summary":" Combined Metrics for Quality Assessment of 3D<br>Printed Surfaces for Aesthetic Purposes: Towards<br>Higher Accordance with Subjective Evaluations","x":-9.0205488205,"y":43.9361343384,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.0205488205,"tsne_y":43.9361343384,"subcluster":0,"subcluster_description":"Individual Mangrove Tree Modelingcombined","shape":"p"},{"cord_uid":"o3zmg7wj","source_x":"PMC","title":"Interactive Travel Aid for the Visually Impaired: from Depth Maps to Sonic Patterns and Verbal Messages","doi":"10.1007\/978-3-030-50436-6_22","abstract":"This paper presents user trials of a prototype micro-navigation aid for the visually impaired. The main advantage of the system is its small form factor. The device consists of a Structure Sensor depth camera, a smartphone, a remote controller and a pair of headphones. An original feature of the system is its interactivity. The user can activate different space scanning modes and different sound presentation schemes for 3D scenes on demand. The results of the trials are documented by timeline logs recording the activation of different interactive modes. The aim of the first trial was to test system capability for aiding the visually impaired to avoid obstacles. The second tested system efficiency at detecting open spaces. The two visually impaired testers performed the trials successfully, although the times required to complete the tasks seem rather long. Nevertheless, the trials show the potential usefulness of the system as a navigational aid and have enabled us to introduce numerous improvements to the tested prototype.","publish_time":1590364800000,"author_summary":" Skulimowski, Piotr; Strumillo, Pawel","abstract_summary":" This paper presents user trials of a prototype<br>micro-navigation aid for the visually impaired. The main<br>advantage of the system is its small form factor. The<br>device consists of a Structure Sensor depth camera, a<br>smartphone, a remote controller and a pair of headphones. An<br>original feature of the system is its interactivity. The<br>user can activate different space scanning modes<br>and different sound presentation schemes for 3D<br>scenes on demand. The results of the trials are<br>documented by timeline logs recording the activation of<br>different interactive modes. The aim of the first trial<br>was to test system capability for...","title_summary":" Interactive Travel Aid for the Visually<br>Impaired: from Depth Maps to Sonic Patterns and Verbal<br>Messages","x":-8.9521417618,"y":43.186088562,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.9521417618,"tsne_y":43.186088562,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ex497vbq","source_x":"PMC","title":"Image complexity analysis with scanpath identification using remote gaze estimation model","doi":"10.1007\/s11042-020-09117-9","abstract":"Analysis of gaze points has been a vital tool for understanding varied human behavioral pattern and underlying psychological processing. Gaze points are analyzed generally in terms of two events of fixations and saccades that are collectively termed as scanpath. Scanpath could potentially establish correlation between visual scenery and human cognitive tendencies. Scanpath has been analyzed for different domains that include visual perception, usability, memory, visual search or low level attributes like color, illumination and edges in an image. Visual search is one prominent area that examines scanpath of subjects while a target object is searched in a given set of images. Visual search explores behavioral tendencies of subjects with respect to image complexity. Complexity of an image is governed by spatial, frequency and color information present in the image. Scanpath based image complexity analysis determines human visual behavior that could lead to development of interactive and intelligent systems. There are several sophisticated eye tracking devices and associated algorithms for recording and classification of scanpath. However, in the present scenario when the chances of viral infections (COVID-19) from known and unknown sources are high, it is very important that the contact less methods and models be designed. In addition, even though the devices acquire and process eye movement data with fair accuracy but are intrusive and costly. The objective of current research work is to establish the complexity of the given set of images while target objects are searched and to present analysis of gaze search pattern. To achieve these objectives a remote gaze estimation and analysis model has been proposed for scanpath identification and analysis. The model is an alternate option for gaze point tracking and scanpath analysis that is non intrusive and low cost. The gaze points are tracked remotely as against sophisticated wearable eye tracking devices available in the market. The model employs easily available softwares and hardware devices. In the current work, complexity is derived on the basis of analysis of fixation and saccade gaze points. Based on the results generated by the proposed model, influence on subjects due to external stimuli is studied. The set of images chosen, act as external stimuli for the subjects during visual search. In order to statistically analyze scanpath for different subjects, certain scanpath parameters have been identified. The model maps and classifies eye movement gaze points into fixations and saccades and generates data for identified parameters. For eye detection and subsequent iris detection voila jones and circular hough transform (CHT) algorithms have been used. Identification by dispersion threshold (I-DT) is implemented for scanpath identification. The algorithms are customized for better iris and scanpath detection. Algorithms are developed for gaze screen mapping and classification of fixations and saccades. The experimentation has been carried on different subjects. Variations during visual search have been observed and analyzed. The present model requires no contact of human subject with any equipment including eye tracking devices, screen or computing devices.","publish_time":1592611200000,"author_summary":" Ishrat, Mohsina; Abrol, Pawanesh","abstract_summary":" Analysis of gaze points has been a vital tool for<br>understanding varied human behavioral pattern and<br>underlying psychological processing. Gaze points are<br>analyzed generally in terms of two events of fixations<br>and saccades that are collectively termed as<br>scanpath. Scanpath could potentially establish<br>correlation between visual scenery and human cognitive<br>tendencies. Scanpath has been analyzed for different<br>domains that include visual perception, usability,<br>memory, visual search or low level attributes like<br>color, illumination and edges in an image. Visual<br>search is one prominent area that examines scanpath of<br>subjects while a target object is searched in a given set<br>of...","title_summary":" Image complexity analysis with scanpath<br>identification using remote gaze estimation model","x":-8.9076652527,"y":42.6172332764,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.9076652527,"tsne_y":42.6172332764,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"xqgf8jif","source_x":"PMC","title":"Training Physical and Geometrical Mid-Points for Multi-person Pose Estimation and Human Detection Under Congestion and Low Resolution","doi":"10.1007\/s42979-020-00217-9","abstract":"This paper introduces the design and evaluation of NeoPose which is developed for multi-person pose estimation and human detection. The design of NeoPose is targeting the issue of human detection under congested situation and with low resolution in the image. Under such situations, we compared the performance of different versions of NeoPose as well as other existing algorithms in a human detection task. Throughout the task, the usefulness of two kinds of mid-point (physical and geometrical mid-points) and a deconvolution structure was discussed. Experiment results indicated that NeoPose which applied geometrical mid-points and deconvolution structure performed the best in terms of both precision and recall in the evaluation.","publish_time":1592697600000,"author_summary":" Pan, Yadong; Kawai, Ryo; Yoshida, Noboru;<br>Ikeda, Hiroo; Nishimura, Shoji","abstract_summary":" This paper introduces the design and<br>evaluation of NeoPose which is developed for multi-person<br>pose estimation and human detection. The design of<br>NeoPose is targeting the issue of human detection under<br>congested situation and with low resolution in the image.<br>Under such situations, we compared the performance<br>of different versions of NeoPose as well as other<br>existing algorithms in a human detection task.<br>Throughout the task, the usefulness of two kinds of<br>mid-point (physical and geometrical mid-points) and a<br>deconvolution structure was discussed. Experiment results<br>indicated that NeoPose which applied geometrical<br>mid-points and deconvolution structure performed the<br>best in terms...","title_summary":" Training Physical and Geometrical Mid-Points<br>for Multi-person Pose Estimation and Human<br>Detection Under Congestion and Low Resolution","x":-9.1816730499,"y":41.8025131226,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.1816730499,"tsne_y":41.8025131226,"subcluster":7,"subcluster_description":"Cnnan Efficient Image Descriptor","shape":"p"},{"cord_uid":"uu6rpcpr","source_x":"PMC","title":"Mobile Assistive Application for Blind People in Indoor Navigation","doi":"10.1007\/978-3-030-51517-1_36","abstract":"Navigation is an important human task that needs the human sense of vision. In this context, recent technologies developments provide technical assistance to support the visually impaired in their daily tasks and improve their quality of life. In this paper, we present a mobile assistive application called \u201cGuiderMoi\u201d that retrieves information about directions using color targets and identifies the next orientation for the visually impaired. In order to avoid the failure in detection and the inaccurate tracking caused by the mobile camera, the proposed method based on the CamShift algorithm aims to introduce better location and identification of color targets. Tests were conduct in natural indoor scene. The results depending on the distance and the angle of view, defined the accurate values to have a highest rate of target recognition. This work has perspectives for this such as implicating the augmented reality and the intelligent navigation based on machine learning and real-time processing.","publish_time":1590883200000,"author_summary":" Jabnoun, Hanen; Hashish, Mohammad Abu;<br>Benzarti, Faouzi","abstract_summary":" Navigation is an important human task that<br>needs the human sense of vision. In this context,<br>recent technologies developments provide technical<br>assistance to support the visually impaired in their daily<br>tasks and improve their quality of life. In this<br>paper, we present a mobile assistive application<br>called \u201cGuiderMoi\u201d that retrieves information about<br>directions using color targets and identifies the next<br>orientation for the visually impaired. In order to avoid the<br>failure in detection and the inaccurate tracking<br>caused by the mobile camera, the proposed method based<br>on the CamShift algorithm aims to introduce<br>better location and identification of color targets....","title_summary":" Mobile Assistive Application for Blind People<br>in Indoor Navigation","x":-9.0336332321,"y":42.2778053284,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.0336332321,"tsne_y":42.2778053284,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"j26thar0","source_x":"PMC","title":"Object Detector Combination for Increasing Accuracy and Detecting More Overlapping Objects","doi":"10.1007\/978-3-030-51935-3_31","abstract":"Object detection is considered as the cornerstone of many modern applications such as Drone vision and Self-driven cars. Object detectors, mainly those which are based on Convolutional Neural Net-works (CNNs) have received great attention from many researchers because they were able to yield remarkable results. However, most of them fail when it comes to detecting overlapping and small objects in images. There are two families of detectors: the first family detects more objects but with imprecise bounding boxes, while those of the second family do the opposite. In this paper, we propose a solution to this problem by combining the two families, in a way similar to classifier combination. Our solution has been validated through the combination of two famous detectors, Faster R-CNN which detects more objects and YOLO which produces accurate bounding boxes. However, it is more general and it can be applied to other detectors. The evaluation of our method has been applied to the PASCAL VOC dataset and it gave promising results.","publish_time":1591315200000,"author_summary":" Drid, Khaoula; Allaoui, Mebarka; Kherfi,<br>Mohammed Lamine","abstract_summary":" Object detection is considered as the<br>cornerstone of many modern applications such as Drone<br>vision and Self-driven cars. Object detectors,<br>mainly those which are based on Convolutional Neural<br>Net-works (CNNs) have received great attention from many<br>researchers because they were able to yield remarkable<br>results. However, most of them fail when it comes to<br>detecting overlapping and small objects in images. There<br>are two families of detectors: the first family<br>detects more objects but with imprecise bounding<br>boxes, while those of the second family do the<br>opposite. In this paper, we propose a solution to this<br>problem by combining the...","title_summary":" Object Detector Combination for Increasing<br>Accuracy and Detecting More Overlapping Objects","x":-9.1984615326,"y":41.5982170105,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.1984615326,"tsne_y":41.5982170105,"subcluster":6,"subcluster_description":"Educational Image Classificationrobust 3D","shape":"p"},{"cord_uid":"73sonemg","source_x":"PMC","title":"Image-Based Place Recognition Using Semantic Segmentation and Inpainting to Remove Dynamic Objects","doi":"10.1007\/978-3-030-51935-3_28","abstract":"Place recognition is an important step in intelligent driving, allowing the vehicle recognize where it is to plan its route. Obtaining distinguishable features can ensure the success of image-based place recognition. However, generating robust features across drastically appearance changing images is still a challenging problem. Deep features are frequently chosen instead of local features in the tasks of place recognition following the development of convolutional neural networks. But even the deep features generated by powerful neural models can cause unsatisfactory recognition results. This is perhaps due to a lack of information selecting process. The technology of semantic segmentation allows recognizing and classifying image information. Semantic segmentation followed by image inpainting provide a possibility of detecting, deleting and reconstructing annoying information. This paper proves that dynamic information present in images such as vehicles and pedestrians damages the performance of place recognition and proposes a feature extraction system that includes a step to decrease the presence of dynamic information of an image. This system is composed of two stages: 1) dynamic objects detection and removing, 2) image inpainting to reconstruct the background of removed regions. Objects detection and removing consists of deleting unstable objects recognized by semantic segmentation method from images. Image inpainting and reconstructing deals with generating inpaint-images by repairing missing regions through image inpainting method. The robustness of the proposed approach is evaluated by comparing to the non-selecting deep feature based place recognition approaches over three datasets.","publish_time":1591315200000,"author_summary":" Liu, Linrunjia; Cappelle, Cindy; Ruichek,<br>Yassine","abstract_summary":" Place recognition is an important step in<br>intelligent driving, allowing the vehicle recognize where<br>it is to plan its route. Obtaining<br>distinguishable features can ensure the success of image-based<br>place recognition. However, generating robust<br>features across drastically appearance changing<br>images is still a challenging problem. Deep features<br>are frequently chosen instead of local features in<br>the tasks of place recognition following the<br>development of convolutional neural networks. But even the<br>deep features generated by powerful neural models<br>can cause unsatisfactory recognition results.<br>This is perhaps due to a lack of information<br>selecting process. The technology of semantic<br>segmentation allows recognizing...","title_summary":" Image-Based Place Recognition Using Semantic<br>Segmentation and Inpainting to Remove Dynamic Objects","x":-9.3173675537,"y":41.8327674866,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3173675537,"tsne_y":41.8327674866,"subcluster":7,"subcluster_description":"Cnnan Efficient Image Descriptor","shape":"p"},{"cord_uid":"kzuhfx7f","source_x":"PMC","title":"Multispectral Dynamic Codebook and Fusion Strategy for Moving Objects Detection","doi":"10.1007\/978-3-030-51935-3_4","abstract":"The Codebook model is one of the popular real-time models for background subtraction to detect moving objects. In this paper, we propose two techniques to adapt the original Codebook algorithm to multispectral images: dynamic mechanism and fusion strategy. For each channel, only absolute spectral value is used to calculate the spectral similarity between the current frame pixel and reference average value in the matching process, which can simplify the matching equations. Besides, the deciding boundaries are obtained based on statistical information extracted from the data and always adjusting themselves to the scene changes. Results demonstrate that with the proposed techniques, we can acquire a comparable accuracy with other methods using the same multispectral dataset for background subtraction.","publish_time":1591315200000,"author_summary":" Liu, Rongrong; Ruichek, Yassine; El Bagdouri,<br>Mohammed","abstract_summary":" The Codebook model is one of the popular<br>real-time models for background subtraction to detect<br>moving objects. In this paper, we propose two<br>techniques to adapt the original Codebook algorithm to<br>multispectral images: dynamic mechanism and fusion<br>strategy. For each channel, only absolute spectral value<br>is used to calculate the spectral similarity<br>between the current frame pixel and reference average<br>value in the matching process, which can simplify the<br>matching equations. Besides, the deciding boundaries<br>are obtained based on statistical information<br>extracted from the data and always adjusting themselves<br>to the scene changes. Results demonstrate that<br>with the proposed techniques,...","title_summary":" Multispectral Dynamic Codebook and Fusion<br>Strategy for Moving Objects Detection","x":-9.7023334503,"y":42.4397087097,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.7023334503,"tsne_y":42.4397087097,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"n8zgwxmc","source_x":"PMC","title":"A Bottom-Up Approach for Pig Skeleton Extraction Using RGB Data","doi":"10.1007\/978-3-030-51935-3_6","abstract":"Animal behavior analysis is a crucial task for the industrial farming. In an indoor farm setting, extracting Key joints of animals is essential for tracking the animal for a longer period of time. In this paper, we proposed a deep network that exploits transfer learning to train the network for the pig skeleton extraction in an end to end fashion. The backbone of the architecture is based on an hourglass stacked dense-net. In order to train the network, keyframes are selected from the test data using K-mean sampler. In total, 9 Keypoints are annotated that gives a brief detailed behavior analysis in the farm setting. Extensive experiments are conducted and the quantitative results show that the network has the potential of increasing the tracking performance by a substantial margin.","publish_time":1591315200000,"author_summary":" Quddus Khan, Akif; Khan, Salman; Ullah, Mohib;<br>Cheikh, Faouzi Alaya","abstract_summary":" Animal behavior analysis is a crucial task for<br>the industrial farming. In an indoor farm setting,<br>extracting Key joints of animals is essential for tracking<br>the animal for a longer period of time. In this<br>paper, we proposed a deep network that exploits<br>transfer learning to train the network for the pig<br>skeleton extraction in an end to end fashion. The<br>backbone of the architecture is based on an hourglass<br>stacked dense-net. In order to train the network,<br>keyframes are selected from the test data using K-mean<br>sampler. In total, 9 Keypoints are annotated that gives a<br>brief detailed behavior...","title_summary":" A Bottom-Up Approach for Pig Skeleton<br>Extraction Using RGB Data","x":-8.7278738022,"y":41.2721290588,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.7278738022,"tsne_y":41.2721290588,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"0rq5xrlf","source_x":"PMC","title":"Speech Enhancement Based on Deep AutoEncoder for Remote Arabic Speech Recognition","doi":"10.1007\/978-3-030-51935-3_24","abstract":"Remote applications that deal with speech need the speech signal to be compressed. First, speech coding transforms the continuous waveform into a numerical form. Then, the digitized signal is compressed with or without loss of information. This transformation affects the original waveform and degrades performances for further recognition of the speech signal. Meanwhile, the transmission is another source of speech degradation. To restore the original \u201cclean\u201d speech, speech enhancement (SE) is widely used, and deep learning algorithms are state-of-the-art, nowadays. In this paper, the target application is a remote Arabic speech recognition system, and the aim of using SE is to improve the accuracy of the speech recognizer. For that purpose, a Deep Auto Encoder (DAE) is used. The effect of the DAE-based SE is studied through different configurations, and the performances are evaluated through accuracy. The results showed an improvement of about 3.17 between the accuracy prior to the SE and that computed with the enhanced speech.","publish_time":1591315200000,"author_summary":" Dendani, Bilal; Bahi, Halima; Sari, Toufik","abstract_summary":" Remote applications that deal with speech need<br>the speech signal to be compressed. First, speech<br>coding transforms the continuous waveform into a<br>numerical form. Then, the digitized signal is compressed<br>with or without loss of information. This<br>transformation affects the original waveform and degrades<br>performances for further recognition of the speech signal.<br>Meanwhile, the transmission is another source of speech<br>degradation. To restore the original \u201cclean\u201d speech, speech<br>enhancement (SE) is widely used, and deep learning<br>algorithms are state-of-the-art, nowadays. In this<br>paper, the target application is a remote Arabic<br>speech recognition system, and the aim of using SE is...","title_summary":" Speech Enhancement Based on Deep AutoEncoder<br>for Remote Arabic Speech Recognition","x":-11.3563814163,"y":40.7244644165,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.3563814163,"tsne_y":40.7244644165,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"vi0flxhx","source_x":"PMC","title":"Proposed Integration Algorithm to Optimize the Separation of Audio Signals Using the ICA and Wavelet Transform","doi":"10.1007\/978-3-030-51935-3_39","abstract":"In the present work, an integration of two combined methodologies is developed for the blind separation of mixed audio signals. The mathematical methodologies are the independent component analysis (ICA) and the discrete Wavelet transform (DWT). The DWT optimizes processing time by decreasing the amount of data, before that signals are processed by ICA. A traditional methodology for signal processing such as Wavelet is combined with a statistical process as ICA, which assumes that the source signals are mixed and they are statistically independent of each other. The problem refers to very common situations where the human being listens to several sound sources at the same time. The human brain being able to pay attention to the message of a particular signal. The results are very satisfactory, effectively achieving signal separation, where only a small background noise and a attenuation in the amplitude of the recovered signal are noticed, but that nevertheless the signal message is identified in such a way.","publish_time":1591315200000,"author_summary":" San Juan, Enrique; Dehghan Firoozabadi, Ali;<br>Soto, Ismael; Adasme, Pablo; Ca\u00f1ete, Lucio","abstract_summary":" In the present work, an integration of two<br>combined methodologies is developed for the blind<br>separation of mixed audio signals. The mathematical<br>methodologies are the independent component analysis (ICA)<br>and the discrete Wavelet transform (DWT). The DWT<br>optimizes processing time by decreasing the amount of<br>data, before that signals are processed by ICA. A<br>traditional methodology for signal processing such as<br>Wavelet is combined with a statistical process as ICA,<br>which assumes that the source signals are mixed and<br>they are statistically independent of each other.<br>The problem refers to very common situations where<br>the human being listens to several...","title_summary":" Proposed Integration Algorithm to Optimize<br>the Separation of Audio Signals Using the ICA and<br>Wavelet Transform","x":-11.6354627609,"y":41.2630271912,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.6354627609,"tsne_y":41.2630271912,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"qxpvpoz7","source_x":"PMC","title":"Towards the Tactile Discovery of Cultural Heritage with Multi-approach Segmentation","doi":"10.1007\/978-3-030-51935-3_2","abstract":"This paper presents a new way to access visual information in museums through tactile exploration, and related techniques to efficiently transform visual data into tactile objects. Accessibility to cultural heritage and artworks for people with visual impairments requires the segmentation of images and paintings to extract and classify their contents into meaningful elements which can then be presented through a tactile medium. In this paper, we investigate the feasibility and how to optimize the tactile discovery of an image. First, we study the emergence of image comprehension through tactile discovery, using 3D-printed objects extracted from paintings. Later, we present a dynamic Force Feedback Tablet (F2T) used to convey the 2D shape and texture information of objects through haptic feedback. We then explore several image segmentation methods to automate the extraction of meaningful objects from selected artworks, to be presented to visually impaired people through the F2T. Finally, we evaluate how to best combine the F2T\u2019s haptic effects in order to convey the extracted objects and features to the users, with the aim of facilitating the comprehension of the represented objects and their affordances.","publish_time":1591315200000,"author_summary":" Souradi, Ali; Lecomte, Christele; Romeo,<br>Katerine; Gay, Simon; Riviere, Marc-Aurele; El Moataz,<br>Abderrahim; Pissaloux, Edwige","abstract_summary":" This paper presents a new way to access visual<br>information in museums through tactile exploration, and<br>related techniques to efficiently transform visual<br>data into tactile objects. Accessibility to<br>cultural heritage and artworks for people with visual<br>impairments requires the segmentation of images and<br>paintings to extract and classify their contents into<br>meaningful elements which can then be presented through a<br>tactile medium. In this paper, we investigate the<br>feasibility and how to optimize the tactile discovery of an<br>image. First, we study the emergence of image<br>comprehension through tactile discovery, using 3D-printed<br>objects extracted from paintings. Later, we present a...","title_summary":" Towards the Tactile Discovery of Cultural<br>Heritage with Multi-approach Segmentation","x":-9.2556266785,"y":43.7170600891,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2556266785,"tsne_y":43.7170600891,"subcluster":0,"subcluster_description":"Individual Mangrove Tree Modelingcombined","shape":"p"},{"cord_uid":"l5ahhs0c","source_x":"PMC","title":"Logo Detection Based on FCM Clustering Algorithm and Texture Features","doi":"10.1007\/978-3-030-51935-3_35","abstract":"Logo detection methods usually depend on logo shapes and need for training data or a-priori information on the processed images. This limits their effectiveness to real-world applications. In this paper, we tackle these challenges by exploring the textural information. Specifically we propose a novel approach for administrative logo detection based on a fuzzy classification with a multi-fractal texture feature, capable of automatically characterizing texture measures describing logo and non-logo regions. Experimental results, using two real datasets, confirm the feasibility of the proposed method for degraded administrative documents. Extensive comparative evaluations demonstrate the superiority of this approach over the state-of-the-art methods.","publish_time":1591315200000,"author_summary":" Zaaboub, Wala; Tlig, Lotfi; Sayadi, Mounir;<br>Solaiman, Basel","abstract_summary":" Logo detection methods usually depend on logo<br>shapes and need for training data or a-priori<br>information on the processed images. This limits their<br>effectiveness to real-world applications. In this paper, we<br>tackle these challenges by exploring the textural<br>information. Specifically we propose a novel approach for<br>administrative logo detection based on a fuzzy classification<br>with a multi-fractal texture feature, capable of<br>automatically characterizing texture measures describing<br>logo and non-logo regions. Experimental results,<br>using two real datasets, confirm the feasibility of<br>the proposed method for degraded administrative<br>documents. Extensive comparative evaluations<br>demonstrate the superiority of this approach over the<br>state-of-the-art methods.","title_summary":" Logo Detection Based on FCM Clustering<br>Algorithm and Texture Features","x":-9.6888380051,"y":42.5196037292,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.6888380051,"tsne_y":42.5196037292,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"v3eip7m5","source_x":"PMC","title":"Multistage Deep Neural Network Framework for People Detection and Localization Using Fusion of Visible and Thermal Images","doi":"10.1007\/978-3-030-51935-3_15","abstract":"In Computer vision object detection and classification are active fields of research. Applications of object detection and classification include a diverse range of fields such as surveillance, autonomous cars and robotic vision. Many intelligent systems are built by researchers to achieve the accuracy of human perception but could not quite achieve it yet. Convolutional Neural Networks (CNN) and Deep Learning architectures are used to achieve human like perception for object detection and scene identification. We are proposing a novel method by combining previously used techniques. We are proposing a model which takes multi-spectral images, fuses them together, drops the useless images and then provides semantic segmentation for each object (person) present in the image. In our proposed methodology we are using CNN for fusion of Visible and thermal images and Deep Learning architectures for classification and localization. Fusion of visible and thermal images is carried out to combine informative features of both images into one image. For fusion we are using Encoder-decoder architecture. Fused image is then fed into Resnet-152 architecture for classification of images. Images obtained from Resnet-152 are then fed into Mask-RCNN for localization of persons. Mask-RCNN uses Resnet-101 architecture for localization of objects. From the results it can be clearly seen that Fused model for object localization outperforms the Visible model and gives promising results for person detection for surveillance purposes. Our proposed model gives the Miss Rate of 5.25% which is much better than the previous state of the art method applied on KAIST dataset.","publish_time":1591315200000,"author_summary":" Khalid, Bushra; Akram, Muhammad Usman; Khan,<br>Asad Mansoor","abstract_summary":" In Computer vision object detection and<br>classification are active fields of research. Applications of<br>object detection and classification include a<br>diverse range of fields such as surveillance,<br>autonomous cars and robotic vision. Many intelligent<br>systems are built by researchers to achieve the<br>accuracy of human perception but could not quite achieve<br>it yet. Convolutional Neural Networks (CNN) and<br>Deep Learning architectures are used to achieve<br>human like perception for object detection and scene<br>identification. We are proposing a novel method by combining<br>previously used techniques. We are proposing a model which<br>takes multi-spectral images, fuses them together,<br>drops the useless...","title_summary":" Multistage Deep Neural Network Framework for<br>People Detection and Localization Using Fusion of<br>Visible and Thermal Images","x":-9.2370615005,"y":41.4498443604,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2370615005,"tsne_y":41.4498443604,"subcluster":6,"subcluster_description":"Educational Image Classificationrobust 3D","shape":"p"},{"cord_uid":"6a4jfz94","source_x":"PMC","title":"CNN-SVM Learning Approach Based Human Activity Recognition","doi":"10.1007\/978-3-030-51935-3_29","abstract":"Although it has been encountered for a long time, the human activity recognition remains a big challenge to tackle. Recently, several deep learning approaches have been proposed to enhance the recognition performance with different areas of application. In this paper, we aim to combine a recent deep learning-based method and a traditional classifier based hand-crafted feature extractors in order to replace the artisanal feature extraction method with a new one. To this end, we used a deep convolutional neural network that offers the possibility of having more powerful extracted features from sequence video frames. The resulting feature vector is then fed as an input to the support vector machine (SVM) classifier to assign each instance to the corresponding label and bythere, recognize the performed activity. The proposed architecture was trained and evaluated on MSR Daily activity 3D dataset. Compared to state of art methods, our proposed technique proves that it has performed better.","publish_time":1591315200000,"author_summary":" Basly, Hend; Ouarda, Wael; Sayadi, Fatma<br>Ezahra; Ouni, Bouraoui; Alimi, Adel M.","abstract_summary":" Although it has been encountered for a long<br>time, the human activity recognition remains a big<br>challenge to tackle. Recently, several deep learning<br>approaches have been proposed to enhance the recognition<br>performance with different areas of application. In this<br>paper, we aim to combine a recent deep learning-based<br>method and a traditional classifier based<br>hand-crafted feature extractors in order to replace the<br>artisanal feature extraction method with a new one. To<br>this end, we used a deep convolutional neural<br>network that offers the possibility of having more<br>powerful extracted features from sequence video<br>frames. The resulting feature vector is then...","title_summary":" CNN-SVM Learning Approach Based Human<br>Activity Recognition","x":-9.5137214661,"y":40.7849845886,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.5137214661,"tsne_y":40.7849845886,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"2lnym735","source_x":"PMC","title":"Extraction and Recognition of Bangla Texts from Natural Scene Images Using CNN","doi":"10.1007\/978-3-030-51935-3_26","abstract":"The semantic information presents in the scene images may be the useful information for the viewers who is searching for a specific location or any specific shop and address. This type of information can also be useful in licenseplate detection, controlling the vehicle on the road, robot navigation, and assisting visually impaired persons. An efficient method is presented in this paper to detect and extract Bangla texts from scene images based on a connected component approach along with rule-based filtering and vertical scanning scheme. Next, extracted characters are recognized by using Convolutional Neural Network (CNN). The method consists of the four basic consecutive steps such as detection and extraction of the Region of Interest (ROI), segmentation of the words, extraction of characters, and recognition of the extracted characters. After extracting the ROI from the input image, connected component(CC) analysis and bounding box technology are used for segmentation of Bangla words. To separate and extract Bangla characters from the segmented Bangla words, vertical scanning based method along with a dynamic threshold value has been applied. Finally, character recognition is carried out using CNN. The proposed algorithm is applied to 600 scene images of different writing styles and colors, and we have obtained 89.25% accuracy in text detection and 94.50% accuracy in the extraction of characters. We have achieved an accuracy of 99.30% and 95.76% in recognition of Bangla digits and characters respectively. By combining both the digits and characters, obtained recognition accuracy is 95.39%.","publish_time":1591315200000,"author_summary":" Islam, Rashedul; Islam, Md Rafiqul; Talukder,<br>Kamrul Hasan","abstract_summary":" The semantic information presents in the scene<br>images may be the useful information for the viewers<br>who is searching for a specific location or any<br>specific shop and address. This type of information can<br>also be useful in licenseplate detection,<br>controlling the vehicle on the road, robot navigation, and<br>assisting visually impaired persons. An efficient<br>method is presented in this paper to detect and extract<br>Bangla texts from scene images based on a connected<br>component approach along with rule-based filtering and<br>vertical scanning scheme. Next, extracted characters<br>are recognized by using Convolutional Neural<br>Network (CNN). The method consists of the...","title_summary":" Extraction and Recognition of Bangla Texts<br>from Natural Scene Images Using CNN","x":-9.2752399445,"y":41.9873466492,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2752399445,"tsne_y":41.9873466492,"subcluster":7,"subcluster_description":"Cnnan Efficient Image Descriptor","shape":"p"},{"cord_uid":"it234nvq","source_x":"PMC","title":"A Deep CNN-LSTM Framework for Fast Video Coding","doi":"10.1007\/978-3-030-51935-3_22","abstract":"High Efficiency Video Coding (HEVC) doubles the compression rates over the previous H.264 standard for the same video quality. To improve the coding efficiency, HEVC adopts the hierarchical quadtree structured Coding Unit (CU). However, the computational complexity significantly increases due to the full search for Rate-Distortion Optimization (RDO) to find the optimal Coding Tree Unit (CTU) partition. Here, this paper proposes a deep learning model to predict the HEVC CU partition at inter-mode, instead of brute-force RDO search. To learn the learning model, a large-scale database for HEVC inter-mode is first built. Second, to predict the CU partition of HEVC, we propose as a model a combination of a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) network. The simulation results prove that the proposed scheme can achieve a best compromise between complexity reduction and RD performance, compared to existing approaches.","publish_time":1591315200000,"author_summary":" Bouaafia, Soulef; Khemiri, Randa; Sayadi,<br>Fatma Ezahra; Atri, Mohamed; Liouane, Noureddine","abstract_summary":" High Efficiency Video Coding (HEVC) doubles<br>the compression rates over the previous H.264<br>standard for the same video quality. To improve the<br>coding efficiency, HEVC adopts the hierarchical<br>quadtree structured Coding Unit (CU). However, the<br>computational complexity significantly increases due to the<br>full search for Rate-Distortion Optimization<br>(RDO) to find the optimal Coding Tree Unit (CTU)<br>partition. Here, this paper proposes a deep learning model<br>to predict the HEVC CU partition at inter-mode,<br>instead of brute-force RDO search. To learn the<br>learning model, a large-scale database for HEVC<br>inter-mode is first built. Second, to predict the CU<br>partition of HEVC,...","title_summary":" A Deep CNN-LSTM Framework for Fast Video Coding","x":-10.9994544983,"y":40.7475471497,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.9994544983,"tsne_y":40.7475471497,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"zlj2al6d","source_x":"PMC","title":"Convolutional Neural Networks Backbones for Object Detection","doi":"10.1007\/978-3-030-51935-3_30","abstract":"Detecting objects in images is an extremely important step in many image and video analysis applications. Object detection is considered as one of the main challenges in the field of computer vision, which focuses on identifying and locating objects of different classes in an image. In this paper, we aim to highlight the important role of deep learning and convolutional neural networks in particular in the object detection task. We analyze and focus on the various state-of-the-art convolutional neural networks serving as a backbone in object detection models. We test and evaluate them in the common datasets and benchmarks up-to-date. We Also outline the main features of each architecture. We demonstrate that the application of some convolutional neural network architectures has yielded very promising state-of-the-art results in image classification in the first place and then in the object detection task. The results have surpassed all the traditional methods, and in some cases, outperformed the human being\u2019s performance.","publish_time":1591315200000,"author_summary":" Benali Amjoud, Ayoub; Amrouch, Mustapha","abstract_summary":" Detecting objects in images is an extremely<br>important step in many image and video analysis<br>applications. Object detection is considered as one of the<br>main challenges in the field of computer vision,<br>which focuses on identifying and locating objects of<br>different classes in an image. In this paper, we aim to<br>highlight the important role of deep learning and<br>convolutional neural networks in particular in the object<br>detection task. We analyze and focus on the various<br>state-of-the-art convolutional neural networks serving as a<br>backbone in object detection models. We test and<br>evaluate them in the common datasets and benchmarks<br>up-to-date....","title_summary":" Convolutional Neural Networks Backbones for<br>Object Detection","x":-9.3091411591,"y":41.4246940613,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3091411591,"tsne_y":41.4246940613,"subcluster":6,"subcluster_description":"Educational Image Classificationrobust 3D","shape":"p"},{"cord_uid":"yloyz5ym","source_x":"PMC","title":"Handwriting Based Gender Classification Using COLD and Hinge Features","doi":"10.1007\/978-3-030-51935-3_25","abstract":"Gender Classification from handwriting is still considered to be challenging due to homogeneous vision comparing male and female handwritten documents. This paper presents a new method based on Cloud of Line Distribution (COLD) and Hinge feature for distinguishing the gender from handwriting. The SVM classifier combination decides the assigned class based on the maximum of the two decisions values resulting from COLD and Hinge feature. The proposed approach is evaluated on the standard QUWI dataset and following the framework protocol described in the ICFHR 2016 competition. Obtained results are promising regarding the classification rates announced in the literature.","publish_time":1591315200000,"author_summary":" Gattal, Abdeljalil; Djeddi, Chawki;<br>Bensefia, Ameur; Ennaji, Abdellatif","abstract_summary":" Gender Classification from handwriting is<br>still considered to be challenging due to<br>homogeneous vision comparing male and female handwritten<br>documents. This paper presents a new method based on Cloud<br>of Line Distribution (COLD) and Hinge feature for<br>distinguishing the gender from handwriting. The SVM<br>classifier combination decides the assigned class based<br>on the maximum of the two decisions values<br>resulting from COLD and Hinge feature. The proposed<br>approach is evaluated on the standard QUWI dataset and<br>following the framework protocol described in the ICFHR<br>2016 competition. Obtained results are promising<br>regarding the classification rates announced in the<br>literature.","title_summary":" Handwriting Based Gender Classification<br>Using COLD and Hinge Features","x":-9.2600545883,"y":41.1200065613,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2600545883,"tsne_y":41.1200065613,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"rpmlod07","source_x":"PMC","title":"A New Texture Descriptor: The Homogeneous Local Binary Pattern (HLBP)","doi":"10.1007\/978-3-030-51935-3_33","abstract":"This paper presents a simple and novel descriptor named Homogeneous Local Binary Pattern (HLBP) for texture analysis. The purpose of this description is to improve the Local Binary Pattern (LBP) approach basing on the impact of criterion homogeneous region using General Adaptive Neighborhood (GAN) principle. HLBP method is generated by using the criterion homogeneity which helps to represent a significant feature based on relationships between neighboring pixels. The main idea of HLBP is to threshold the distance between the current pixel and each of its neighbors with a homogeneity tolerance value which correspond more to the underlying spatial structures consequently allow extracting highly distinctive invariant features of the image. To assess the performance of the our proposed descriptor, we use \u201cOutex\" database and compared with the basic (LBPs). The experimental results show that the proposed Homogeneous Local Binary Pattern gives a good performance in term of classification accuracy.","publish_time":1591315200000,"author_summary":" Al Saidi, Ibtissam; Rziza, Mohammed; Debayle,<br>Johan","abstract_summary":" This paper presents a simple and novel<br>descriptor named Homogeneous Local Binary Pattern (HLBP)<br>for texture analysis. The purpose of this<br>description is to improve the Local Binary Pattern (LBP)<br>approach basing on the impact of criterion homogeneous<br>region using General Adaptive Neighborhood (GAN)<br>principle. HLBP method is generated by using the criterion<br>homogeneity which helps to represent a significant feature<br>based on relationships between neighboring pixels.<br>The main idea of HLBP is to threshold the distance<br>between the current pixel and each of its neighbors with<br>a homogeneity tolerance value which correspond<br>more to the underlying spatial structures<br>consequently...","title_summary":" A New Texture Descriptor: The Homogeneous<br>Local Binary Pattern (HLBP)","x":-9.6251335144,"y":42.1528511047,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.6251335144,"tsne_y":42.1528511047,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"jhy486g6","source_x":"PMC","title":"Detection of Elliptical Traffic Signs","doi":"10.1007\/978-3-030-51935-3_27","abstract":"Detection of elliptical features is a challenging and important task in computer vision. In fact, ellipses can describe many objects in real images like manufactured objects, cells, ball, or traffic signs. Furthermore, an ellipse is defined by five parameters: the center coordinates, the semi major axe, the semi minor axe and the orientation, which require more computational power to estimate them. Some non-ideal ellipses cause also difficulties for detection such as occlusion, appearance of multiple ellipses at same time and non-parallel ellipse. In this paper, we are interested in detecting elliptical traffic signs. We present a method for detecting different cases of ellipses: simple, partially occluded, non-parallel, multiple ellipses in images. The method selects three lines to find the ellipse center, and then it calculates the value of the semi minor and major axes of the ellipse. Experiments show that the proposed method performs well on real images in the presence or not of noises.","publish_time":1591315200000,"author_summary":" El Baz, Manal; Zaki, Taher; Douzi, Hassan","abstract_summary":" Detection of elliptical features is a<br>challenging and important task in computer vision. In fact,<br>ellipses can describe many objects in real images like<br>manufactured objects, cells, ball, or traffic signs.<br>Furthermore, an ellipse is defined by five parameters: the<br>center coordinates, the semi major axe, the semi minor<br>axe and the orientation, which require more<br>computational power to estimate them. Some non-ideal<br>ellipses cause also difficulties for detection such as<br>occlusion, appearance of multiple ellipses at same time<br>and non-parallel ellipse. In this paper, we are<br>interested in detecting elliptical traffic signs. We<br>present a method for detecting different...","title_summary":" Detection of Elliptical Traffic Signs","x":-9.3752565384,"y":42.2794570923,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3752565384,"tsne_y":42.2794570923,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"eif05t1o","source_x":"PMC","title":"A Spectral Hazy Image Database","doi":"10.1007\/978-3-030-51935-3_5","abstract":"We introduce a new database to promote visibility enhancement techniques intended for spectral image dehazing. SHIA (Spectral Hazy Image database for Assessment) is composed of two real indoor scenes M1 and M2 of 10 levels of fog each and their corresponding fog-free (ground-truth) images, taken in the visible and the near infrared ranges every 10 nm starting from 450 to 1000 nm. The number of images that form SHIA is 1540 with a size of [Formula: see text] pixels. All images are captured under the same illumination conditions. Three of the well-known dehazing image methods based on different approaches were adjusted and applied on the spectral foggy images. This study confirms once again a strong dependency between dehazing methods and fog densities. It urges the design of spectral-based image dehazing able to handle simultaneously the accurate estimation of the parameters of the visibility degradation model and the limitation of artifacts and post-dehazing noise. The database can be downloaded freely at http:\/\/chic.u-bourgogne.fr.","publish_time":1591315200000,"author_summary":" El Khoury, Jessica; Thomas, Jean-Baptiste;<br>Mansouri, Alamin","abstract_summary":" We introduce a new database to promote<br>visibility enhancement techniques intended for spectral<br>image dehazing. SHIA (Spectral Hazy Image database<br>for Assessment) is composed of two real indoor<br>scenes M1 and M2 of 10 levels of fog each and their<br>corresponding fog-free (ground-truth) images, taken in the<br>visible and the near infrared ranges every 10 nm<br>starting from 450 to 1000 nm. The number of images that<br>form SHIA is 1540 with a size of [Formula: see text]<br>pixels. All images are captured under the same<br>illumination conditions. Three of the well-known dehazing<br>image methods based on different approaches were...","title_summary":" A Spectral Hazy Image Database","x":-9.2268743515,"y":42.6911201477,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.2268743515,"tsne_y":42.6911201477,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"mqtw9x3i","source_x":"PMC","title":"Application of a Novel and Improved VGG-19 Network in the Detection of Workers Wearing Masks","doi":"10.1088\/1742-6596\/1518\/1\/012041","abstract":"In order to work and travel safely during the outbreak of COVID-19, a method of security detection based on deep learning is proposed by using machine vision instead of manual monitoring. To detect the illegal behaviors of workers without masks in workplaces and densely populated areas, an improved convolutional neural network VGG-19 algorithm is proposed under the framework of tensorflow, and more than 3000 images are collected for model training and testing. Using VGG-19 network model, three FC layers are optimized into one flat layer and two FC layers with reduced parameters. The softmax classification layer of the original model is replaced by a 2-label softmax classifier. The experimental results show that the precision of the model is 97.62% and the recall is 96.31%. The precision of identifying the workers without masks is 96.82%, the recall is 94.07%, and the data set provided has a high precision. For the future social health and safety to provide favorable test data.","publish_time":1586822400000,"author_summary":" Xiao, Jian; Wang, Jia; Cao, Shaozhong; Li,<br>Bilong","abstract_summary":" In order to work and travel safely during the<br>outbreak of COVID-19, a method of security detection<br>based on deep learning is proposed by using machine<br>vision instead of manual monitoring. To detect the<br>illegal behaviors of workers without masks in<br>workplaces and densely populated areas, an improved<br>convolutional neural network VGG-19 algorithm is proposed<br>under the framework of tensorflow, and more than 3000<br>images are collected for model training and testing.<br>Using VGG-19 network model, three FC layers are<br>optimized into one flat layer and two FC layers with<br>reduced parameters. The softmax classification layer<br>of the original model...","title_summary":" Application of a Novel and Improved VGG-19<br>Network in the Detection of Workers Wearing Masks","x":-8.3456325531,"y":40.9967041016,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.3456325531,"tsne_y":40.9967041016,"subcluster":4,"subcluster_description":"Manufacturing Defects","shape":"p"},{"cord_uid":"ag3kck6f","source_x":"PMC","title":"PCA Based Kernel Initialization for Convolutional Neural Networks","doi":"10.1007\/978-981-15-7205-0_7","abstract":"The initialization of Convolutional Neural Networks (CNNs) is about providing reasonable initial values for the convolution kernels and the fully connected layers. In this paper, we proposed a convolution kernel initialization method based on the two-dimensional principal component analysis (2DPCA), in which a parametric equalization normalization method is used to adjust the scale between each neuron weight. After that the weight initial value can be adaptively adjusted according to different data samples. This method enables each neuron to fully back-propagate errors and accelerate network model training. Finally, a network model was built and experiments were performed using Tanh and ReLU activation functions. The experimental results verify the effectiveness of the proposed method through the distribution of histograms and the curve comparison diagrams of model training.","publish_time":1594425600000,"author_summary":" Wang, Yifeng; Rong, Yuxi; Pan, Hongyue; Liu,<br>Ke; Hu, Yang; Wu, Fangmin; Peng, Wei; Xue, Xingsi;<br>Chen, Junfeng","abstract_summary":" The initialization of Convolutional Neural<br>Networks (CNNs) is about providing reasonable initial<br>values for the convolution kernels and the fully<br>connected layers. In this paper, we proposed a<br>convolution kernel initialization method based on the<br>two-dimensional principal component analysis (2DPCA), in<br>which a parametric equalization normalization<br>method is used to adjust the scale between each neuron<br>weight. After that the weight initial value can be<br>adaptively adjusted according to different data samples.<br>This method enables each neuron to fully<br>back-propagate errors and accelerate network model training.<br>Finally, a network model was built and experiments were<br>performed using Tanh and ReLU...","title_summary":" PCA Based Kernel Initialization for<br>Convolutional Neural Networks","x":-10.3289585114,"y":41.1963882446,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.3289585114,"tsne_y":41.1963882446,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"1x03ui2p","source_x":"PMC","title":"Multi-objective Combinatorial Generative Adversarial Optimization and Its Application in Crowdsensing","doi":"10.1007\/978-3-030-53956-6_38","abstract":"With the increasing of the decision variables in multi-objective combinatorial optimization problems, the traditional evolutionary algorithms perform worse due to the low efficiency for generating the offspring by a stochastic mechanism. To address the issue, a multi-objective combinatorial generative adversarial optimization method is proposed to make the algorithm capable of learning the implicit information embodied in the evolution process. After classifying the optimal non-dominated solutions in the current generation as real data, the generative adversarial network (GAN) is trained by them, with the purpose of learning their distribution information. The Adam algorithm that employs the adaptively learning rate for each parameter is introduced to update the main parameters of GAN. Following that, an offspring reproduction strategy is designed to form a new feasible solution from the decimal output of the generator. To further verify the rationality of the proposed method, it is applied to solve the participant selection problem of the crowdsensing and the detailed offspring reproduction strategy is given. The experimental results for the crowdsensing systems with various tasks and participants show that the proposed algorithm outperforms the others in both convergence and distribution.","publish_time":1592784000000,"author_summary":" Guo, Yi-nan; Ji, Jianjiao; Tan, Ying; Cheng,<br>Shi","abstract_summary":" With the increasing of the decision variables<br>in multi-objective combinatorial optimization<br>problems, the traditional evolutionary algorithms<br>perform worse due to the low efficiency for generating<br>the offspring by a stochastic mechanism. To<br>address the issue, a multi-objective combinatorial<br>generative adversarial optimization method is proposed<br>to make the algorithm capable of learning the<br>implicit information embodied in the evolution<br>process. After classifying the optimal non-dominated<br>solutions in the current generation as real data, the<br>generative adversarial network (GAN) is trained by them,<br>with the purpose of learning their distribution<br>information. The Adam algorithm that employs the adaptively<br>learning rate for each...","title_summary":" Multi-objective Combinatorial Generative<br>Adversarial Optimization and Its Application in<br>Crowdsensing","x":-11.9989395142,"y":40.199546814,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.9989395142,"tsne_y":40.199546814,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"n68ddpas","source_x":"Medline","title":"A Human Support Robot for the Cleaning and Maintenance of Door Handles Using a Deep-Learning Framework.","doi":"10.3390\/s20123543","abstract":"The role of mobile robots for cleaning and sanitation purposes is increasing worldwide. Disinfection and hygiene are two integral parts of any safe indoor environment, and these factors become more critical in COVID-19-like pandemic situations. Door handles are highly sensitive contact points that are prone to be contamination. Automation of the door-handle cleaning task is not only important for ensuring safety, but also to improve efficiency. This work proposes an AI-enabled framework for automating cleaning tasks through a Human Support Robot (HSR). The overall cleaning process involves mobile base motion, door-handle detection, and control of the HSR manipulator for the completion of the cleaning tasks. The detection part exploits a deep-learning technique to classify the image space, and provides a set of coordinates for the robot. The cooperative control between the spraying and wiping is developed in the Robotic Operating System. The control module uses the information obtained from the detection module to generate a task\/operational space for the robot, along with evaluating the desired position to actuate the manipulators. The complete strategy is validated through numerical simulations, and experiments on a Toyota HSR platform.","publish_time":1592870400000,"author_summary":" Ramalingam, Balakrishnan; Yin, Jia; Rajesh<br>Elara, Mohan; Tamilselvam, Yokhesh Krishnasamy;<br>Mohan Rayguru, Madan; Muthugala, M A Viraj J; F\u00e9lix<br>G\u00f3mez, Braulio","abstract_summary":" The role of mobile robots for cleaning and<br>sanitation purposes is increasing worldwide.<br>Disinfection and hygiene are two integral parts of any safe<br>indoor environment, and these factors become more<br>critical in COVID-19-like pandemic situations. Door<br>handles are highly sensitive contact points that are<br>prone to be contamination. Automation of the<br>door-handle cleaning task is not only important for<br>ensuring safety, but also to improve efficiency. This<br>work proposes an AI-enabled framework for<br>automating cleaning tasks through a Human Support Robot<br>(HSR). The overall cleaning process involves mobile<br>base motion, door-handle detection, and control of<br>the HSR manipulator for the...","title_summary":" A Human Support Robot for the Cleaning and<br>Maintenance of Door Handles Using a Deep-Learning<br>Framework.","x":-8.8706998825,"y":40.1188850403,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.8706998825,"tsne_y":40.1188850403,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ts83s1fg","source_x":"Medline","title":"Nonnegative Blind Source Separation for Ill-Conditioned Mixtures via John Ellipsoid.","doi":"10.1109\/tnnls.2020.3002618","abstract":"Nonnegative blind source separation (nBSS) is often a challenging inverse problem, namely, when the mixing system is ill-conditioned. In this work, we focus on an important nBSS instance, known as hyperspectral unmixing (HU) in remote sensing. HU is a matrix factorization problem aimed at factoring the so-called endmember matrix, holding the material hyperspectral signatures, and the abundance matrix, holding the material fractions at each image pixel. The hyperspectral signatures are usually highly correlated, leading to a fast decay of the singular values (and, hence, high condition number) of the endmember matrix, so HU often introduces an ill-conditioned nBSS scenario. We introduce a new theoretical framework to attack such tough scenarios via the John ellipsoid (JE) in functional analysis. The idea is to identify the maximum volume ellipsoid inscribed in the data convex hull, followed by affinely mapping such ellipsoid into a Euclidean ball. By applying the same affine mapping to the data mixtures, we prove that the endmember matrix associated with the mapped data has condition number 1, the lowest possible, and that these (preconditioned) endmembers form a regular simplex. Exploiting this regular structure, we design a novel nBSS criterion with a provable identifiability guarantee and devise an algorithm to realize the criterion. Moreover, for the first time, the optimization problem for computing JE is exactly solved for a large-scale instance; our solver employs a split augmented Lagrangian shrinkage algorithm with all proximal operators solved by closed-form solutions. The competitiveness of the proposed method is illustrated by numerical simulations and real data experiments.","publish_time":1593561600000,"author_summary":" Lin, Chia-Hsiang; Bioucas-Dias, Jose M","abstract_summary":" Nonnegative blind source separation (nBSS) is<br>often a challenging inverse problem, namely, when<br>the mixing system is ill-conditioned. In this<br>work, we focus on an important nBSS instance, known as<br>hyperspectral unmixing (HU) in remote sensing. HU is a matrix<br>factorization problem aimed at factoring the so-called<br>endmember matrix, holding the material hyperspectral<br>signatures, and the abundance matrix, holding the material<br>fractions at each image pixel. The hyperspectral<br>signatures are usually highly correlated, leading to a<br>fast decay of the singular values (and, hence, high<br>condition number) of the endmember matrix, so HU often<br>introduces an ill-conditioned nBSS scenario. We...","title_summary":" Nonnegative Blind Source Separation for<br>Ill-Conditioned Mixtures via John Ellipsoid.","x":-13.0557556152,"y":40.8376693726,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-13.0557556152,"tsne_y":40.8376693726,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"7tzy3vid","source_x":"Medline","title":"P-ODN: Prototype-based Open Deep Network for Open Set Recognition.","doi":"10.1038\/s41598-020-63649-6","abstract":"Most of the existing recognition algorithms are proposed for closed set scenarios, where all categories are known beforehand. However, in practice, recognition is essentially an open set problem. There are categories we know called \"knowns\", and there are more we do not know called \"unknowns\". Enumerating all categories beforehand is never possible, consequently, it is infeasible to prepare sufficient training samples for those unknowns. Applying closed set recognition methods will naturally lead to unseen-category errors. To address this problem, we propose the prototype-based Open Deep Network (P-ODN) for open set recognition tasks. Specifically, we introduce prototype learning into open set recognition. Prototypes and prototype radiuses are trained jointly to guide a CNN network to derive more discriminative features. Then P-ODN detects the unknowns by applying a multi-class triplet thresholding method based on the distance metric between features and prototypes. Manual labeling the unknowns which are detected in the previous process as new categories. Predictors for new categories are added to the classification layer to \"open\" the deep neural networks to incorporate new categories dynamically. The weights of new predictors are initialized exquisitely by applying a distances based algorithm to transfer the learned knowledge. Consequently, this initialization method speeds up the fine-tuning process and reduce the samples needed to train new predictors. Extensive experiments show that P-ODN can effectively detect unknowns and needs only few samples with human intervention to recognize a new category. In the real world scenarios, our method achieves state-of-the-art performance on the UCF11, UCF50, UCF101 and HMDB51 datasets.","publish_time":1588032000000,"author_summary":" Shu, Yu; Shi, Yemin; Wang, Yaowei; Huang,<br>Tiejun; Tian, Yonghong","abstract_summary":" Most of the existing recognition algorithms<br>are proposed for closed set scenarios, where all<br>categories are known beforehand. However, in practice,<br>recognition is essentially an open set problem. There are<br>categories we know called \"knowns\", and there are more we do<br>not know called \"unknowns\". Enumerating all<br>categories beforehand is never possible, consequently,<br>it is infeasible to prepare sufficient training<br>samples for those unknowns. Applying closed set<br>recognition methods will naturally lead to<br>unseen-category errors. To address this problem, we propose the<br>prototype-based Open Deep Network (P-ODN) for open set<br>recognition tasks. Specifically, we introduce prototype<br>learning into open set...","title_summary":" P-ODN: Prototype-based Open Deep Network for<br>Open Set Recognition.","x":-9.6708850861,"y":40.8300590515,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.6708850861,"tsne_y":40.8300590515,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"624ka04p","source_x":"Elsevier","title":"Deep Learning Based Approach for Identifying Conventional Machining Processes from CAD Data","doi":"10.1016\/j.promfg.2020.05.130","abstract":"Abstract Manufacturing has evolved to become more automated in pursuit of higher quality, better productivity and lower cost. However, industrial logistics between the end customer and manufacturing supply chain still demands human labor and intelligence. This logistic work requires engineers with experienced knowledge to identify machining processes from the CAD model provided by customers at the beginning, and later sourcing for qualified manufacturing suppliers according to identified manufacturing processes. Developing an efficient automatic Machining Process Identification (MPI) system becomes a pivotal problem for logistic automation. In this paper, a novel MPI system is presented based on 3D Convolutional Neural Networks (CNN) and Transfer learning. The proposed system admits triangularly tessellated surface (STL) models as inputs and outputs machining process labels (e.g. milling, turning etc.) as the results of classification of the neural network. Computer-synthesized workpiece models are utilized in training the neural network. In addition to the MPI system, a portable framework was developed for future applications in related fields. The MPI system shows more than 98% accuracy for both synthesized models and real workpiece models which verifies its robustness and real-time reliability.","publish_time":1609372800000,"author_summary":" Peddireddy, Dheeraj; Fu, Xingyu; Wang, Haobo;<br>Joung, Byung Gun; Aggarwal, Vaneet; Sutherland, John<br>W.; Byung-Guk Jun, Martin","abstract_summary":" Abstract Manufacturing has evolved to become<br>more automated in pursuit of higher quality, better<br>productivity and lower cost. However, industrial logistics<br>between the end customer and manufacturing supply<br>chain still demands human labor and intelligence.<br>This logistic work requires engineers with<br>experienced knowledge to identify machining processes<br>from the CAD model provided by customers at the<br>beginning, and later sourcing for qualified<br>manufacturing suppliers according to identified<br>manufacturing processes. Developing an efficient automatic<br>Machining Process Identification (MPI) system becomes a<br>pivotal problem for logistic automation. In this<br>paper, a novel MPI system is presented based on 3D<br>Convolutional Neural Networks...","title_summary":" Deep Learning Based Approach for Identifying<br>Conventional Machining Processes from CAD Data","x":-8.1784534454,"y":40.5575141907,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.1784534454,"tsne_y":40.5575141907,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"rzs5alfk","source_x":"Elsevier","title":"Deep Learning-Based Intelligent Defect Detection of Cutting Wheels with Industrial Images in Manufacturing","doi":"10.1016\/j.promfg.2020.05.128","abstract":"Abstract The cutting wheel is an important tool in the television liquid crystal display (LCD) panel manufacturing process. The degradation of the cutting wheel significantly affects the LCD panel quality. Currently, there is few effective approaches that can detect the degradation of the cutting wheel at the working station for health monitoring purpose, due to the small size of the component and the complex manufacturing operation. That leads to high economic costs in the production lines in the real industries. In order to address this issue, this paper presents a deep convolutional neural network-based method for defect detection of the cutting wheels using the industrial images. An end-to-end health monitoring system is built based on machine vision, which directly takes the raw images as inputs, and outputs the detection results. That facilitates the industrial applications since little prior knowledge on image processing and fault detection is required. The experiments on a real-world cutting wheel degradation dataset are carried out for validation. High fault diagnosis testing accuracies are obtained, that indicates the proposed method offers an effective and promising approach for the cutting wheel health monitoring problem.","publish_time":1609372800000,"author_summary":" Yang, Shaojie; Li, Xiang; Jia, Xiaodong; Wang,<br>Yinglu; Zhao, Haodong; Lee, Jay","abstract_summary":" Abstract The cutting wheel is an important tool<br>in the television liquid crystal display (LCD)<br>panel manufacturing process. The degradation of the<br>cutting wheel significantly affects the LCD panel<br>quality. Currently, there is few effective approaches<br>that can detect the degradation of the cutting wheel<br>at the working station for health monitoring<br>purpose, due to the small size of the component and the<br>complex manufacturing operation. That leads to high<br>economic costs in the production lines in the real<br>industries. In order to address this issue, this paper<br>presents a deep convolutional neural network-based<br>method for defect detection of the...","title_summary":" Deep Learning-Based Intelligent Defect<br>Detection of Cutting Wheels with Industrial Images in<br>Manufacturing","x":-8.2209300995,"y":40.5877799988,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.2209300995,"tsne_y":40.5877799988,"subcluster":4,"subcluster_description":"Manufacturing Defects","shape":"p"},{"cord_uid":"41a7vtxv","source_x":"Elsevier","title":"Real-Time Assembly Operation Recognition with Fog Computing and Transfer Learning for Human-Centered Intelligent Manufacturing","doi":"10.1016\/j.promfg.2020.05.131","abstract":"Abstract In a human-centered intelligent manufacturing system, every element is to assist the operator in achieving the optimal operational performance. The primary task of developing such a human-centered system is to accurately understand human behavior. In this paper, we propose a fog computing framework for assembly operation recognition, which brings computing power close to the data source in order to achieve real-time recognition. For data collection, the operator's activity is captured using visual cameras from different perspectives. For operation recognition, instead of directly building and training a deep learning model from scratch, which needs a huge amount of data, transfer learning is applied to transfer the learning abilities to our application. A worker assembly operation dataset is established, which at present contains 10 sequential operations in an assembly task of installing a desktop CNC machine. The developed transfer learning model is evaluated on this dataset and achieves a recognition accuracy of 95% in the testing experiments.","publish_time":1609372800000,"author_summary":" Tao, Wenjin; Al-Amin, Md; Chen, Haodong; Leu,<br>Ming C.; Yin, Zhaozheng; Qin, Ruwen","abstract_summary":" Abstract In a human-centered intelligent<br>manufacturing system, every element is to assist the operator<br>in achieving the optimal operational<br>performance. The primary task of developing such a<br>human-centered system is to accurately understand human<br>behavior. In this paper, we propose a fog computing<br>framework for assembly operation recognition, which<br>brings computing power close to the data source in<br>order to achieve real-time recognition. For data<br>collection, the operator's activity is captured using<br>visual cameras from different perspectives. For<br>operation recognition, instead of directly building and<br>training a deep learning model from scratch, which needs<br>a huge amount of data, transfer...","title_summary":" Real-Time Assembly Operation Recognition<br>with Fog Computing and Transfer Learning for<br>Human-Centered Intelligent Manufacturing","x":-8.5848150253,"y":40.5278320312,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.5848150253,"tsne_y":40.5278320312,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ve0k8nzk","source_x":"ArXiv","title":"Efficient Noise-Blind $\\ell_1$-Regression of Nonnegative Compressible Signals","doi":null,"abstract":"In compressed sensing the goal is to recover a signal from as few as possible noisy, linear measurements. The general assumption is that the signal has only a few non-zero entries. Given an estimate for the noise level a common convex approach to recover the signal is basis pursuit denoising (BPDN). If the measurement matrix has the robust null space property with respect to the $\\ell_2$-norm, BPDN obeys stable and robust recovery guarantees. In the case of unknown noise levels, nonnegative least squares recovers non-negative signals if the measurement matrix fulfills an additional property (sometimes called the $M^+$-criterion). However, if the measurement matrix is the biadjacency matrix of a random left regular bipartite graph it obeys with a high probability the null space property with respect to the $\\ell_1$-norm with optimal parameters. Therefore, we discuss non-negative least absolute deviation (NNLAD). For these measurement matrices, we prove a uniform, stable and robust recovery guarantee. Such guarantees are important, since binary expander matrices are sparse and thus allow for fast sketching and recovery. We will further present a method to solve the NNLAD numerically and show that this is comparable to state of the art methods. Lastly, we explain how the NNLAD can be used for group testing in the recent COVID-19 crisis and why contamination of specimens may be modeled as peaky noise, which favors $\\ell_1$ based data fidelity terms.","publish_time":1585440000000,"author_summary":" Petersen, Hendrik Bernd; Bah, Bubacarr; Jung,<br>Peter","abstract_summary":" In compressed sensing the goal is to recover a<br>signal from as few as possible noisy, linear<br>measurements. The general assumption is that the signal has<br>only a few non-zero entries. Given an estimate for<br>the noise level a common convex approach to recover<br>the signal is basis pursuit denoising (BPDN). If<br>the measurement matrix has the robust null space<br>property with respect to the $\\ell_2$-norm, BPDN obeys<br>stable and robust recovery guarantees. In the case of<br>unknown noise levels, nonnegative least squares<br>recovers non-negative signals if the measurement<br>matrix fulfills an additional property (sometimes<br>called the $M^+$-criterion). However, if...","title_summary":" Efficient Noise-Blind $\\ell_1$-Regression<br>of Nonnegative Compressible Signals","x":-13.244805336,"y":40.8014030457,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-13.244805336,"tsne_y":40.8014030457,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"hsghwe72","source_x":"ArXiv","title":"Simple Primary Colour Editing for Consumer Product Images","doi":null,"abstract":"We present a simple primary colour editing method for consumer product images. We show that by using colour correction and colour blending, we can automate the pain-staking colour editing task and save time for consumer colour preference researchers. To improve the colour harmony between the primary colour and its complementary colours, our algorithm also tunes the other colours in the image. Preliminary experiment has shown some promising results compared with a state-of-the-art method and human editing.","publish_time":1591401600000,"author_summary":" Gong, Han; Yu, Luwen; Westland, Stephen","abstract_summary":" We present a simple primary colour editing<br>method for consumer product images. We show that by<br>using colour correction and colour blending, we can<br>automate the pain-staking colour editing task and save<br>time for consumer colour preference researchers.<br>To improve the colour harmony between the primary<br>colour and its complementary colours, our algorithm<br>also tunes the other colours in the image.<br>Preliminary experiment has shown some promising results<br>compared with a state-of-the-art method and human<br>editing.","title_summary":" Simple Primary Colour Editing for Consumer<br>Product Images","x":-9.3224430084,"y":43.8004646301,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.3224430084,"tsne_y":43.8004646301,"subcluster":0,"subcluster_description":"Individual Mangrove Tree Modelingcombined","shape":"p"},{"cord_uid":"ahh0zamx","source_x":"ArXiv","title":"Deep Geometric Texture Synthesis","doi":null,"abstract":"Recently, deep generative adversarial networks for image generation have advanced rapidly; yet, only a small amount of research has focused on generative models for irregular structures, particularly meshes. Nonetheless, mesh generation and synthesis remains a fundamental topic in computer graphics. In this work, we propose a novel framework for synthesizing geometric textures. It learns geometric texture statistics from local neighborhoods (i.e., local triangular patches) of a single reference 3D model. It learns deep features on the faces of the input triangulation, which is used to subdivide and generate offsets across multiple scales, without parameterization of the reference or target mesh. Our network displaces mesh vertices in any direction (i.e., in the normal and tangential direction), enabling synthesis of geometric textures, which cannot be expressed by a simple 2D displacement map. Learning and synthesizing on local geometric patches enables a genus-oblivious framework, facilitating texture transfer between shapes of different genus.","publish_time":1593475200000,"author_summary":" Hertz, Amir; Hanocka, Rana; Giryes, Raja;<br>Cohen-Or, Daniel","abstract_summary":" Recently, deep generative adversarial<br>networks for image generation have advanced rapidly;<br>yet, only a small amount of research has focused on<br>generative models for irregular structures,<br>particularly meshes. Nonetheless, mesh generation and<br>synthesis remains a fundamental topic in computer<br>graphics. In this work, we propose a novel framework for<br>synthesizing geometric textures. It learns geometric<br>texture statistics from local neighborhoods (i.e.,<br>local triangular patches) of a single reference 3D<br>model. It learns deep features on the faces of the input<br>triangulation, which is used to subdivide and generate offsets<br>across multiple scales, without parameterization of<br>the reference or target mesh....","title_summary":" Deep Geometric Texture Synthesis","x":-10.569152832,"y":41.5858879089,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.569152832,"tsne_y":41.5858879089,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"abl691jz","source_x":"ArXiv","title":"Translating Diffusion, Wavelets, and Regularisation into Residual Networks","doi":null,"abstract":"Convolutional neural networks (CNNs) often perform well, but their stability is poorly understood. To address this problem, we consider the simple prototypical problem of signal denoising, where classical approaches such as nonlinear diffusion, wavelet-based methods and regularisation offer provable stability guarantees. To transfer such guarantees to CNNs, we interpret numerical approximations of these classical methods as a specific residual network (ResNet) architecture. This leads to a dictionary which allows to translate diffusivities, shrinkage functions, and regularisers into activation functions, and enables a direct communication between the four research communities. On the CNN side, it does not only inspire new families of nonmonotone activation functions, but also introduces intrinsically stable architectures for an arbitrary number of layers.","publish_time":1581033600000,"author_summary":" Alt, Tobias; Weickert, Joachim; Peter, Pascal","abstract_summary":" Convolutional neural networks (CNNs) often<br>perform well, but their stability is poorly<br>understood. To address this problem, we consider the simple<br>prototypical problem of signal denoising, where classical<br>approaches such as nonlinear diffusion, wavelet-based<br>methods and regularisation offer provable stability<br>guarantees. To transfer such guarantees to CNNs, we<br>interpret numerical approximations of these classical<br>methods as a specific residual network (ResNet)<br>architecture. This leads to a dictionary which allows to<br>translate diffusivities, shrinkage functions, and<br>regularisers into activation functions, and enables a<br>direct communication between the four research<br>communities. On the CNN side, it does not only inspire new<br>families...","title_summary":" Translating Diffusion, Wavelets, and<br>Regularisation into Residual Networks","x":-11.2843389511,"y":41.3058319092,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.2843389511,"tsne_y":41.3058319092,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"v66a7jno","source_x":"ArXiv","title":"Comparison of Image Quality Models for Optimization of Image Processing Systems","doi":null,"abstract":"The performance of objective image quality assessment (IQA) models has been evaluated primarily by comparing model predictions to human judgments. Perceptual datasets (e.g., LIVE and TID2013) gathered for this purpose provide useful benchmarks for improving IQA methods, but their heavy use creates a risk of overfitting. Here, we perform a large-scale comparison of perceptual IQA models in terms of their use as objectives for the optimization of image processing algorithms. Specifically, we evaluate eleven full-reference IQA models by using them as objective functions to train deep neural networks for four low-level vision tasks: denoising, deblurring, super-resolution, and compression. Extensive subjective testing on the optimized images allows us to rank the competing models in terms of their perceptual performance, elucidate their relative advantages and disadvantages for these tasks, and propose a set of desirable properties for incorporation into future IQA models. The implementations are available at https:\/\/github.com\/dingkeyan93\/IQA-optimization.","publish_time":1588550400000,"author_summary":" Ding, Keyan; Ma, Kede; Wang, Shiqi;<br>Simoncelli, Eero P.","abstract_summary":" The performance of objective image quality<br>assessment (IQA) models has been evaluated primarily by<br>comparing model predictions to human judgments.<br>Perceptual datasets (e.g., LIVE and TID2013) gathered for<br>this purpose provide useful benchmarks for<br>improving IQA methods, but their heavy use creates a risk<br>of overfitting. Here, we perform a large-scale<br>comparison of perceptual IQA models in terms of their use as<br>objectives for the optimization of image processing<br>algorithms. Specifically, we evaluate eleven<br>full-reference IQA models by using them as objective functions<br>to train deep neural networks for four low-level<br>vision tasks: denoising, deblurring,<br>super-resolution, and compression. Extensive subjective...","title_summary":" Comparison of Image Quality Models for<br>Optimization of Image Processing Systems","x":-8.4435377121,"y":42.5578041077,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.4435377121,"tsne_y":42.5578041077,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"80it9lxn","source_x":"ArXiv","title":"Personalization of Hearing Aid Compression by Human-In-Loop Deep Reinforcement Learning","doi":null,"abstract":"Existing prescriptive compression strategies used in hearing aid fitting are designed based on gain averages from a group of users which are not necessarily optimal for a specific user. Nearly half of hearing aid users prefer settings that differ from the commonly prescribed settings. This paper presents a human-in-loop deep reinforcement learning approach that personalizes hearing aid compression to achieve improved hearing perception. The developed approach is designed to learn a specific user's hearing preferences in order to optimize compression based on the user's feedbacks. Both simulation and subject testing results are reported which demonstrate the effectiveness of the developed personalized compression.","publish_time":1593561600000,"author_summary":" Alamdari, Nasim; Lobarinas, Edward;<br>Kehtarnavaz, Nasser","abstract_summary":" Existing prescriptive compression<br>strategies used in hearing aid fitting are designed based<br>on gain averages from a group of users which are not<br>necessarily optimal for a specific user. Nearly half of<br>hearing aid users prefer settings that differ from the<br>commonly prescribed settings. This paper presents a<br>human-in-loop deep reinforcement learning approach that<br>personalizes hearing aid compression to achieve improved<br>hearing perception. The developed approach is<br>designed to learn a specific user's hearing preferences<br>in order to optimize compression based on the<br>user's feedbacks. Both simulation and subject<br>testing results are reported which demonstrate the<br>effectiveness of the developed...","title_summary":" Personalization of Hearing Aid Compression by<br>Human-In-Loop Deep Reinforcement Learning","x":-11.3004283905,"y":39.8416061401,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.3004283905,"tsne_y":39.8416061401,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"wd8xn4v8","source_x":"ArXiv","title":"A Semi-Supervised Generative Adversarial Network for Prediction of Genetic Disease Outcomes","doi":null,"abstract":"For most diseases, building large databases of labeled genetic data is an expensive and time-demanding task. To address this, we introduce genetic Generative Adversarial Networks (gGAN), a semi-supervised approach based on an innovative GAN architecture to create large synthetic genetic data sets starting with a small amount of labeled data and a large amount of unlabeled data. Our goal is to determine the propensity of a new individual to develop the severe form of the illness from their genetic profile alone. The proposed model achieved satisfactory results using real genetic data from different datasets and populations, in which the test populations may not have the same genetic profiles. The proposed model is self-aware and capable of determining whether a new genetic profile has enough compatibility with the data on which the network was trained and is thus suitable for prediction. The code and datasets used can be found at https:\/\/github.com\/caio-davi\/gGAN.","publish_time":1593648000000,"author_summary":" Davi, Caio; Braga-Neto, Ulisses","abstract_summary":" For most diseases, building large databases of<br>labeled genetic data is an expensive and<br>time-demanding task. To address this, we introduce genetic<br>Generative Adversarial Networks (gGAN), a<br>semi-supervised approach based on an innovative GAN<br>architecture to create large synthetic genetic data sets<br>starting with a small amount of labeled data and a large<br>amount of unlabeled data. Our goal is to determine the<br>propensity of a new individual to develop the severe form of<br>the illness from their genetic profile alone. The<br>proposed model achieved satisfactory results using<br>real genetic data from different datasets and<br>populations, in which the test...","title_summary":" A Semi-Supervised Generative Adversarial<br>Network for Prediction of Genetic Disease Outcomes","x":-11.424489975,"y":39.9019203186,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.424489975,"tsne_y":39.9019203186,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"jr9cz14h","source_x":"ArXiv","title":"Removing Multi-frame Gaussian Noise by Combining Patch-based Filters with Optical Flow","doi":null,"abstract":"Patch-based methods such as 3D block matching (BM3D) and the non-local Bayes (NLB) approach produce state-of-the-art results for removing Gaussian noise from single-frame images. In this work, we propose two extensions for these filters when there exist multiple frames of the same scene. To this end, we combine two novel inter-frame connectivity strategies with robust optical flow methods. Our extensions do not require additional parameters and outperform existing techniques qualitatively by a significant margin. By exploiting spatial and temporal separability, one of our approaches is also faster than its competitors. Since our strategy is not restricted to BM3D and NLB, it can be generalised to other similar single-frame patch-based methods.","publish_time":1579651200000,"author_summary":" Bodduna, Kireeti; Weickert, Joachim","abstract_summary":" Patch-based methods such as 3D block matching<br>(BM3D) and the non-local Bayes (NLB) approach produce<br>state-of-the-art results for removing Gaussian noise from<br>single-frame images. In this work, we propose two extensions<br>for these filters when there exist multiple frames<br>of the same scene. To this end, we combine two novel<br>inter-frame connectivity strategies with robust optical<br>flow methods. Our extensions do not require<br>additional parameters and outperform existing<br>techniques qualitatively by a significant margin. By<br>exploiting spatial and temporal separability, one of our<br>approaches is also faster than its competitors. Since our<br>strategy is not restricted to BM3D and NLB,...","title_summary":" Removing Multi-frame Gaussian Noise by<br>Combining Patch-based Filters with Optical Flow","x":-9.5997657776,"y":42.7229347229,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.5997657776,"tsne_y":42.7229347229,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"9qnly3um","source_x":"ArXiv","title":"Multi-Objective Neural Architecture Search Based on Diverse Structures and Adaptive Recommendation","doi":null,"abstract":"The search space of neural architecture search (NAS) for convolutional neural network (CNN) is huge. To reduce searching cost, most NAS algorithms use fixed outer network level structure, and search the repeatable cell structure only. Such kind of fixed architecture performs well when enough cells and channels are used. However, when the architecture becomes more lightweight, the performance decreases significantly. To obtain better lightweight architectures, more flexible and diversified neural architectures are in demand, and more efficient methods should be designed for larger search space. Motivated by this, we propose MoARR algorithm, which utilizes the existing research results and historical information to quickly find architectures that are both lightweight and accurate. We use the discovered high-performance cells to construct network architectures. This method increases the network architecture diversity while also reduces the search space of cell structure design. In addition, we designs a novel multi-objective method to effectively analyze the historical evaluation information, so as to efficiently search for the Pareto optimal architectures with high accuracy and small parameter number. Experimental results show that our MoARR can achieve a powerful and lightweight model (with 1.9% error rate and 2.3M parameters) on CIFAR-10 in 6 GPU hours, which is better than the state-of-the-arts. The explored architecture is transferable to ImageNet and achieves 76.0% top-1 accuracy with 4.9M parameters.","publish_time":1593993600000,"author_summary":" Wang, Chunnan; Wang, Hongzhi; Feng, Guocheng;<br>Geng, Fei","abstract_summary":" The search space of neural architecture search<br>(NAS) for convolutional neural network (CNN) is<br>huge. To reduce searching cost, most NAS algorithms<br>use fixed outer network level structure, and<br>search the repeatable cell structure only. Such kind<br>of fixed architecture performs well when enough<br>cells and channels are used. However, when the<br>architecture becomes more lightweight, the performance<br>decreases significantly. To obtain better lightweight<br>architectures, more flexible and diversified neural<br>architectures are in demand, and more efficient methods<br>should be designed for larger search space. Motivated<br>by this, we propose MoARR algorithm, which<br>utilizes the existing research results and historical...","title_summary":" Multi-Objective Neural Architecture Search<br>Based on Diverse Structures and Adaptive<br>Recommendation","x":-12.7799682617,"y":40.283115387,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.7799682617,"tsne_y":40.283115387,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"x9n0mjjo","source_x":"ArXiv","title":"Image denoising with less artefacts: Novel non-linear filtering on fast patch reorderings","doi":null,"abstract":"Leading denoising methods such as 3D block matching (BM3D) are patch-based. However, they can suffer from frequency domain artefacts and require to specify explicit noise models. We present a patch-based method that avoids these drawbacks. It combines a simple and fast patch reordering with a non-linear smoothing. The smoothing rewards both patch and pixel similarities in a multiplicative way. We perform experiments on real world images with additive white Gaussian noise (AWGN), and on electron microscopy data with a more general additive noise model. Our filter outperforms BM3D in 77% of the experiments, with improvements of up to 29% with respect to the mean squared error.","publish_time":1580688000000,"author_summary":" Bodduna, Kireeti; Weickert, Joachim","abstract_summary":" Leading denoising methods such as 3D block<br>matching (BM3D) are patch-based. However, they can<br>suffer from frequency domain artefacts and require to<br>specify explicit noise models. We present a<br>patch-based method that avoids these drawbacks. It<br>combines a simple and fast patch reordering with a<br>non-linear smoothing. The smoothing rewards both patch<br>and pixel similarities in a multiplicative way. We<br>perform experiments on real world images with additive<br>white Gaussian noise (AWGN), and on electron<br>microscopy data with a more general additive noise model.<br>Our filter outperforms BM3D in 77% of the<br>experiments, with improvements of up to 29% with...","title_summary":" Image denoising with less artefacts: Novel<br>non-linear filtering on fast patch reorderings","x":-9.6349821091,"y":42.9778900146,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.6349821091,"tsne_y":42.9778900146,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"ky0d4du1","source_x":"ArXiv","title":"SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks","doi":null,"abstract":"We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model, leading to fewer trainable parameters and thus decreased sample complexity (i.e. we need less training data). The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy $N$-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.","publish_time":1592438400000,"author_summary":" Fuchs, Fabian B.; Worrall, Daniel E.; Fischer,<br>Volker; Welling, Max","abstract_summary":" We introduce the SE(3)-Transformer, a variant<br>of the self-attention module for 3D point clouds,<br>which is equivariant under continuous 3D<br>roto-translations. Equivariance is important to ensure stable and<br>predictable performance in the presence of nuisance<br>transformations of the data input. A positive corollary of<br>equivariance is increased weight-tying within the model,<br>leading to fewer trainable parameters and thus<br>decreased sample complexity (i.e. we need less training<br>data). The SE(3)-Transformer leverages the benefits<br>of self-attention to operate on large point<br>clouds with varying number of points, while<br>guaranteeing SE(3)-equivariance for robustness. We<br>evaluate our model on a toy $N$-body particle simulation...","title_summary":" SE(3)-Transformers: 3D Roto-Translation<br>Equivariant Attention Networks","x":-11.321138382,"y":40.9259147644,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.321138382,"tsne_y":40.9259147644,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"i1imfq3g","source_x":"ArXiv","title":"Proceedings of the ICLR Workshop on Computer Vision for Agriculture (CV4A) 2020","doi":null,"abstract":"This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held in conjunction with the International Conference on Learning Representations (ICLR) 2020. The Computer Vision for Agriculture (CV4A) 2020 workshop was scheduled to be held in Addis Ababa, Ethiopia, on April 26th, 2020. It was held virtually that same day due to the COVID-19 pandemic. The workshop was held in conjunction with the International Conference on Learning Representations (ICLR) 2020.","publish_time":1587600000000,"author_summary":" Kalantidis, Yannis; Sevilla-Lara, Laura;<br>Mwebaze, Ernest; Machuve, Dina; Alemohammad, Hamed;<br>Guerena, David","abstract_summary":" This is the proceedings of the Computer Vision<br>for Agriculture (CV4A) Workshop that was held in<br>conjunction with the International Conference on Learning<br>Representations (ICLR) 2020. The Computer Vision for<br>Agriculture (CV4A) 2020 workshop was scheduled to be held in<br>Addis Ababa, Ethiopia, on April 26th, 2020. It was<br>held virtually that same day due to the COVID-19<br>pandemic. The workshop was held in conjunction with the<br>International Conference on Learning Representations<br>(ICLR) 2020.","title_summary":" Proceedings of the ICLR Workshop on Computer<br>Vision for Agriculture (CV4A) 2020","x":-9.4007053375,"y":40.8021240234,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.4007053375,"tsne_y":40.8021240234,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"ej3naer9","source_x":"ArXiv","title":"Are you wearing a mask? Improving mask detection from speech using augmentation by cycle-consistent GANs","doi":null,"abstract":"The task of detecting whether a person wears a face mask from speech is useful in modelling speech in forensic investigations, communication between surgeons or people protecting themselves against infectious diseases such as COVID-19. In this paper, we propose a novel data augmentation approach for mask detection from speech. Our approach is based on (i) training Generative Adversarial Networks (GANs) with cycle-consistency loss to translate unpaired utterances between two classes (with mask and without mask), and on (ii) generating new training utterances using the cycle-consistent GANs, assigning opposite labels to each translated utterance. Original and translated utterances are converted into spectrograms which are provided as input to a set of ResNet neural networks with various depths. The networks are combined into an ensemble through a Support Vector Machines (SVM) classifier. With this system, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 Computational Paralinguistics Challenge, surpassing the baseline proposed by the organizers by 2.8%. Our data augmentation technique provided a performance boost of 0.9% on the private test set. Furthermore, we show that our data augmentation approach yields better results than other baseline and state-of-the-art augmentation methods.","publish_time":1592352000000,"author_summary":" Ristea, Nicolae-Cuatualin; Ionescu, Radu<br>Tudor","abstract_summary":" The task of detecting whether a person wears a<br>face mask from speech is useful in modelling speech<br>in forensic investigations, communication<br>between surgeons or people protecting themselves<br>against infectious diseases such as COVID-19. In this<br>paper, we propose a novel data augmentation approach<br>for mask detection from speech. Our approach is<br>based on (i) training Generative Adversarial<br>Networks (GANs) with cycle-consistency loss to<br>translate unpaired utterances between two classes (with<br>mask and without mask), and on (ii) generating new<br>training utterances using the cycle-consistent GANs,<br>assigning opposite labels to each translated utterance.<br>Original and translated utterances are converted into...","title_summary":" Are you wearing a mask? Improving mask<br>detection from speech using augmentation by<br>cycle-consistent GANs","x":-10.3757982254,"y":40.2083740234,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.3757982254,"tsne_y":40.2083740234,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"c73xaebv","source_x":"ArXiv","title":"Differentiable Segmentation of Sequences","doi":null,"abstract":"Segmented models are widely used to describe non-stationary sequential data with discrete change points. Their estimation usually requires solving a mixed discrete-continuous optimization problem, where the segmentation is the discrete part and all other model parameters are continuous. A number of estimation algorithms have been developed that are highly specialized for their specific model assumptions. The dependence on non-standard algorithms makes it hard to integrate segmented models in state-of-the-art deep learning architectures that critically depend on gradient-based optimization techniques. In this work, we formulate a relaxed variant of segmented models that enables joint estimation of all model parameters, including the segmentation, with gradient descent. We build on recent advances in learning continuous warping functions and propose a novel family of warping functions based on the two-sided power (TSP) distribution. TSP-based warping functions are differentiable, have simple closed-form expressions, and can represent segmentation functions exactly. Our formulation includes the important class of segmented generalized linear models as a special case, which makes it highly versatile. We use our approach to model the spread of COVID-19 by segmented Poisson regression, perform logistic regression on Fashion-MNIST with artificial concept drift, and demonstrate its capacities for phoneme segmentation.","publish_time":1592870400000,"author_summary":" Scharwachter, Erik; Lennartz, Jonathan;<br>Muller, Emmanuel","abstract_summary":" Segmented models are widely used to describe<br>non-stationary sequential data with discrete change points.<br>Their estimation usually requires solving a mixed<br>discrete-continuous optimization problem, where the segmentation<br>is the discrete part and all other model<br>parameters are continuous. A number of estimation<br>algorithms have been developed that are highly<br>specialized for their specific model assumptions. The<br>dependence on non-standard algorithms makes it hard to<br>integrate segmented models in state-of-the-art deep<br>learning architectures that critically depend on<br>gradient-based optimization techniques. In this work, we<br>formulate a relaxed variant of segmented models that<br>enables joint estimation of all model parameters,<br>including the...","title_summary":" Differentiable Segmentation of Sequences","x":-10.352563858,"y":42.3588294983,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.352563858,"tsne_y":42.3588294983,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"zvlm8ebh","source_x":"ArXiv","title":"The DeepFake Detection Challenge Dataset","doi":null,"abstract":"Deepfakes are a recent off-the-shelf manipulation technique that allows anyone to swap two identities in a single video. In addition to Deepfakes, a variety of GAN-based face swapping methods have also been published with accompanying code. To counter this emerging threat, we have constructed an extremely large face swap video dataset to enable the training of detection models, and organized the accompanying DeepFake Detection Challenge (DFDC) Kaggle competition. Importantly, all recorded subjects agreed to participate in and have their likenesses modified during the construction of the face-swapped dataset. The DFDC dataset is by far the largest currently and publicly available face swap video dataset, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. In addition to describing the methods used to construct the dataset, we provide a detailed analysis of the top submissions from the Kaggle contest. We show although Deepfake detection is extremely difficult and still an unsolved problem, a Deepfake detection model trained only on the DFDC can generalize to real\"in-the-wild\"Deepfake videos, and such a model can be a valuable analysis tool when analyzing potentially Deepfaked videos. Training, validation and testing corpuses can be downloaded from https:\/\/ai.facebook.com\/datasets\/dfdc.","publish_time":1591920000000,"author_summary":" Dolhansky, Brian; Bitton, Joanna; Pflaum,<br>Ben; Lu, Jikuo; Howes, Russ; Wang, Menglin; Ferrer,<br>Cristian Canton","abstract_summary":" Deepfakes are a recent off-the-shelf<br>manipulation technique that allows anyone to swap two<br>identities in a single video. In addition to Deepfakes, a<br>variety of GAN-based face swapping methods have also<br>been published with accompanying code. To counter<br>this emerging threat, we have constructed an<br>extremely large face swap video dataset to enable the<br>training of detection models, and organized the<br>accompanying DeepFake Detection Challenge (DFDC) Kaggle<br>competition. Importantly, all recorded subjects agreed to<br>participate in and have their likenesses modified during<br>the construction of the face-swapped dataset. The<br>DFDC dataset is by far the largest currently and<br>publicly available...","title_summary":" The DeepFake Detection Challenge Dataset","x":-10.8100337982,"y":40.4585952759,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.8100337982,"tsne_y":40.4585952759,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"12l3t9zh","source_x":"ArXiv","title":"MaskIt: Masking for efficient utilization of incomplete public datasets for training deep learning models","doi":null,"abstract":"A major challenge in training deep learning models is the lack of high quality and complete datasets. In the paper, we present a masking approach for training deep learning models from a publicly available but incomplete dataset. For example, city of Hamburg, Germany maintains a list of trees along the roads, but this dataset does not contain any information about trees in private homes and parks. To train a deep learning model on such a dataset, we mask the street trees and aerial images with the road network. Road network used for creating the mask is downloaded from OpenStreetMap, and it marks the area where the training data is available. The mask is passed to the model as one of the inputs and it also coats the output. Our model learns to successfully predict trees only in the masked region with 78.4% accuracy.","publish_time":1592784000000,"author_summary":" Kariryaa, Ankit","abstract_summary":" A major challenge in training deep learning<br>models is the lack of high quality and complete<br>datasets. In the paper, we present a masking approach for<br>training deep learning models from a publicly available<br>but incomplete dataset. For example, city of<br>Hamburg, Germany maintains a list of trees along the<br>roads, but this dataset does not contain any<br>information about trees in private homes and parks. To train<br>a deep learning model on such a dataset, we mask<br>the street trees and aerial images with the road<br>network. Road network used for creating the mask is<br>downloaded from OpenStreetMap, and...","title_summary":" MaskIt: Masking for efficient utilization of<br>incomplete public datasets for training deep learning<br>models","x":-10.6873579025,"y":40.5598258972,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.6873579025,"tsne_y":40.5598258972,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"dibnbb3q","source_x":"ArXiv","title":"Compressing Piecewise Smooth Images with the Mumford-Shah Cartoon Model","doi":null,"abstract":"Compressing piecewise smooth images is important for many data types such as depth maps in 3D videos or optic flow fields for motion compensation. Specialised codecs that rely on explicitly stored segmentations excel in this task since they preserve discontinuities between smooth regions. However, current approaches rely on ad hoc segmentations that lack a clean interpretation in terms of energy minimisation. As a remedy, we derive a generic region merging algorithm from the Mumford-Shah cartoon model. It adapts the segmentation to arbitrary reconstruction operators for the segment content. In spite of its conceptual simplicity, our framework can outperform previous segment-based compression methods as well as BPG by up to 3 dB.","publish_time":1583884800000,"author_summary":" Jost, Ferdinand; Peter, Pascal; Weickert,<br>Joachim","abstract_summary":" Compressing piecewise smooth images is<br>important for many data types such as depth maps in 3D<br>videos or optic flow fields for motion compensation.<br>Specialised codecs that rely on explicitly stored<br>segmentations excel in this task since they preserve<br>discontinuities between smooth regions. However, current<br>approaches rely on ad hoc segmentations that lack a clean<br>interpretation in terms of energy minimisation. As a remedy, we<br>derive a generic region merging algorithm from the<br>Mumford-Shah cartoon model. It adapts the segmentation to<br>arbitrary reconstruction operators for the segment<br>content. In spite of its conceptual simplicity, our<br>framework can outperform previous segment-based...","title_summary":" Compressing Piecewise Smooth Images with the<br>Mumford-Shah Cartoon Model","x":-9.6888847351,"y":43.0593986511,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.6888847351,"tsne_y":43.0593986511,"subcluster":5,"subcluster_description":"Elliptical Traffic Signsa Spectral","shape":"p"},{"cord_uid":"rfusqd6z","source_x":"ArXiv","title":"GACELA -- A generative adversarial context encoder for long audio inpainting","doi":null,"abstract":"We introduce GACELA, a generative adversarial network (GAN) designed to restore missing musical audio data with a duration ranging between hundreds of milliseconds to a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the conditional GAN. This addresses the inherent multi-modality of audio inpainting at such long gaps and provides the option of user-defined inpainting. GACELA was tested in listening tests on music signals of varying complexity and gap durations ranging from 375~ms to 1500~ms. While our subjects were often able to detect the inpaintings, the severity of the artifacts decreased from unacceptable to mildly disturbing. GACELA represents a framework capable to integrate future improvements such as processing of more auditory-related features or more explicit musical features.","publish_time":1589155200000,"author_summary":" Marafioti, Andres; Majdak, Piotr; Holighaus,<br>Nicki; Perraudin, Nathanael","abstract_summary":" We introduce GACELA, a generative adversarial<br>network (GAN) designed to restore missing musical<br>audio data with a duration ranging between hundreds<br>of milliseconds to a few seconds, i.e., to perform<br>long-gap audio inpainting. While previous work either<br>addressed shorter gaps or relied on exemplars by copying<br>available information from other signal parts, GACELA<br>addresses the inpainting of long gaps in two aspects.<br>First, it considers various time scales of audio<br>information by relying on five parallel discriminators<br>with increasing resolution of receptive fields.<br>Second, it is conditioned not only on the available<br>information surrounding the gap, i.e., the context, but...","title_summary":" GACELA -- A generative adversarial context<br>encoder for long audio inpainting","x":-11.1565380096,"y":40.2791976929,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.1565380096,"tsne_y":40.2791976929,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"h3e1aydg","source_x":"ArXiv","title":"Learning for Video Compression with Recurrent Auto-Encoder and Recurrent Probability Model","doi":null,"abstract":"The past few years have witnessed increasing interests in applying deep learning to video compression. However, the existing approaches compress a video frame with only a few number of reference frames, which limits their ability to fully exploit the temporal correlation among video frames. To overcome this shortcoming, this paper proposes a Recurrent Learned Video Compression (RLVC) approach with the Recurrent Auto-Encoder (RAE) and Recurrent Probability Model (RPM). Specifically, the RAE employs recurrent cells in both the encoder and decoder. As such, the temporal information in a large range of frames can be used for generating latent representations and reconstructing compressed outputs. Furthermore, the proposed RPM network recurrently estimates the Probability Mass Function (PMF) of the latent representation, conditioned on the distribution of previous latent representations. Due to the correlation among consecutive frames, the conditional cross entropy can be lower than the independent cross entropy, thus reducing the bit-rate. The experiments show that our approach achieves the state-of-the-art learned video compression performance in terms of both PSNR and MS-SSIM. Moreover, our approach outperforms the default Low-Delay P (LDP) setting of x265 on PSNR, and also has better performance on MS-SSIM than the SSIM-tuned x265 and the slowest setting of x265.","publish_time":1592956800000,"author_summary":" Yang, Ren; Mentzer, Fabian; Gool, Luc Van;<br>Timofte, Radu","abstract_summary":" The past few years have witnessed increasing<br>interests in applying deep learning to video<br>compression. However, the existing approaches compress a<br>video frame with only a few number of reference<br>frames, which limits their ability to fully exploit the<br>temporal correlation among video frames. To overcome<br>this shortcoming, this paper proposes a Recurrent<br>Learned Video Compression (RLVC) approach with the<br>Recurrent Auto-Encoder (RAE) and Recurrent Probability<br>Model (RPM). Specifically, the RAE employs<br>recurrent cells in both the encoder and decoder. As such,<br>the temporal information in a large range of frames<br>can be used for generating latent representations<br>and reconstructing...","title_summary":" Learning for Video Compression with Recurrent<br>Auto-Encoder and Recurrent Probability Model","x":-11.1670198441,"y":40.6363639832,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-11.1670198441,"tsne_y":40.6363639832,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"iuk8wqrj","source_x":"ArXiv","title":"Using Wavelets and Spectral Methods to Study Patterns in Image-Classification Datasets","doi":null,"abstract":"Deep learning models extract, before a final classification layer, features or patterns which are key for their unprecedented advantageous performance. However, the process of complex nonlinear feature extraction is not well understood, a major reason why interpretation, adversarial robustness, and generalization of deep neural nets are all open research problems. In this paper, we use wavelet transformation and spectral methods to analyze the contents of image classification datasets, extract specific patterns from the datasets and find the associations between patterns and classes. We show that each image can be written as the summation of a finite number of rank-1 patterns in the wavelet space, providing a low rank approximation that captures the structures and patterns essential for learning. Regarding the studies on memorization vs learning, our results clearly reveal disassociation of patterns from classes, when images are randomly labeled. Our method can be used as a pattern recognition approach to understand and interpret learnability of these datasets. It may also be used for gaining insights about the features and patterns that deep classifiers learn from the datasets.","publish_time":1592352000000,"author_summary":" Yousefzadeh, Roozbeh; Huang, Furong","abstract_summary":" Deep learning models extract, before a final<br>classification layer, features or patterns which are key for<br>their unprecedented advantageous performance.<br>However, the process of complex nonlinear feature<br>extraction is not well understood, a major reason why<br>interpretation, adversarial robustness, and generalization<br>of deep neural nets are all open research<br>problems. In this paper, we use wavelet transformation<br>and spectral methods to analyze the contents of<br>image classification datasets, extract specific<br>patterns from the datasets and find the associations<br>between patterns and classes. We show that each image<br>can be written as the summation of a finite number of<br>rank-1 patterns...","title_summary":" Using Wavelets and Spectral Methods to Study<br>Patterns in Image-Classification Datasets","x":-10.521364212,"y":41.221321106,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.521364212,"tsne_y":41.221321106,"subcluster":1,"subcluster_description":"Convolutional Neural Networksdeep Geometric","shape":"p"},{"cord_uid":"bxgcjbdj","source_x":"ArXiv","title":"Novelty Detection via Robust Variational Autoencoding","doi":null,"abstract":"We propose a new method for novelty detection that can tolerate nontrivial corruption of the training points. Previous works assumed either no or very low corruption. Our method trains a robust variational autoencoder (VAE), which aims to generate a model for the uncorrupted training points. To gain robustness to corruption, we incorporate three changes to the common VAE: 1. Modeling the latent distribution as a mixture of Gaussian inliers and outliers, while using only the inlier component when testing; 2. Applying the Wasserstein-1 metric for regularization, instead of Kullback-Leibler divergence; and 3. Using a least absolute deviation error for reconstruction, which is equivalent to assuming a heavy-tailed likelihood. We illustrate state-of-the-art results on standard benchmark datasets for novelty detection.","publish_time":1591660800000,"author_summary":" Lai, Chieh-Hsin; Zou, Dongmian; Lerman, Gilad","abstract_summary":" We propose a new method for novelty detection<br>that can tolerate nontrivial corruption of the<br>training points. Previous works assumed either no or<br>very low corruption. Our method trains a robust<br>variational autoencoder (VAE), which aims to generate a<br>model for the uncorrupted training points. To gain<br>robustness to corruption, we incorporate three changes to<br>the common VAE: 1. Modeling the latent<br>distribution as a mixture of Gaussian inliers and outliers,<br>while using only the inlier component when testing;<br>2. Applying the Wasserstein-1 metric for<br>regularization, instead of Kullback-Leibler divergence; and<br>3. Using a least absolute deviation error for<br>reconstruction,...","title_summary":" Novelty Detection via Robust Variational<br>Autoencoding","x":-12.1265869141,"y":40.4312705994,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.1265869141,"tsne_y":40.4312705994,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"g8tzzu1m","source_x":"ArXiv","title":"Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining Paradigms","doi":null,"abstract":"Pretraining has become a standard technique in computer vision and natural language processing, which usually helps to improve performance substantially. Previously, the most dominant pretraining method is transfer learning (TL), which uses labeled data to learn a good representation network. Recently, a new pretraining approach -- self-supervised learning (SSL) -- has demonstrated promising results on a wide range of applications. SSL does not require annotated labels. It is purely conducted on input data by solving auxiliary tasks defined on the input data examples. The current reported results show that in certain applications, SSL outperforms TL and the other way around in other applications. There has not been a clear understanding on what properties of data and tasks render one approach outperforms the other. Without an informed guideline, ML researchers have to try both methods to find out which one is better empirically. It is usually time-consuming to do so. In this work, we aim to address this problem. We perform a comprehensive comparative study between SSL and TL regarding which one works better under different properties of data and tasks, including domain difference between source and target tasks, the amount of pretraining data, class imbalance in source data, and usage of target data for additional pretraining, etc. The insights distilled from our comparative studies can help ML researchers decide which method to use based on the properties of their applications.","publish_time":1592524800000,"author_summary":" Yang, Xingyi; He, Xuehai; Liang, Yuxiao; Yang,<br>Yue; Zhang, Shanghang; Xie, Pengtao","abstract_summary":" Pretraining has become a standard technique in<br>computer vision and natural language processing, which<br>usually helps to improve performance substantially.<br>Previously, the most dominant pretraining method is<br>transfer learning (TL), which uses labeled data to learn<br>a good representation network. Recently, a new<br>pretraining approach -- self-supervised learning (SSL) --<br>has demonstrated promising results on a wide range<br>of applications. SSL does not require annotated<br>labels. It is purely conducted on input data by solving<br>auxiliary tasks defined on the input data examples. The<br>current reported results show that in certain<br>applications, SSL outperforms TL and the other way around...","title_summary":" Transfer Learning or Self-supervised<br>Learning? A Tale of Two Pretraining Paradigms","x":-12.5881595612,"y":39.9756736755,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.5881595612,"tsne_y":39.9756736755,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"uqenldhr","source_x":"ArXiv","title":"Sample-Efficient Optimization in the Latent Space of Deep Generative Models via Weighted Retraining","doi":null,"abstract":"Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted retraining can be easily implemented on top of existing methods, and is empirically shown to significantly improve their efficiency and performance on synthetic and real-world optimization problems.","publish_time":1592265600000,"author_summary":" Tripp, Austin; Daxberger, Erik;<br>Hern'andez-Lobato, Jos'e Miguel","abstract_summary":" Many important problems in science and<br>engineering, such as drug design, involve optimizing an<br>expensive black-box objective function over a complex,<br>high-dimensional, and structured input space. Although machine<br>learning techniques have shown promise in solving such<br>problems, existing approaches substantially lack<br>sample efficiency. We introduce an improved method<br>for efficient black-box optimization, which<br>performs the optimization in the low-dimensional,<br>continuous latent manifold learned by a deep generative<br>model. In contrast to previous approaches, we<br>actively steer the generative model to maintain a latent<br>manifold that is highly useful for efficiently<br>optimizing the objective. We achieve this by periodically<br>retraining the...","title_summary":" Sample-Efficient Optimization in the Latent<br>Space of Deep Generative Models via Weighted<br>Retraining","x":-12.4375982285,"y":40.2060813904,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-12.4375982285,"tsne_y":40.2060813904,"subcluster":2,"subcluster_description":"Deep Neural Network","shape":"p"},{"cord_uid":"35s1z880","source_x":"ArXiv","title":"Using Photo Modeling Based 3DGRSL to Promote the Sustainability of Geo-Education, a case study from China","doi":null,"abstract":"In earth science education, observation of field geological phenomena is very important. Due to China's huge student population, it is difficult to guarantee education fairness and teaching quality in field teaching. Specimens are indispensable geo-education resources. However, the specimen cabinet or picture specimen library has many limitations and it is difficult to meet the internet-spirit or geo-teaching needs. Based on photo modeling, this research builds a 3D Geo-Resource Sharing Library (3DGRSL) for Geo-Education. It uses the Cesium engine and data-oriented distributed architecture to provide the educational resources to many universities. With Browser\/Server (B\/S) architecture, the system can realize multi-terminal and multi-scenario access of mobile phones, tablets, VR, PC, indoor, outdoor, field, providing a flexible and convenient way for preserving and sharing scientific information about geo-resources. This makes sense to students who cannot accept field teaching in under-funded colleges, and the ones with mobility problems. Tests and scoring results show that 3DGRSL is a suitable solution for displaying and sharing geological specimens. Which is of great significance for the sustainable use and protection of geoscience teaching resources, the maintenance of the right to fair education, and the construction of virtual simulation solutions in the future.","publish_time":1586649600000,"author_summary":" Sang, Xuejia; Xue, Linfu; Leng, Xiaopeng; Li,<br>Xiaoshun; Zhou, Jianping","abstract_summary":" In earth science education, observation of<br>field geological phenomena is very important. Due to<br>China's huge student population, it is difficult to<br>guarantee education fairness and teaching quality in<br>field teaching. Specimens are indispensable<br>geo-education resources. However, the specimen cabinet or<br>picture specimen library has many limitations and it is<br>difficult to meet the internet-spirit or geo-teaching<br>needs. Based on photo modeling, this research builds a<br>3D Geo-Resource Sharing Library (3DGRSL) for<br>Geo-Education. It uses the Cesium engine and data-oriented<br>distributed architecture to provide the educational<br>resources to many universities. With Browser\/Server<br>(B\/S) architecture, the system can realize<br>multi-terminal and...","title_summary":" Using Photo Modeling Based 3DGRSL to Promote<br>the Sustainability of Geo-Education, a case study<br>from China","x":-9.6251926422,"y":44.1942176819,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.6251926422,"tsne_y":44.1942176819,"subcluster":0,"subcluster_description":"Individual Mangrove Tree Modelingcombined","shape":"p"},{"cord_uid":"qpsjl9g0","source_x":"ArXiv","title":"Complex Human Action Recognition in Live Videos Using Hybrid FR-DL Method","doi":null,"abstract":"Automated human action recognition is one of the most attractive and practical research fields in computer vision, in spite of its high computational costs. In such systems, the human action labelling is based on the appearance and patterns of the motions in the video sequences; however, the conventional methodologies and classic neural networks cannot use temporal information for action recognition prediction in the upcoming frames in a video sequence. On the other hand, the computational cost of the preprocessing stage is high. In this paper, we address challenges of the preprocessing phase, by an automated selection of representative frames among the input sequences. Furthermore, we extract the key features of the representative frame rather than the entire features. We propose a hybrid technique using background subtraction and HOG, followed by application of a deep neural network and skeletal modelling method. The combination of a CNN and the LSTM recursive network is considered for feature selection and maintaining the previous information, and finally, a Softmax-KNN classifier is used for labelling human activities. We name our model as Feature Reduction&Deep Learning based action recognition method, or FR-DL in short. To evaluate the proposed method, we use the UCF dataset for the benchmarking which is widely-used among researchers in action recognition research. The dataset includes 101 complicated activities in the wild. Experimental results show a significant improvement in terms of accuracy and speed in comparison with six state-of-the-art articles.","publish_time":1593993600000,"author_summary":" Serpush, Fatemeh; Rezaei, Mahdi","abstract_summary":" Automated human action recognition is one of<br>the most attractive and practical research fields<br>in computer vision, in spite of its high<br>computational costs. In such systems, the human action<br>labelling is based on the appearance and patterns of the<br>motions in the video sequences; however, the<br>conventional methodologies and classic neural networks<br>cannot use temporal information for action<br>recognition prediction in the upcoming frames in a video<br>sequence. On the other hand, the computational cost of the<br>preprocessing stage is high. In this paper, we address<br>challenges of the preprocessing phase, by an automated<br>selection of representative frames among...","title_summary":" Complex Human Action Recognition in Live<br>Videos Using Hybrid FR-DL Method","x":-9.4702415466,"y":41.1221466064,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.4702415466,"tsne_y":41.1221466064,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"wdmf5e98","source_x":"ArXiv","title":"Zero-Shot Learning and its Applications from Autonomous Vehicles to COVID-19 Diagnosis: A Review","doi":null,"abstract":"The challenge of learning a new concept, object, or a new medical disease recognition without receiving any examples beforehand is called Zero-Shot Learning (ZSL). One of the major issues in deep learning based methodologies such as in Medical Imaging, Autonomous Systems and other real-world applications is the requirement of feeding a large annotated and labelled datasets, prepared by an expert human to train the network model. ZSL is known for having minimal human intervention by mainly relying only on previously known concepts and current auxiliary information. This is an ever-growing research for the cases where we have very limited or no datasets available and at the same time, the detection\/recognition system has human-like characteristics in learning new concepts. Therefore, it makes it applicable in real-world scenarios, from developing autonomous vehicles to medical imaging and COVID-19 Chest X-Ray (CXR) based diagnosis. In this review paper, we present the definition of the problem, we review over fundamentals, and the challenging steps of Zero-Shot Learning, including state-of-the-art categories of solutions as well as our recommended solution, motivations behind each approach, and their advantages over each category to guide the researchers to proceed with the best techniques and practices based on their applications. Inspired from different settings and extensions, we introduce a novel and broaden solution called one\/few-shot learning. We then review through different image datasets inducing medical and non-medical images, the variety of splits, and the evaluation protocols proposed so far. Finally, we discuss the recent applications and future directions of ZSL. We aim to convey a useful intuition through this paper towards the goal of handling complex computer vision learning tasks more similar to the way humans learn.","publish_time":1588118400000,"author_summary":" Rezaei, Mahdi; Shahidi, Mahsa","abstract_summary":" The challenge of learning a new concept,<br>object, or a new medical disease recognition without<br>receiving any examples beforehand is called Zero-Shot<br>Learning (ZSL). One of the major issues in deep learning<br>based methodologies such as in Medical Imaging,<br>Autonomous Systems and other real-world applications is<br>the requirement of feeding a large annotated and<br>labelled datasets, prepared by an expert human to train<br>the network model. ZSL is known for having minimal<br>human intervention by mainly relying only on<br>previously known concepts and current auxiliary<br>information. This is an ever-growing research for the cases<br>where we have very limited or...","title_summary":" Zero-Shot Learning and its Applications from<br>Autonomous Vehicles to COVID-19 Diagnosis: A Review","x":-8.725520134,"y":40.9294624329,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.725520134,"tsne_y":40.9294624329,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"dxg8phgy","source_x":"ArXiv","title":"Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization","doi":null,"abstract":"We propose detection of deepfake videos based on the dissimilarity between the audio and visual modalities, termed as the Modality Dissonance Score (MDS). We hypothesize that manipulation of either modality will lead to dis-harmony between the two modalities, eg, loss of lip-sync, unnatural facial and lip movements, etc. MDS is computed as an aggregate of dissimilarity scores between audio and visual segments in a video. Discriminative features are learnt for the audio and visual channels in a chunk-wise manner, employing the cross-entropy loss for individual modalities, and a contrastive loss that models inter-modality similarity. Extensive experiments on the DFDC and DeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art by up to 7%. We also demonstrate temporal forgery localization, and show how our technique identifies the manipulated video segments.","publish_time":1590710400000,"author_summary":" Chugh, Komal; Gupta, Parul; Dhall, Abhinav;<br>Subramanian, Ramanathan","abstract_summary":" We propose detection of deepfake videos based<br>on the dissimilarity between the audio and visual<br>modalities, termed as the Modality Dissonance Score (MDS).<br>We hypothesize that manipulation of either<br>modality will lead to dis-harmony between the two<br>modalities, eg, loss of lip-sync, unnatural facial and lip<br>movements, etc. MDS is computed as an aggregate of<br>dissimilarity scores between audio and visual segments in a<br>video. Discriminative features are learnt for the<br>audio and visual channels in a chunk-wise manner,<br>employing the cross-entropy loss for individual<br>modalities, and a contrastive loss that models<br>inter-modality similarity. Extensive experiments on the DFDC<br>and...","title_summary":" Not made for each other- Audio-Visual<br>Dissonance-based Deepfake Detection and Localization","x":-10.8628196716,"y":40.3826942444,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-10.8628196716,"tsne_y":40.3826942444,"subcluster":3,"subcluster_description":"Remote Arabic Speech Recognitiona","shape":"p"},{"cord_uid":"dwh3eax3","source_x":"Medline; PMC","title":"Effect of Wall Texture on Perceptual Spaciousness of Indoor Space","doi":"10.3390\/ijerph17114177","abstract":"As the main place of people\u2019s daily activities, indoor space (its size, shape, colors, material and textures, and so on) has important physical, emotional and health-based implications on people\u2019s behavior and quality of life. Material texture is an integral part of architectural environment perception and quality evaluation, but the effect of material texture on perceptual spaciousness lacks the support of experimental data. This research examined the effects between different wall textures on the observer\u2019s perception of spaciousness in indoor space, the influence of wall texture changes in different room sizes, and how the associational meaning of texture affects the degree of influence of wall texture on the spaciousness of indoor space. By using VR technology and the magnitude estimation (ME) analysis method, the authors found that the effect of wall texture on perceptual spaciousness varies depending on the wall material, and the textural effect is affected by room size. The perception of spaciousness is influenced by the observer\u2019s associational meaning of material texture, and the influence of associational meaning of material texture varies contingent on the room size. In relatively small rooms, the objective aspect (such as hardness, surface reflectivity, texture direction and texture depth) of the wall texture has a significant impact on perceived space. In contrast, the effects of subjective aspects (such as affinity and ecology) become more pronounced in relatively larger rooms. This research makes up for the lack of material texture research in perceptual spaciousness, and provides a new way for the designer to choose materials for the design of a spatial scale.","publish_time":1591833600000,"author_summary":" Wang, Chong; Lu, Wei; Ohno, Ryuzo; Gu, Zongchao","abstract_summary":" As the main place of people\u2019s daily activities,<br>indoor space (its size, shape, colors, material and<br>textures, and so on) has important physical, emotional<br>and health-based implications on people\u2019s<br>behavior and quality of life. Material texture is an<br>integral part of architectural environment perception<br>and quality evaluation, but the effect of material<br>texture on perceptual spaciousness lacks the support<br>of experimental data. This research examined the<br>effects between different wall textures on the<br>observer\u2019s perception of spaciousness in indoor space,<br>the influence of wall texture changes in different<br>room sizes, and how the associational meaning of<br>texture affects the degree...","title_summary":" Effect of Wall Texture on Perceptual<br>Spaciousness of Indoor Space","x":-9.0623502731,"y":43.7056427002,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.0623502731,"tsne_y":43.7056427002,"subcluster":0,"subcluster_description":"Individual Mangrove Tree Modelingcombined","shape":"p"},{"cord_uid":"j25603cd","source_x":"ArXiv; Elsevier","title":"Computer Vision Toolkit for Non-invasive Monitoring of Factory Floor Artifacts","doi":"10.1016\/j.promfg.2020.05.141","abstract":"Abstract Digitization has led to smart, connected technologies be an integral part of businesses, governments and communities. For manufacturing digitization, there has been active research and development with a focus on Cloud Manufacturing (CM) and the Industrial Internet of Things (IIoT). This work presents a computer vision toolkit (CV Toolkit) for non-invasive digitization of the factory floor in line with Industry 4.0 requirements for factory data collection. Currently, technical challenges persist towards digitization of legacy systems due to the limitation for changes in their design and sensors. This novel toolkit is developed to facilitate easy integration of legacy production machinery and factory floor artifacts with the digital and smart manufacturing environment with no requirement of any physical changes in the machines. The system developed is modular, and allows real-time monitoring of production machinery. The modularity aspect allows the incorporation of new software applications in the current framework of CV Toolkit. To allow connectivity of this toolkit with manufacturing floors in a simple, deployable and cost-effective manner, the toolkit is integrated with a known manufacturing data standard, MTConnect, to \u201ctranslate\u201d the digital inputs into data streams that can be read by commercial status tracking and reporting software solutions. The proposed toolkit is demonstrated using a mock-panel environment developed in house at the University of Cincinnati to highlight its usability.","publish_time":1609372800000,"author_summary":" Deshpande, Aditya M.; Telikicherla, Anil<br>Kumar; Jakkali, Vinay; Wickelhaus, David A.; Kumar,<br>Manish; Anand, Sam","abstract_summary":" Abstract Digitization has led to smart,<br>connected technologies be an integral part of<br>businesses, governments and communities. For<br>manufacturing digitization, there has been active research<br>and development with a focus on Cloud<br>Manufacturing (CM) and the Industrial Internet of Things<br>(IIoT). This work presents a computer vision toolkit<br>(CV Toolkit) for non-invasive digitization of the<br>factory floor in line with Industry 4.0 requirements<br>for factory data collection. Currently,<br>technical challenges persist towards digitization of<br>legacy systems due to the limitation for changes in<br>their design and sensors. This novel toolkit is<br>developed to facilitate easy integration of legacy<br>production machinery...","title_summary":" Computer Vision Toolkit for Non-invasive<br>Monitoring of Factory Floor Artifacts","x":-8.6553764343,"y":40.039981842,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.6553764343,"tsne_y":40.039981842,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"2l3io6v3","source_x":"Medline; PMC","title":"Vision Based Wall Following Framework: A Case Study With HSR Robot for Cleaning Application","doi":"10.3390\/s20113298","abstract":"Periodic cleaning of all frequently touched social areas such as walls, doors, locks, handles, windows has become the first line of defense against all infectious diseases. Among those, cleaning of large wall areas manually is always tedious, time-consuming, and astounding task. Although numerous cleaning companies are interested in deploying robotic cleaning solutions, they are mostly not addressing wall cleaning. To this end, we are proposing a new vision-based wall following framework that acts as an add-on for any professional robotic platform to perform wall cleaning. The proposed framework uses Deep Learning (DL) framework to visually detect, classify, and segment the wall\/floor surface and instructs the robot to wall follow to execute the cleaning task. Also, we summarized the system architecture of Toyota Human Support Robot (HSR), which has been used as our testing platform. We evaluated the performance of the proposed framework on HSR robot under various defined scenarios. Our experimental results indicate that the proposed framework could successfully classify and segment the wall\/floor surface and also detect the obstacle on wall and floor with high detection accuracy and demonstrates a robust behavior of wall following.","publish_time":1591747200000,"author_summary":" Teng, Tey Wee; Veerajagadheswar, Prabakaran;<br>Ramalingam, Balakrishnan; Yin, Jia; Elara Mohan, Rajesh;<br>G\u00f3mez, Braulio F\u00e9lix","abstract_summary":" Periodic cleaning of all frequently touched<br>social areas such as walls, doors, locks, handles,<br>windows has become the first line of defense against all<br>infectious diseases. Among those, cleaning of large wall<br>areas manually is always tedious, time-consuming,<br>and astounding task. Although numerous cleaning<br>companies are interested in deploying robotic cleaning<br>solutions, they are mostly not addressing wall cleaning.<br>To this end, we are proposing a new vision-based<br>wall following framework that acts as an add-on for<br>any professional robotic platform to perform wall<br>cleaning. The proposed framework uses Deep Learning (DL)<br>framework to visually detect, classify, and segment the...","title_summary":" Vision Based Wall Following Framework: A Case<br>Study With HSR Robot for Cleaning Application","x":-8.8707408905,"y":40.2074890137,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.8707408905,"tsne_y":40.2074890137,"subcluster":-1,"subcluster_description":-1,"shape":"p"},{"cord_uid":"2yr9ui3z","source_x":"Elsevier; Medline; PMC","title":"An efficient image descriptor for image classification and CBIR","doi":"10.1016\/j.ijleo.2020.164833","abstract":"Pattern recognition and feature extraction of images always have been important subjects in improving the performance of image classification and Content-Based Image Retrieval (CBIR). Recently, Machine Learning and Deep Learning algorithms are utilized widely in order to achieve these targets. In this research, an efficient method for image description is proposed which is developed by Machine Learning and Deep Learning algorithms. This method is created using combination of an improved AlexNet Convolutional Neural Network (CNN), Histogram of Oriented Gradients (HOG) and Local Binary Pattern (LBP) descriptors. Furthermore, the Principle Component Analysis (PCA) algorithm has been used for dimension reduction. The experimental results demonstrate the superiority of the offered method compared to existing methods by improving the accuracy, mean Average Precision (mAP) and decreasing the complex computation. The experiments have been run on Corel-1000, OT and FP datasets.","publish_time":1588550400000,"author_summary":" Shakarami, Ashkan; Tarrah, Hadis","abstract_summary":" Pattern recognition and feature extraction of<br>images always have been important subjects in<br>improving the performance of image classification and<br>Content-Based Image Retrieval (CBIR). Recently, Machine<br>Learning and Deep Learning algorithms are utilized<br>widely in order to achieve these targets. In this<br>research, an efficient method for image description is<br>proposed which is developed by Machine Learning and Deep<br>Learning algorithms. This method is created using<br>combination of an improved AlexNet Convolutional Neural<br>Network (CNN), Histogram of Oriented Gradients (HOG)<br>and Local Binary Pattern (LBP) descriptors.<br>Furthermore, the Principle Component Analysis (PCA)<br>algorithm has been used for dimension reduction. The...","title_summary":" An efficient image descriptor for image<br>classification and CBIR","x":-9.4606313705,"y":41.8389358521,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-9.4606313705,"tsne_y":41.8389358521,"subcluster":7,"subcluster_description":"Cnnan Efficient Image Descriptor","shape":"p"},{"cord_uid":"dl0nvx30","source_x":"ArXiv; Elsevier","title":"One-Shot Recognition of Manufacturing Defects in Steel Surfaces","doi":"10.1016\/j.promfg.2020.05.146","abstract":"Abstract Quality control is an essential process in manufacturing to make the product defect-free as well as to meet customer needs. The automation of this process is important to maintain high quality along with the high manufacturing throughput. With recent developments in deep learning and computer vision technologies, it has become possible to detect various features from the images with near-human accuracy. However, many of these approaches are data intensive. Training and deployment of such a system on manufacturing floors may become expensive and time-consuming. The need for large amounts of training data is one of the limitations of the applicability of these approaches in real-world manufacturing systems. In this work, we propose the application of a Siamese convolutional neural network to do one-shot recognition for such a task. Our results demonstrate how one-shot learning can be used in quality control of steel by identification of defects on the steel surface. This method can significantly reduce the requirements of training data and can also be run in real-time.","publish_time":1609372800000,"author_summary":" Deshpande, Aditya M.; Minai, Ali A.; Kumar,<br>Manish","abstract_summary":" Abstract Quality control is an essential<br>process in manufacturing to make the product<br>defect-free as well as to meet customer needs. The<br>automation of this process is important to maintain high<br>quality along with the high manufacturing throughput.<br>With recent developments in deep learning and<br>computer vision technologies, it has become possible to<br>detect various features from the images with<br>near-human accuracy. However, many of these approaches<br>are data intensive. Training and deployment of<br>such a system on manufacturing floors may become<br>expensive and time-consuming. The need for large amounts<br>of training data is one of the limitations of the...","title_summary":" One-Shot Recognition of Manufacturing<br>Defects in Steel Surfaces","x":-8.3281450272,"y":40.6623001099,"cluster":9,"cluster_name":"c10","cluster_description":"Deep Reinforcement Learninga Semi-Supervised","tsne_x":-8.3281450272,"tsne_y":40.6623001099,"subcluster":4,"subcluster_description":"Manufacturing Defects","shape":"p"}]