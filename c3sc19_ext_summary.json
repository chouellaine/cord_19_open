{"cluster": 3, "subcluster": 19, "abstract_summ": "Our results quantify the benefits of: (a) using contextualized embeddings over fixed word embeddings; (b) using a BiLSTM-CRF architecture with contextualized word embeddings over fine-tuning the contextualized embedding model directly; and (c) using domain-specific contextualized embeddings (SciBERT).For models trained on CoNLL03, language model contextualization leads to a +1.2% maximal relative micro-F1 score increase in-domain against +13% out-of-domain on the WNUT dataset (The code is available at https://github.com/btaille/contener).It is important to locate important context words in the sentences and model them appropriately to perform event detection (ED) effectively.We first use this method to measure the learnability of concepts on pretrained word embeddings.We then develop a statistical analysis of concept learnability, based on hypothesis testing and ROC curves, in order to compare the relative merits of various embedding algorithms using a fixed corpora and hyper parameters.In this work, we propose a novel method that learns to select relevant context words for ED based on the Gumbel-Softmax trick.", "title_summ": "Keyphrase Extraction as Sequence Labeling Using Contextualized EmbeddingsContextualized Embeddings in Named-Entity Recognition: An Empirical Study on GeneralizationLearning to Select Important Context Words for Event DetectionOn the Learnability of Concepts:With Applications to Comparing Word Embedding Algorithms", "title_abstract_phrases": "Our results quantify the benefits of: (a) using contextualized embeddings over fixed word embeddings; (b) using a BiLSTM-CRF architecture with contextualized word embeddings over fine-tuning the contextualized embedding model directly; and (c) using domain-specific contextualized embeddings (SciBERT).For models trained on CoNLL03, language model contextualization leads to a +1.2% maximal relative micro-F1 score increase in-domain against +13% out-of-domain on the WNUT dataset (The code is available at https://github.com/btaille/contener).Learning to Select Important Context Words for Event DetectionIt is important to locate important context words in the sentences and model them appropriately to perform event detection (ED) effectively.Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on GeneralizationContextualized embeddings use unsupervised language model pretraining to compute word representations depending on their context.We first use this method to measure the learnability of concepts on pretrained word embeddings.Keyphrase Extraction as Sequence Labeling Using Contextualized EmbeddingsIn this paper, we formulate keyphrase extraction from scholarly articles as a sequence labeling task solved using a BiLSTM-CRF, where the words in the input text are represented using deep contextualized embeddings."}