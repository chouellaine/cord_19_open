{"cluster": 7, "subcluster": 8, "abstract_summ": "The ever increasing demand for higher memory performance and\u2014at the same time\u2014larger memory capacity is leading the industry towards hybrid main memory designs, i.e., memory systems that consist of multiple different memory technologies.We evaluate the proposed designs against state-of-the-art MPI libraries and show up to 41% and 22% reduction in latency for collective operations and stencil-based application kernels on 1024 and 128 nodes, respectively, as well as 23% improvement in communication performance of the P3DFFT application.We identify several patterns common to multi-threaded OpenSHMEM applications, leverage user-level threads to increase overlap of communication and computation, and explore the impact of different thread scheduling policies.Overlap of computation and communication is critical for good application-level performance.Results indicate that user-level threading can enable blocking communication to meet the performance of highly-optimized, non-blocking, single-threaded codes with significantly lower application-level complexity.", "title_summ": "Load-Balancing Parallel Relational AlgebraPattern-Aware Staging for Hybrid Memory SystemsUnderstanding HPC Benchmark Performance on Intel Broadwell and Cascade Lake ProcessorsCommunication-AwareThread SchedulingA Current Task-Based Programming Paradigms AnalysisX-CEL: A Method to Estimate Near-Memory Acceleration Potential in Tile-Based MPSoCsHardware-Assisted MPI Overlap EngineSimplifying Communication Overlap in OpenSHMEM Through Integrated User-Level", "title_abstract_phrases": "Staging for Hybrid Memory SystemsThe ever increasing demand for higher memory performance and\u2014at the same time\u2014larger memory capacity is leading the industry towards hybrid main memory designs, i.e., memory systems that consist of multiple different memory technologies.Our work is based on the following observations: (a) the high-bandwidth fast memory outperforms the large memory for memory intensive tasks; (b) but those tasks can run for much longer than a bulk data copy to/from the fast memory, especially when the access pattern is more irregular/sparse.We evaluate the proposed designs against state-of-the-art MPI libraries and show up to 41% and 22% reduction in latency for collective operations and stencil-based application kernels on 1024 and 128 nodes, respectively, as well as 23% improvement in communication performance of the P3DFFT application.We identify several patterns common to multi-threaded OpenSHMEM applications, leverage user-level threads to increase overlap of communication and computation, and explore the impact of different thread scheduling policies.We highlight relevant hardware configuration settings that can have a decisive impact on code performance and show how to properly measure on-chip and off-chip data transfer bandwidths."}