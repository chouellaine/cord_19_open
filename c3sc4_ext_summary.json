{"cluster": 3, "subcluster": 4, "abstract_summ": "By employing an innovative neural network architecture based on graph attention networks and temporal convolutions, our framework jointly learns graph representations contemplating evolving graph structure and temporal patterns.We show that sparsified graphs provided by SGCN can be used as inputs to GCN, leading to better or comparable node classification performance with that of original graphs in GCN, DeepWalk, and GraphSAGE.Graph embedding methods are useful for a wide range of graph analysis tasks including link prediction and node classification.Most existing dynamic graph representation learning methods focus on modeling dynamic graphs with fixed nodes due to the complexity of modeling dynamic graphs, and therefore, cannot efficiently learn the evolutionary patterns of real-world evolving graphs.Most graph embedding methods learn only the topological structure of graphs.RASE is a novel graph representation learning model which effectively preserves both graph structure and node attributes through a unified loss function.", "title_summ": "A Graph Sparsifier Based on Graph Convolutional NetworksRobust Attribute and Structure Preserving Graph EmbeddingNegative Sampling for Hyperlink Prediction in NetworksBottom-Up and Top-Down Graph PoolingQuality-Aware Streaming Network Embedding with Memory RefreshingRole Equivalence Attention for Label Propagation in Graph Neural NetworksAttention-Based Graph EvolutionHyperbolic Knowledge Graph Embeddings for Knowledge Base CompletionImage-Based World-perceiving Knowledge Graph (WpKG) with ImprecisionDDNE: Discriminative Distance Metric Learning for Network EmbeddingNode Classification in Complex Social Graphs via Knowledge-GraphAttention-Based Dynamic Graph Representation LearningAttention-Based Aggregation Graph Networks for Knowledge Graph Information TransferSubRank: Subgraph Embeddings via a Subgraph Proximity MeasureMSGE: A Multi-step Gated Model for Knowledge Graph CompletionSGCN:Knowledge Graph Entity Alignment with Graph Convolutional Networks: Lessons LearnedDynamic Heterogeneous Graph Embedding Using Hierarchical AttentionsInfluence of Random Walk Parametrization on Graph EmbeddingsSemantic Path-Based Learning for Review Volume PredictionImprovingInterpretable attribution for feature interactionsFast Graph Attention Networks Using Effective Resistance Based Graph SparsificationGraph Neural Network Based Coarse-Grained Mapping PredictionDeep Graph Contrastive Representation LearningSoft Labels Guided Graph Attention NetworksAttribute-Driven Capsule Network for Entity Relation PredictionEstimating Descriptors for Large GraphsTemporalGAT:", "title_abstract_phrases": "By employing an innovative neural network architecture based on graph attention networks and temporal convolutions, our framework jointly learns graph representations contemplating evolving graph structure and temporal patterns.We show that sparsified graphs provided by SGCN can be used as inputs to GCN, leading to better or comparable node classification performance with that of original graphs in GCN, DeepWalk, and GraphSAGE.Robust Attribute and Structure Preserving Graph EmbeddingGraph embedding methods are useful for a wide range of graph analysis tasks including link prediction and node classification.Most existing dynamic graph representation learning methods focus on modeling dynamic graphs with fixed nodes due to the complexity of modeling dynamic graphs, and therefore, cannot efficiently learn the evolutionary patterns of real-world evolving graphs.Most graph embedding methods learn only the topological structure of graphs.RASE is a novel graph representation learning model which effectively preserves both graph structure and node attributes through a unified loss function."}