{"cluster": 3, "subcluster": 21, "abstract_summ": "It aims to enhance the performance of text-based image retrieval by boosting the rank of relevant images using visual information.While most of the current approaches for cross-modal retrieval revolve around learning how to represent text and images in a shared latent space, we take a different direction: we propose to generalize the cross-modal relevance feedback mechanism, a simple yet effective unsupervised method, that relies on standard information retrieval heuristics and the choice of a few hyper-parameters.By leveraging reconstructed semantic representations, the hash codes are learnt by considering the underlying correlations between labels, hash codes and original features, resulting in a further performance improvement.We introduce a novel mechanism, the Topic-Word Attention (TWA), that generates document representations based on the interplay between word and topic representations.To tackle these problems, we propose a shallow supervised hash learning method \u2013 Semantics-reconstructing Cross-modal Hashing (SCH), which reconstructs semantic representation and learns the hash codes for the entire dataset jointly.", "title_summ": ": Rank Fusion applied to Coordination Level Matching for Ranking in Systematic ReviewsVisual Re-Ranking via Adaptive Collaborative Hypergraph Learning for Image RetrievalSemantics-Reconstructing Hashing for Cross-Modal RetrievalGraph-Based Image Retrieval: State of the ArtLearning to Rank Images with Cross-Modal Graph ConvolutionsInductive Document Network Embedding with Topic-Word AttentionYouCan Teach an Old Dog New Tricks", "title_abstract_phrases": "It aims to enhance the performance of text-based image retrieval by boosting the rank of relevant images using visual information.Learning to Rank Images with Cross-Modal Graph ConvolutionsWe are interested in the problem of cross-modal retrieval for web image search, where the goal is to retrieve images relevant to a text query.However, finding visual and relevant information in an image is a huge task for Image Retrieval community and a very discussed issue in digital image processing.While most of the current approaches for cross-modal retrieval revolve around learning how to represent text and images in a shared latent space, we take a different direction: we propose to generalize the cross-modal relevance feedback mechanism, a simple yet effective unsupervised method, that relies on standard information retrieval heuristics and the choice of a few hyper-parameters.We introduce a novel mechanism, the Topic-Word Attention (TWA), that generates document representations based on the interplay between word and topic representations."}